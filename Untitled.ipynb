{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpansion:\n",
    "    \"\"\"Pre-retrieval kNN based approach for query expansion.\n",
    "    Args:\n",
    "        stopwords (list): List of words, that we want to remove from the tokenized text.\n",
    "        wv (Word2VecKeyedVectors): Word embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords, wv):\n",
    "        self.stopwords = stopwords\n",
    "        self.wv = wv\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes, lowers words and removes stopwords from the document.\n",
    "        Args:\n",
    "            text (str): Text that we want to tokenize. \n",
    "        Returns:\n",
    "            filtered_tokens (list): List of low case tokens wich does not contain stop words.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [w.lower() for w in tokens if not w in self.stopwords]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def extend_tokens(self, token_list):\n",
    "        \"\"\"Extends token list summing consecutive vector pairs.\n",
    "        Args: \n",
    "            token_list (list): List of tokens that we want to extend.\n",
    "        Returns:\n",
    "            extension (list): List of extensions.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for token in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in self.wv.vocab.keys():\n",
    "                tokens.append(token)\n",
    "        extension = set()\n",
    "        for i in range(len(tokens)-1):\n",
    "            new_token = self.wv.most_similar(positive=[tokens[i], tokens[i+1]])[0][0]\n",
    "            extension.add(new_token)\n",
    "        extension = list(extension)\n",
    "        return extension\n",
    " \n",
    "    def get_candidate_expansion_terms(self, tokens, k):\n",
    "        \"\"\"Gets the candidates for expansions based on kNN.\n",
    "        Args: \n",
    "            tokens (list): List of tokens that we want to expand.\n",
    "            k (int): Number of nearest neighbours.\n",
    "        Returns:\n",
    "            candidates (list): List of candidates.\n",
    "        \"\"\"\n",
    "        candidates = set()\n",
    "        for token in tokens:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in self.wv.vocab.keys():\n",
    "                result = self.wv.similar_by_word(token)\n",
    "                limit = k if len(result) > k else len(result)\n",
    "                # iterate through the most similar words\n",
    "                for i in range(limit):\n",
    "                    candidates.add(result[i][0])\n",
    "        # return list of candidates\n",
    "        candidates = list(candidates)\n",
    "        return candidates\n",
    "\n",
    "\n",
    "    def similarity(self, token, token_list):\n",
    "        \"\"\"Calculates the similarity between token and list of tokens.\n",
    "        Args: \n",
    "            token (str): String for wich we are calculating similarity.\n",
    "            token_list (list): List of tokens to wich we are calculating similarity of token.\n",
    "        Returns:\n",
    "            avreage_similarity (float): Number that signifes the similarity of token to token list words.\n",
    "        \"\"\"\n",
    "        similarity = 0\n",
    "        num_of_tokens = 0\n",
    "        for toks in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if toks in self.wv.vocab.keys():\n",
    "                num_of_tokens += 1\n",
    "                similarity += self.wv.similarity(toks, token)\n",
    "        avreage_similarity = similarity/num_of_tokens\n",
    "        return avreage_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_similarity_pairs(self,tokens, candidates):\n",
    "        \"\"\"Calculates similarity to tokens for list of candidates.\n",
    "        Args: \n",
    "            tokens (list): List of tokens to wich similarity is calculated\n",
    "            candidates (list): List of tokens for wich similarity is calculated.\n",
    "        Returns:\n",
    "            similarity_pairs (list): List of tuples. Tuples are pairs of candidates and their similarity to tokens.\n",
    "        \"\"\"\n",
    "        similarity_pairs = []\n",
    "        for candidate in candidates:\n",
    "            sim = self.similarity(candidate, tokens)\n",
    "            similarity_pairs.append((candidate, sim))\n",
    "        # return the list of expansion terms with their similarities\n",
    "        return similarity_pairs\n",
    "\n",
    "    def pre_retrieval_KNN(self, query, k, n):\n",
    "        \"\"\"Find n most similar tokens to the given query\n",
    "        Args: \n",
    "            query (string): User query we want to expand.\n",
    "            wv (): \n",
    "            n (int): Number of candidates (with the highest simiarity) that is returned.\n",
    "        Returns:\n",
    "            similarity_pairs (list): List of n similarity pairs with the highest similarity to query tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(query)\n",
    "        candidates = self.get_candidate_expansion_terms(tokens, k)\n",
    "        candidates_sim = self.get_top_expansion_terms(tokens, candidates)\n",
    "        def takeSecond(elem):\n",
    "            return elem[1]\n",
    "        sort = sorted(candidates_sim, key=takeSecond)[::-1]\n",
    "        return sort[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words 2519370\n"
     ]
    }
   ],
   "source": [
    "wiki_en_align = './../data/fasttext/wiki.en.align.vec' #'../../data/fasttext/wiki.en.align.vec'\n",
    "# get fasttext wiki embeddings for english\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n",
    "print('english words {}'.format(len(list(wv_wiki_en.vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = QueryExpansion(stop_words, wv_wiki_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'fish']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.tokenize(\"Hello fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderfish']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.extend_tokens(['hello', 'fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gamefish',\n",
       " 'milkfishes',\n",
       " 'helloooo',\n",
       " 'baitfish',\n",
       " 'shellfishes',\n",
       " 'hello,my',\n",
       " 'fishes',\n",
       " 'hello,',\n",
       " 'hello,im',\n",
       " 'hello,i']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_candidate_expansion_terms( ['hello', 'fish'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39556539239578403"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.similarity( \"fishy\", ['hello', 'fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QueryExpansion' object has no attribute 'get_similarity_pairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2d5deb6b34d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_similarity_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fish'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'hell'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'QueryExpansion' object has no attribute 'get_similarity_pairs'"
     ]
    }
   ],
   "source": [
    "obj.get_similarity_pairs(['hello', 'fish'], ['hell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QueryExpansion' object has no attribute 'pre_retrieval_KNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-23de383b77ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_retrieval_KNN\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"Hello fish\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'QueryExpansion' object has no attribute 'pre_retrieval_KNN'"
     ]
    }
   ],
   "source": [
    "obj.pre_retrieval_KNN( \"Hello fish\", 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from modules.library.QueryExpansion import QueryExpansion\n",
    "qe = QueryExpansion(stop_words, wv_wiki_en) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39556539239578403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe.similarity( \"fishy\", ['hello', 'fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hell', 0.24798550996917706)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe.get_similarity_pairs(['hello', 'fish'], ['hell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fishes', 0.4700446672545525),\n",
       " ('baitfish', 0.43702516617969844),\n",
       " ('milkfishes', 0.4292284858020971),\n",
       " ('hello,im', 0.4204467161960133),\n",
       " ('hello,i', 0.4198617718206871),\n",
       " ('shellfishes', 0.41280553988894253),\n",
       " ('gamefish', 0.41239430182793024),\n",
       " ('helloooo', 0.41166375549810463),\n",
       " ('hello,my', 0.40989039522526416),\n",
       " ('hello,', 0.395156441780529)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe.pre_retrieval_KNN( \"Hello fish\", 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
