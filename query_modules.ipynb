{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from modules.library import documentRetrieval as dr\n",
    "from modules.library import queryExp as qe\n",
    "from modules.library.postgresql import PostgresQL\n",
    "pg = PostgresQL() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modules.library.documentRetrieval' from 'C:\\\\Users\\\\sarab\\\\work\\\\try\\\\enviroLENS\\\\word-embeddings\\\\modules\\\\library\\\\documentRetrieval.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-cab7617bd286>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-cab7617bd286>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tfidf_score_str(tokens,texts,tfidf_function_name,m,*args):\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words 2519370\n"
     ]
    }
   ],
   "source": [
    "# importing word vectors\n",
    "from gensim.models import KeyedVectors\n",
    "wiki_en_align = './../data/fasttext/wiki.en.align.vec' #'../../data/fasttext/wiki.en.align.vec'\n",
    "# get fasttext wiki embeddings for english\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n",
    "print('english words {}'.format(len(list(wv_wiki_en.vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stopword list\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pg.connect(database=\"envirolens\", user=\"postgres\", password=\"dbpass\") #\"eurlex_env_only\" \"solata.2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_query(query_words):\n",
    "    \"\"\" Iz baze vrne seznam slovarjev, ki vsebujejo dokumente (tekst) in njihove IDje, ki v tekstu vsebujejo vsaj eno od besed iz seznama(list) query_words\"\"\"\n",
    "    output = '|'.join(query_words)\n",
    "    SQL = \"\"\"\n",
    "            SELECT document_id, fulltext_cleaned FROM documents\n",
    "            WHERE to_tsvector('english', fulltext_cleaned) @@ to_tsquery('english',\"\"\" + '\\''+ output + '\\'' + \"\"\");\"\"\"\n",
    "    documents = pg.execute(SQL)\n",
    "    return(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = qe.tokenize('water pollution underground', stopWords)\n",
    "ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "candidates = qe.pre_retrieval_KNN('water pollution underground', 5, wv_wiki_en, 10, stopWords,extension=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pollutions',\n",
       " '—fishing',\n",
       " 'shellfishing',\n",
       " '‘fishing',\n",
       " 'earpollution',\n",
       " 'flwfishing',\n",
       " 'pollution,',\n",
       " 'fishing,\\u3000',\n",
       " '#pollution',\n",
       " 'biopollution',\n",
       " 'fishings',\n",
       " 'polluting',\n",
       " 'billfishing',\n",
       " 'sollution',\n",
       " 'gamefishing',\n",
       " 'pollut',\n",
       " 'fishing,',\n",
       " 'antipollution',\n",
       " 'pollutants',\n",
       " 'codfishing']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe.pre_retrieval_KNN('fishing and pollution', 15, wv_wiki_en, 30, stopWords,extension=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [\"pollution\",\"fish\"]\n",
    "docs = db_query(tokenized) \n",
    "#premisli kaj (tokens, ext, candidates) das v db_query (I guess vse? --> ce dela na osnovnem queryju dovolj dobro pusti osnovni, drugace prevec dokumentov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13771\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 1000002956,\n",
       " 'fulltext_cleaned': '7 12 2018 en official journal european union c 443 9 minutes sitting wednesday 20 june 2018 2018 c 443 04 contents 1 summary report workshop 9 2 report economic social partners 9 3 report youth conference 10 4 statement carl b greenidge vice president minister foreign affairs guyana 10 5 statement ekaterina zaharieva deputy prime minister judicial reform minister foreign affairs bulgaria president office eu council 10 6 question time council 10 7 debate council — ‘catch eye’ 11 8 report 15th regional meeting held nairobi kenya east african region 11 13 april 2018 — report co presidents 11 9 approval minutes afternoon sitting monday 18 june 2018 morning afternoon sittings tuesday 19 june 2018 11 10 vote motions resolution included reports submitted three standing committees 12 11 vote urgent motions resolution 13 12 business 13 13 date place 36th session joint parliamentary assembly 13 annex alphabetical list members joint parliamentary assembly 14 annex ii record attendance session held brussels belgium 18 20 june 2018 18 minutes sitting wednesday 20 june 2018 sitting opened 9 08 chair joseph owona kono acting co president 1 summary report workshop clifford andre seychelles gave summary workshop ‘biotechnological research sustainable agriculture focus developing countries’ sunday 17 june participants visited vlaams instituut voor biotechnologie vib ghent belgium focuses basic research molecular foundations life specific biotechnological research focus developing countries participants heard three presentations given guided visit laboratories speakers catherine bearder halifa sallah gambia 2 report economic social partners presentation brenda king member acp follow committee european economic social committee eesc brenda king member acp follow committee european economic social committee replacing mr jaroslaw mulewicz president eu acp follow committee gave overview eesc’s recent activities since participation last meeting jpa future activities follow committee speakers michel kamano guinee halifa sallah gambia agnima alain michel lobognon côte divoire 3 report youth conference agreement co presidents young representative chosen participants youth conference given floor present conclusions conference centred need encourage support intercultural dialogue integration forms subsequent debate much enthusiasm role played young people several proposals put forward involvement youth assemblys work louis michel suggested creation ‘assembly acp eu youth’ organised together session joint parliamentary assembly similar way members suggested creation youth fora provide receive input jpa taking inspiration already existing models speakers nafie ali nafie ahmed sudan halifa sallah gambia cheryl sandra v husbands barbados louis michel ben abdallah banda ghana sir louis straker st vincent grenadines mathew sahr nyuma sierra leone dexter nduna zimbabwe nafie ali nafie ahmed sudan margaret kamar kenya jacob oulanyah uganda adjedoue weidou chad heather mwiza sibungo namibia waven william seychelles liuga faumuina samoa agnima alain michel lobognon côte divoire michèle rivasi 4 statement carl b greenidge vice president minister foreign affairs guyana carl b greenidge vice president minister foreign affairs guyana behalf president office acp council ministers described importance acp eu jpa forum dialogue development underlining fact structured forum kind covering four global geographical spaces world 79 acp 28 member states potential play decisive role shaping global governance decision making international fora pointed need ensure new framework cooperation one effective strategic balanced results driven future partnership beyond 2020 furthermore announced results 107th session acp council ministers held lomé togo 30 may 2018 acp group adopted negotiating mandate post cotonou partnership agreement mandate considered strong manifestation unity solidarity governments peoples acp states concluded expressing appreciation strong support shown assembly preservation unity acp group emphasising importance acp eu negotiating single equal entities 5 statement ekaterina zaharieva deputy prime minister judicial reform minister foreign affairs bulgaria president office eu council ekaterina zaharieva deputy prime minister judicial reform minister foreign affairs bulgaria president office eu council described key priorities presidency bulgaria pointing particular future acp eu relations end cotonou agreement 2020 underlined importance taking advantage momentum acp eu join forces drive forward common global multilateral agenda welcomed adoption acp group negotiating mandate announced eu would adopt following monday calling increased dialogue understanding acp eu added essential adapt acp eu relations new realities global situation order effective emphasised necessity deepening acp eu partnership regional level reflect integrate regional dynamics africa caribbean pacific future agreement seek advance common inter sectorial interests different key areas observed climate action important area cooperation called acp eu coordinated actions cooperation fight build resilience recurrent man made natural disasters undermine development already fragile countries furthermore pointed crucial democratic partnership parliamentary dimension order contribute increased dialogue understanding peoples 6 question time council 2 questions put acp council 7 eu council carl b greenidge replied behalf acp council following written questions followed supplementary questions question 1 sabine lösing stronger role joint parliamentary assembly new partnership agreement question question 2 nathalie griesbeck replaced thierry cornillet illegal fishing caribbean ii ekaterina zaharieva replied behalf eu council following written questions supplementary questions question 3 jo leinen replaced neena gill beginning question situation gabon question 4 neena gill absence council position violent repression security forces drc december 2017 january 2018 question 5 nathalie griesbeck replaced thierry cornillet illegal fishing caribbean question 6 catherine bearder wildlife trafficking acp eu relations question 7 thierry cornillet need improve communication eu funding provide training local stakeholders making applications projects question 8 laura agea future eu programmes policies sdgs question 9 sabine lösing replaced lidia senra rodriguez stronger role joint parliamentary assembly new partnership agreement 7 debate council — ‘catch eye’ speakers michèle rivasi lucie milebou aubsson ép mboussou gabon halifa sallah gambia michel kamano guinee jomo mfanawemakhosi dlamini kingdom eswatini members focused topics eu budget edf traceability agricultural sector large scale production policies acp countries regarding use pesticides plastics cotonou partnership agreement regional integration mining digital economy added value chains future mff world trade conflicts effects acp economies ekaterina zaharieva replied behalf eu council questions 1 3 4 carl b greenidge replied behalf acp council questions 2 3 speakers clifford andre seychelles liuga faumuina samoa spès caritas njebarikanuye burundi ben abdallah banda ghana dexter nduna zimbabwe adjedoue weidou chad louis michel members focused topics parliamentarians’ role negotiating mandate small island states illegal unreported unregulated iuu fishing global warming sea levels plastic waste fish stocks pacific lifting eu sanctions burundi technology transfer acp countries infrastructure development acp proposals boost economy minimise migration budgetisation edf carl b greenidge replied behalf acp council questions 1 5 6 ekaterina zaharieva replied behalf eu council questions 1 2 3 4 5 8 report 15th regional meeting held nairobi kenya east african region 11 13 april 2018 — report co presidents acting co president michèle rivasi gave oral report 15th regional meeting east africa region highlighting main topics discussed making special mention need promote deeper regional integration would strengthen partnerships private sector civil society well strengthen control mechanisms trade achieve sustainable socio economic political development pointed long term solution eliminating causes conflict enhance political dialogue well strengthen cooperation international assistance future acp eu post cotonou agreement highlighted need build cotonou acquis integrate new global realities particular regional specificities equally important need keep civil society acp states informed negotiations progressing 9 approval minutes afternoon sitting monday 18 june 2018 morning afternoon sittings tuesday 19 june 2018 minutes approved sitting adjourned 12 35 resumed 15 01 chair louis michel co president 10 vote motions resolution included reports submitted three standing committees co president reminded assembly voting procedures acp eu relations post cotonou strong parliamentary dimension acp eu 102 400 18 fin committee political affairs report jacob oulanyah uganda cristian dan preda amendments adopted 1 6 7 1 oral amendment recital amendments rejected 2 3 4 5 vote separate houses amendment 2 requested epp group amendment rejected split vote separate houses amendment 3 requested epp group amendment rejected split vote separate houses recital f requested group first part adopted split vote paragraph 6 vote separate houses two parts requested group first part adopted split vote paragraph 11 vote separate houses second part requested epp group first part adopted split vote paragraph 12 vote separate houses second part requested epp group first part adopted split vote paragraph 16 vote separate houses second part requested epp group first part adopted split vote paragraph 20 vote separate houses second part requested epp group first part adopted resolution thus amended adopted 62 votes favour 0 5 abstentions ii impact illegal trade phytosanitary products seeds agricultural inputs acp countries’ economies acp eu 102 403 18 fin committee economic development finance trade report lucie milebou aubusson épouse mboussou gabon maria arena amendments adopted 2 6 7 8 amendments rejected 1 4 5 amendments fallen 3 split vote recital requested epp group recital adopted separate vote paragraph 18 requested epp group paragraph adopted resolution thus amended adopted unanimously iii social environmental consequences urbanisation particularly sound management industrial domestic waste acp countries acp eu 102 408 18 fin committee social affairs environment report halifa sallah gambia eleni theocharous amendment adopted 2 amendment rejected 1 split vote separate houses para 6 requested epp ecr groups first part adopted resolution thus amended adopted unanimously 11 vote urgent motions resolution urgency new measures fight international terrorism acp eu 102 582 18 fin amendments adopted 1 resolution thus amended adopted unanimously ii humanitarian crisis south sudan acp eu 102 583 18 fin amendments adopted 1 resolution thus amended adopted unanimously three abstentions 12 business co president thanked members co secretariat interpreters commission contributions 13 date place 36th session joint parliamentary assembly 36th session assembly take place benin 3 5 december 2018 sitting closed 15 32 joseph owona kono louis michel co presidents patrick gomes josé javier fernández fernández acting co secretaries general annex alphabetical list members joint parliamentary assembly acp representatives ep representatives kono cameroon co president michel co president angola antigua barbuda bahamas barbados vp belize benin botswana burkina faso burundi cameroon cape verde central african republic chad vp comoros vp congo democratic republic congo republic cook islands côte d’ivoire djibouti dominica dominican republic equatorial guinea eritrea eswatini kingdom ethiopia vp fiji vp gabon vp gambia ghana grenada guinea vp guinea bissau guyana haiti jamaica kenya kiribati lesotho liberia madagascar malawi mali marshall islands republic mauritania mauritius micronesia federated states mozambique namibia vp nauru niger nigeria vp niue palau papua new guinea rwanda saint kitts nevis saint lucia saint vincent grenadines samoa vp são tomé príncipe senegal seychelles sierra leone solomon islands somalia south africa vp sudan suriname vp tanzania timor leste togo tonga trinidad tobago tuvalu uganda vanuatu zambia zimbabwe ademov adinolfi agea arena barekov bay bearder beňová campbell bannerman casa christensen ciocca cornillet corrao czesak vp dance delahaye d’ornano engström estaràs ferragut ferrara ferreira vp florenz gal gardiazabal rubial gericke geringer de oedenberg giuffrida goerens griesbeck guerrero salom hannan vp hetman heubuch iturgaiz karski kyenge vip lancini lopez aguilar vip lösing mcavan manscour vip marusik michel mizzi muselier vip mussolini nart vp neuser noichl omarjee papadimoulis pedicini vp pogliese popa preuss punset rangel vip rivasi vp rolin rosati sargentini schaffhauser schreijer pierik senra rodríguez stolojan thomas vaidere valero wenta werner wieland wiśniewska vp záborská zeller zorrinho zovko zwiefka committee political affairs acp members ep members katumwa congo democratic republic co chair mali 2 vc goerens co chair zeller 1 vc guerrero salom 2 vc benin burkina faso burundi chad côte d’ivoire djibouti ethiopia fiji guyana haiti jamaica kenya kiribati lesotho liberia marshall islands nauru saint vincent grenadines sao tome et principe timor leste togo uganda zambia zimbabwe ademov adinolfi barekov casa chauprade corrao czesak dance engström gal karski kyenge lösing lópez aguilar michel pogliese preuss rangel valero werner wieland zorrinho zwiefka committee economic development finance trade acp members ep members parkies south africa co chair cape verde 1 vc comoros 2 vc ferrara co chair estaràs ferragut 1 vc manscour 2 vc angola barbados equatorial guinea gabon guinea guinea bissau eswatini kingdom madagascar mauritius namibia nigeria niue papua new guinea congo republic senegal seychelles solomon islands st kitts nevis st lucia suriname tanzania tonga trinidad tobago arena bay beňová campbell bannerman cornillet delahaye florenz griesbeck hannan mizzi muselier omarjee papadimoulis pedicini popa punset rosati sargentini schreijer pierik stolojan thomas zovko committee social affairs environment acp members ep members co chair eritrea 1 vc rivasi co chair agea 1 vc mussolini 2 vc antigua barbuda bahamas belize botswana cameroon central african republic cook islands dominica dominican republic gambia ghana grenada malawi mauritania micronesia federated states mozambique niger palau rwanda samoa sierra leone somalia sudan tuvalu vanuatu cuba 1 bearder christensen ciocca ferreira gardiazábal rubial gericke geringer de oedenberg giuffrida herranz garcía hetman heubuch marusik mcavan nart neuser noichl rolin senra rodríguez vaidere wenta wiśniewska zaborska 1 observer status annex ii record attendance session held brussels belgium 18 20 june 2018 kono cameroon co president michel co president xirimbimbi angola husbands barbados gbian benin mangole botswana ousmane burkina faso niebarikanuye burundi gberi cameroon yama central african republic weidou chad opimbat congo republic katumwa congo democratic republic lobognon côte d’ivoire abdallah ahmed djibouti pujals nolasco dominican republic naib eritrea wakjira ethiopia dlamini eswatini kingdom oyono bibang equatorial guinea sudhakar fiji milebou aubusson ép mboussou gabon sillah gambia banda ghana diallo guinea greenidge guyana cyprien haiti mayne jamaica cheboi kenya rapapa lesotho gray libera rakotonirina madagascar mhone malawi fowdar mauritius welly micronesia federated states sibungo namibia dioffo niger iriase nigeria rugema rwanda byron nisbett st kitts nevis straker saint vincent grenadines faumuina samoa cassandra correia sao tome principe ndiaye senegal andre seychelles sahr nyuma sierra leone parkies south africa ali nafie ahmed sudan misiekaba surinam kabakaki tanzania ibrahima togo brooke trinidad tobago pie simati tuvalu wamanga uganda licht vanuatu zindi zimbabwe arena barekov 1 2 bay 3 bearder clune 3 cornillet dance 2 diaz de mera 3 engel estaras 3 ferreira 3 gahler gambús 2 gericke 2 3 gill 2 3 guerrero saolom 1 hetman 2 3 heubuch 2 3 hölvényi jimenez becerril barrio kyenge 2 3 leinen lópez aguilar 1 lösing 2 manscour martin neuser 2 3 rivasi 2 3 rosati 2 3 schaffhauser 2 3 senra rodriguez serrão santos 2 sylikiotis 2 3 theocharous 2 3 vaidere 2 valero ward wenta wiśniewska 2 3 záborská 2 zeller zorrinho 1 2 zovko 2 3 zwiefka 2 3 also present angola jesus mukinda semedo tyova simba benin ahonoukoun akplogan botswana moreti mosinki matambo batshu ntongana burkina faso komditamde compaore kiemde lankoande burundi bigirimana njebarikanuye cameroon eteky mongue chad adji central african republic nzessioue congo republic ndoua otsala abondo congo drc basiala maka nzuzi mabaya mukendi kabambi cote d’ivoire lobognon coulibaly tanoe djibouti mohamed hamid daoud ahmed bourhan ali dominican republic puig eritrea hagos ethiopia birhanu eswatini henwood ronald equatorial guinea bang nvo fiji singh gabon mangouala tsiaba owono nguema joumas dit salamba mba alloumba yalis mngomezulu gambia ceesay sowe cardos madi ceesay ghana armah guinea sylla guyana hales haiti lumerant kenya kamar mbaya gogo kiteru rasso nthiwa wamalwa lesotho matobo moqolo ramothello bulane mokoaleli madagascar rakotonirina razafison malawi mwanyula makanda micronesia federated states alik mauritania samba mauritius mandary namibia venaani caley niger mahamane dille adamou abdouramane kore abdomaha nigeria frank usman lidani abdullahi olatunbosun yunusa audu amos ufouma samoa penn senegal seck balla lo diaw seychelles william sierra leone conteh emerson lamina aruna koroma south africa moodley sudan osman ali osman abaker ahmed mohamed fad eheesda suriname sharman samidin tanzania sokoine kabakaki togo tchaye abiguime azilan uganda kagoro nokrach ekitui oyet kinyamatama zambia ngalazi ngulube mubanga zimbabwe juru nduna acp council carl greenidge minister foreign affairs guyana president office acp council eu council ekaterina zaharieva deputy prime minister judicial reform minister foreign affairs bulgaria president office eu council european commission neven mimica member commission responsibility international cooperation development european external action service villalonga deputy head division africa 5 european economic social committee king acp secretariat gomes co secretary general eu secretariat fernandez fernandes acting co secretary general acting director 1 present 18 june 2018 2 present 19 june 2018 3 present 20 june 2018 top'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ce vzame cas popravi v metriki\n",
    "def change_dict_structure(dict_list):\n",
    "    \"\"\"Takes list of dicts from db_query and changes to dict with key=id, value = text (used for metrices).\"\"\"\n",
    "    texts = {}\n",
    "    for dic in dict_list:\n",
    "        doc_id = dic.get('document_id')\n",
    "        text = dic.get('fulltext_cleaned')\n",
    "        texts.update({doc_id: text})\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dict = change_dict_structure(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_dict.get(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply0 = dr.probability_score(tokenized,texts_dict, dr.probability_multiply,10)\n",
    "multiply1 = dr.probability_score(tokenized+ext,texts_dict, dr.probability_multiply,10)\n",
    "multiply2 = dr.probability_score(tokenized+ext+candidates,texts_dict, dr.probability_multiply,10)\n",
    "# takes also probability_function probability_sum_weight, but doesnt give final result (used in probability_score_sum_weights)\n",
    "    # top_expansion are the onesthat are not counted as original query, usually ext ((extension by summation of 2 consecutive words) counts as original query\n",
    "#     \"\"\"Assigns score to documents based on probability_function metric.\n",
    "#     Args:\n",
    "#         tokens (list): List of tokens (tokenized query). If needed also extension (extension by summation of 2 consecutive words and/or KNN candidates).\n",
    "#         texts (dict):  Keys represent document ids, values are document text. \n",
    "#         probability_function (function): Metric function that calculates document relavance. Functions: probability_multiply, probability_sum. Require only first 4 arguments.\n",
    "#         n (int): Number of returned tuples, sorted by highest scores.\n",
    "#         top_expansion (list): List of expanded words. Usually candidates (kNN expansion), then have to be also in tokens.\n",
    "#         alpha (float): Number between 0 and 1. Weight that emphasizes the difference between original query words and expansions. \n",
    "#                        For alpha 0.5 all words have same weights (but not same values!), for alpha 1 expansion words have value 0. \n",
    "#                        For alpha -1 values equal to cosine similarity to query words. \n",
    "#         wv (Word2VecKeyedVectors): Word embeddings.\n",
    "#     Returns:\n",
    "#         document_probability (list): Tuples of document ids and scores that measure document relavance. Returns n tuples with highest score.\n",
    "#     \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.probability_score_sum_weights(original_tokens, top_expansion, texts,n, alpha, wv)\n",
    "# \"\"\"As probability_score only weighted.\n",
    "#         Args:\n",
    "#         original_tokens(list): List of strings. Tokenized original query. Usually also extension (extension by summation of 2 consecutive words), but not top_expansion (kNN candidates)\n",
    "#         top_expansion (list): List of expanded words. Usually candidates (kNN expansion).\n",
    "#         texts (dict):  Keys represent document ids, values are document text.\n",
    "#         n (int): Number of returned tuples, sorted by highest scores.\n",
    "#         alpha (float): Number between 0 and 1. Weight that emphasizes the difference between original query words and expansions. \n",
    "#                        For alpha 0.5 all words have same weights (but not same values!), for alpha 1 expansion words have value 0. \n",
    "#                        For alpha -1 values equal to cosine similarity to query words. \n",
    "#         wv (Word2VecKeyedVectors): Word embeddings.\n",
    "#     Returns:\n",
    "#         document_score (list): Tuples of document ids and scores that measure document relavance. Returns n tuples with highest score.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.probability_score_sum_weights(tokenized+ext, candidates, texts_dict,5, 0.6, wv_wiki_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.tfidf_score(tokens, texts, tfidf_function,n, *args)\n",
    "#     \"\"\"Assigns score to documents based on tfidf_function metric.\n",
    "#     Args:\n",
    "#         tokens (list): List of tokens (tokenized query). If needed also extension (extension by summation of 2 consecutive words).For tfidf_sum metric can also add kNN candidates.\n",
    "#         texts (dict):  Keys represent document ids, values are document text. \n",
    "#         probability_function (function): Metric function that calculates document relavance. Functions: tfidf_sum; require only first 4 arguments, tfidf_sum_weight; require all arguments.\n",
    "#         n (int): Number of returned tuples, sorted by highest scores.\n",
    "#         top_expansion (list): List of expanded words. Usually candidates (kNN expansion).\n",
    "#         alpha (float): Number between 0 and 1. Weight that emphasizes the difference between original query words and expansions. \n",
    "#                        For alpha 0.5 all words have same weights (but not same values!), for alpha 1 expansion words have value 0. \n",
    "#                        For alpha -1 values equal to cosine similarity to query words. \n",
    "#         wv (Word2VecKeyedVectors): Word embeddings.\n",
    "#     Returns:\n",
    "#         document_probability (list): Tuples of document ids and scores that measure document relavance. Returns n tuples with highest score.\n",
    "#         not_appear (list): List of words that did not occure in any document.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2841923236846924\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dr.tfidf_score(tokenized+ext, texts_dict, dr.tfidf_sum,10)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['underground', 'fish', 'water', 'pollution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list= ['seawater',\n",
    " 'fishwater',\n",
    " 'groundwaters',\n",
    " 'sewage',\n",
    " 'groundwater','potable',\n",
    " 'undergrounding',\n",
    " 'undergrounders',\n",
    " 'ρwater',\n",
    " 'wastewater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dcs = db_query(tokens) \n",
    "document_list = change_dict_structure(dcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.35601806640625\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dr.tfidf_score(tokens+test_list, document_list, dr.tfidf_sum,10)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.60514283180237\n"
     ]
    }
   ],
   "source": [
    "start2 = time.time()\n",
    "dr.tfidf_score(tokenized+ext, texts_dict, dr.tfidf_sum_weight,10, candidates, 0.6, wv_wiki_en)\n",
    "end2 = time.time()\n",
    "print(end2 - start2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_score_str(tokens,texts,tfidf_function_name,m,*args):\n",
    "    \"\"\"Takes function name as input, returns function\"\"\"\n",
    "    if tfidf_function_name == 'tfidf_sum':\n",
    "            return dr.tfidf_score(tokens,texts,dr.tfidf_sum,m)\n",
    "    else:\n",
    "        print(\"Error, different fn name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8638277053833008\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc = tfidf_score_str(tokenized, texts_dict, \"tfidf_sum\",10)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "# s stevilom besed (+ext) se cas poveca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000091525, 0.00826585345771757),\n",
       " (1000097317, 0.0075142127850723614),\n",
       " (1000094934, 0.007331187304515795),\n",
       " (1000000045, 0.006758104887364456),\n",
       " (1000003451, 0.006355128936113985),\n",
       " (1000093531, 0.006151154273753995),\n",
       " (1000059173, 0.005921035429548822),\n",
       " (1000097318, 0.005864949843612636),\n",
       " (1000094663, 0.005702892018986497),\n",
       " (1000022091, 0.005608434180162704)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results from metric functions to dictionary\n",
    "# def results_to_dictionary(results):\n",
    "#     results_dict = {}\n",
    "#     for tupl in results:\n",
    "#         results_dict.update({tupl[0]:tupl[1]})\n",
    "#     return(results_dict)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREC setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/Users/sarab/work/try/TREC/processed_data/wFT/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-f916a314581f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/sarab/work/try/TREC/processed_data/wFT/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\wFT\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/Users/sarab/work/try/TREC/processed_data/wFT/'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# import documents\n",
    "directory = '/Users/sarab/work/try/TREC/processed_data/wFT/' \n",
    "text = {}\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\wFT\\\\'+filename, 'r') as file:\n",
    "        for line in file: \n",
    "            dicti = json.loads(line)\n",
    "            text = {**text, **dicti}\n",
    "# import questions\n",
    "topics = {} \n",
    "with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\titles_topics', 'r') as file:\n",
    "        for line in file: \n",
    "            dicti = json.loads(line)\n",
    "            topics = {**topics, **dicti}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates pytrec_eval form of results and writes a file\n",
    "# args: name - name of dictionary, we are creating\n",
    "#       topics_dict  - dict of question numbers and questions\n",
    "#       results - metric results we want to convert\n",
    "def make_dict(topics_dict):\n",
    "    main_dicti = {}\n",
    "    for key in topics_dict:\n",
    "        main_dicti[key] = {}\n",
    "    return main_dicti\n",
    "\n",
    "def store_to_dict(this_key, results, main_dicti):\n",
    "    for tupl in results:\n",
    "        main_dicti[this_key][tupl[0]] = tupl[1]\n",
    "    return main_dicti\n",
    "        \n",
    "def write_dict(name,main_dicti):  \n",
    "    file = open(name,\"w+\") \n",
    "    dict_to_write = json.dumps(main_dicti)\n",
    "    file.write(dict_to_write +\"\\n\")\n",
    "    file.close() \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(name, this_key,topics_dict,results):\n",
    "    try:\n",
    "        \n",
    "        with open(name, 'r+') as file:\n",
    "            for line in file: \n",
    "                dicti = json.loads(line)\n",
    "               \n",
    "            for tupl in results:\n",
    "                dicti[this_key][tupl[0]] = tupl[1]\n",
    "\n",
    "            dict_to_write = json.dumps(dicti)\n",
    "            file.seek(0)\n",
    "            file.write(dict_to_write +\"\\n\")\n",
    "            file.truncate()\n",
    "            file.close() \n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "    except:\n",
    "        main_dicti = {}\n",
    "        for key in topics_dict:\n",
    "            main_dicti[key] = {}\n",
    "        for tupl in results:\n",
    "            main_dicti[this_key][tupl[0]] = tupl[1]\n",
    "    \n",
    "        file = open(name,\"w+\") \n",
    "        dict_to_write = json.dumps(main_dicti)\n",
    "        file.write(dict_to_write +\"\\n\")\n",
    "        file.close() \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing metrics  \n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "main_dicti = {}\n",
    "for key in topics:\n",
    "    main_dicti[key] = {}\n",
    "\n",
    "for key in topics:\n",
    "    print(key)\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "    multiply0 = dr.probability_score(tokenized,text, dr.probability_multiply,0)\n",
    "    for tupl in multiply0:\n",
    "        main_dicti[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "file = open('multiply0',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    " \n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "        \n",
    "    multiply1 = dr.probability_score(tokenized+ext,text, dr.probability_multiply,0)\n",
    "    for tupl in multiply1:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    multiply2 = dr.probability_score(tokenized+ext+ext_candidates,text, dr.probability_multiply,0)\n",
    "    for tupl in multiply2:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    multiply3 = dr.probability_score(tokenized+candidates,text, dr.probability_multiply,0)\n",
    "    for tupl in multiply3:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "file = open('t_multiply1',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_multiply2',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_multiply3',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    sum0 = dr.probability_score(tokenized,text, dr.probability_sum,0)\n",
    "    for tupl in sum0:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    sum1 = dr.probability_score(tokenized+ext,text, dr.probability_sum,0)\n",
    "    for tupl in sum1:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    sum2 = dr.probability_score(tokenized+ext+ext_candidates,text, dr.probability_sum,0)\n",
    "    for tupl in sum2:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    sum3 = dr.probability_score(tokenized+candidates,text, dr.probability_sum,0)\n",
    "    for tupl in sum3:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "file = open('t_sum0',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('t_sum1',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_sum2',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_sum3',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3722.501778125763\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "  \n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "    \n",
    "    tfidf_sum0 = dr.tfidf_score(tokenized, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_sum0:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    tfidf_sum1 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_sum1:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_sum2 = dr.tfidf_score(tokenized+ext+ext_candidates, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_sum2:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_sum3 = dr.tfidf_score(tokenized+candidates, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_sum3:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "file = open('t_tfidf_sum0',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('t_tfidf_sum1',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_t_tfidf_sum2',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_sum3',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7318.692469835281\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "main_dicti4 = {}\n",
    "for key in topics:\n",
    "    main_dicti4[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    wsum01 = dr.probability_score_sum_weights(tokenized, candidates, text,0, -1, wv_wiki_en)\n",
    "    for tupl in wsum01:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum006 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.6, wv_wiki_en)\n",
    "    for tupl in wsum006:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum007 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.7, wv_wiki_en)\n",
    "    for tupl in wsum007:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum008 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.8, wv_wiki_en)\n",
    "    for tupl in wsum008:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum009 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.9, wv_wiki_en)\n",
    "    for tupl in wsum009:\n",
    "        main_dicti4[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "        \n",
    "file = open('t_wsum01',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('t_wsum006',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum007',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum008',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum009',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti4)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8254.605898857117\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "main_dicti4 = {}\n",
    "for key in topics:\n",
    "    main_dicti4[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    wsum11 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, -1, wv_wiki_en)\n",
    "    for tupl in wsum11:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum106 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, 0.6, wv_wiki_en)\n",
    "    for tupl in wsum106:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum107 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, 0.7, wv_wiki_en)\n",
    "    for tupl in wsum107:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum108 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, 0.8, wv_wiki_en)\n",
    "    for tupl in wsum108:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum109 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, 0.9, wv_wiki_en)\n",
    "    for tupl in wsum109:\n",
    "        main_dicti4[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "        \n",
    "file = open('t_wsum11',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('t_wsum106',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum107',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum108',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_wsum109',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti4)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "22787/3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "main_dicti4 = {}\n",
    "for key in topics:\n",
    "    main_dicti4[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "   # la = time.time()\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    #ll = time.time()\n",
    "\n",
    "#print(ll - la)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    tfidf_wsum01 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,-1,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum01:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    end = time.time()\n",
    "    #print(end - start)\n",
    "\n",
    "    tfidf_wsum006 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,0.6,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum006:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "\n",
    "    tfidf_wsum007 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,0.7,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum007:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "\n",
    "    tfidf_wsum008 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,0.8,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum008:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "\n",
    "    tfidf_wsum009 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,0.9,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum009:\n",
    "        main_dicti4[key][tupl[0]] = tupl[1]\n",
    "\n",
    "\n",
    "file = open('t_tfidf_wsum01',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "        \n",
    "file = open('t_tfidf_wsum006',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum007',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum008',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum009',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti4)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.009*210158/3600 #cas za eno vprasanje "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ea4fbf311296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmain_dicti0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtupl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtupl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtfidf_wsum106\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf_sum_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mext_candidates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwv_wiki_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtupl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtfidf_wsum106\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmain_dicti1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtupl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtupl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\work\\try\\enviroLENS\\word-embeddings\\modules\\library\\documentRetrieval.py\u001b[0m in \u001b[0;36mtfidf_score\u001b[1;34m(tokens, texts, tfidf_function, m, *args)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtfidf_function\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtfidf_sum_weight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                     \u001b[0mprobability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_sum_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtoken_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mappear\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error, number of arguments does not match\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\work\\try\\enviroLENS\\word-embeddings\\modules\\library\\documentRetrieval.py\u001b[0m in \u001b[0;36mtfidf_sum_weight\u001b[1;34m(probability, token_frequency, n, idf, word, alpha, original_tokens, top_expansion, wv)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mtfidf_value\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNew\u001b[0m \u001b[0mcaclculated\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mtfidf_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_frequency\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mword_value\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_expansion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\work\\try\\enviroLENS\\word-embeddings\\modules\\library\\documentRetrieval.py\u001b[0m in \u001b[0;36mword_value\u001b[1;34m(word, alpha, original_tokens, top_expansion, wv)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0msum_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexp_token\u001b[0m \u001b[1;32min\u001b[0m \u001b[0monly_expanded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0msum_similarity\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moriginal_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\work\\try\\enviroLENS\\word-embeddings\\modules\\library\\documentRetrieval.py\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(token, token_list, wv)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mnum_of_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0msimilarity\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mavreage_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_of_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(self, w1, w2)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \"\"\"\n\u001b[1;32m--> 828\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "main_dicti4 = {}\n",
    "for key in topics:\n",
    "    main_dicti4[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "    #print(key)\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    tfidf_wsum11 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,-1,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum11:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_wsum106 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,0.6,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum106:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_wsum107 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,0.7,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum107:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_wsum108 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,0.8,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum108:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_wsum109 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,0.9,wv_wiki_en)\n",
    "    for tupl in tfidf_wsum109:\n",
    "        main_dicti4[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "        \n",
    "file = open('t_tfidf_wsum11',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('t_tfidf_wsum106',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum107',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum108',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('t_tfidf_wsum109',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti4)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing optimal k and n for query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "\n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "\n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "  \n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    candidates10 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 10, stopWords,extension=False)\n",
    "#     candidates20 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 20, stopWords,extension=False)\n",
    "#     candidates30 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 30, stopWords,extension=False)\n",
    "#    candidates = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    \n",
    "#     tfidf_test = dr.tfidf_score(tokenized+candidates, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_test:\n",
    "#         main_dicti3[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    tfidf_cand10 = dr.tfidf_score(tokenized+candidates10, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_cand10:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "#     tfidf_cand20 = dr.tfidf_score(tokenized+candidates20, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_cand20:\n",
    "#         main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "#     tfidf_cand30 = dr.tfidf_score(tokenized+candidates30, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_cand30:\n",
    "#         main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "# file = open('tfidf_test',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti3)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()    \n",
    "        \n",
    "file = open('tfidf_cand10',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "# file = open('tfidf_cand20',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti1)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "# file = open('tfidf_cand30',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti2)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing metrics  \n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for key in ['301','302','303','304','305']:\n",
    "    print(key)\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "    multiply0 = dr.probability_score(tokenized,text, dr.probability_multiply,0)\n",
    "    multiply0 = to_dict('multiply0',key, topics, multiply0)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    multiply1 = dr.probability_score(tokenized+ext,text, dr.probability_multiply,0)\n",
    "    multiply1 = to_dict('multiply1',key, topics, multiply1)\n",
    "    \n",
    "    multiply2 = dr.probability_score(tokenized+ext+ext_candidates,text, dr.probability_multiply,0)\n",
    "    multiply2 = to_dict('multiply2',key, topics, multiply2)\n",
    "    \n",
    "    multiply3 = dr.probability_score(tokenized+candidates,text, dr.probability_multiply,0)\n",
    "    multiply3 = to_dict('multiply3',key, topics, multiply3)\n",
    "    \n",
    "    sum0 = dr.probability_score(tokenized,text, dr.probability_sum,0)\n",
    "    sum0 = to_dict('sum0',key, topics, sum0)\n",
    "    \n",
    "    sum1 = dr.probability_score(tokenized+ext,text, dr.probability_sum,0)\n",
    "    sum1 = to_dict('sum1',key, topics, sum1)\n",
    "    \n",
    "    sum2 = dr.probability_score(tokenized+ext+ext_candidates,text, dr.probability_sum,0)\n",
    "    sum2 = to_dict('sum2',key, topics, sum2)\n",
    "    \n",
    "    sum3 = dr.probability_score(tokenized+candidates,text, dr.probability_sum,0)\n",
    "    sum3 = to_dict('sum3',key, topics, sum3)\n",
    "    \n",
    "    for alpha in [-1, 0.6, 0.7, 0.8, 0.9]:\n",
    "        wsum0 = dr.probability_score_sum_weights(tokenized, candidates, text,0, alpha, wv_wiki_en)\n",
    "        wsum0 = to_dict('wsum0'+str(alpha),key, topics, wsum0)\n",
    "        \n",
    "        wsum1 = dr.probability_score_sum_weights(tokenized+ext, ext_candidates, text,0, alpha, wv_wiki_en)\n",
    "        wsum1 = to_dict('wsum1'+str(alpha),key, topics, wsum1)\n",
    "        \n",
    "    tfidf_sum0 = dr.tfidf_score(tokenized, text, dr.tfidf_sum,0)\n",
    "    tfidf_sum0 = to_dict('tfidf_sum0',key, topics, tfidf_sum0)\n",
    "    \n",
    "    tfidf_sum1 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum,0)\n",
    "    tfidf_sum1 = to_dict('tfidf_sum1',key, topics, tfidf_sum1)\n",
    "    \n",
    "    tfidf_sum2 = dr.tfidf_score(tokenized+ext+ext_candidates, text, dr.tfidf_sum,0)\n",
    "    tfidf_sum2 = to_dict('tfidf_sum2',key, topics, tfidf_sum2)\n",
    "    \n",
    "    tfidf_sum3 = dr.tfidf_score(tokenized+candidates, text, dr.tfidf_sum,0)\n",
    "    tfidf_sum3 = to_dict('tfidf_sum3',key, topics, tfidf_sum3)\n",
    "    \n",
    "    for alpha in [-1, 0.6, 0.7, 0.8, 0.9]:\n",
    "        tfidf_wsum0 = dr.tfidf_score(tokenized, text, dr.tfidf_sum_weight,0,candidates,alpha,wv_wiki_en)\n",
    "        tfidf_wsum0 = to_dict('tfidf_wsum0'+str(alpha),key, topics, tfidf_wsum0)\n",
    "        \n",
    "        tfidf_wsum1 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum_weight,0,ext_candidates,alpha,wv_wiki_en)\n",
    "        tfidf_wsum1 = to_dict('tfidf_wsum1'+str(alpha),key, topics, tfidf_wsum1)\n",
    "   \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '305'\n",
    "tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "multiply0 = dr.probability_score(tokenized,text, dr.probability_sum,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(multiply0))\n",
    "print(multiply0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply0 = to_dict('multiply0',key, topics, multiply0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply0.get('305')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# za mac\n",
    "import json\n",
    "def import_file(file_path, file_name):\n",
    "    file_name = {} \n",
    "    with open(file_path, 'r') as file:\n",
    "            for line in file: \n",
    "                dicti = json.loads(line)\n",
    "                file_name = {**file_name, **dicti}\n",
    "\n",
    "            \n",
    "            \n",
    "def evaluate(file_path,file_name, qrel):\n",
    "    import_file(file_path, file_name)\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map', 'ndcg'})\n",
    "    file = open(\"written_\"+str(file_name),\"w+\") \n",
    "    dict_to_write = json.dumps(evaluator.evaluate(run), indent=1)\n",
    "    file.write(dict_to_write +\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that convert data into pytrec_eval form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents (TREC disk 4 FT)\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"FT924\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-4/FT/' + dir_name\n",
    "directory_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-4\\\\FT\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file\n",
    "        xml = f.read()\n",
    " \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "\n",
    "    root = ElementTree.fromstring(xml)\n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    count = 1\n",
    "    for doc in root:\n",
    "       # print(doc)\n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        try:\n",
    "            doc_json = {doc.find('DOCNO').text.strip(): doc.find('TEXT').text.strip()}\n",
    "        except:\n",
    "            print(doc.find('DOCNO').text.strip(), count)\n",
    "            count = count +1\n",
    "            doc_json = {doc.find('DOCNO').text.strip(): 'empty'}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "    \n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents (TREC disk 4 FT)\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"01\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-4/FR94/' + dir_name\n",
    "directory_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-4\\\\FR94\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file\n",
    "        xml = f.read()\n",
    " \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "\n",
    "    root = ElementTree.fromstring(xml)\n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    count = 1\n",
    "    for doc in root:\n",
    "       # print(doc)\n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        try:\n",
    "            doc_json = {doc.find('DOCNO').text.strip(): doc.find('TEXT').text.strip()}\n",
    "        except:\n",
    "            print(doc.find('DOCNO').text.strip(), count)\n",
    "            count = count +1\n",
    "            doc_json = {doc.find('DOCNO').text.strip(): 'empty'}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "    \n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents (TREC disk 4 and 5)\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"F\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-5/FBIS/' + dir_name\n",
    "directory_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-5\\\\FBIS\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file\n",
    "        xml = f.read()\n",
    "    parser = ElementTree.XMLParser(encoding=\"utf-8\")\n",
    "    \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "    \n",
    "    root = ElementTree.fromstring(xml, parser = parser)\n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    for doc in root:\n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        doc_json = {doc.find('DOCNO').text.strip(): doc.text.strip()}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "    \n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"L\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-5/LATIMES/' + dir_name\n",
    "directory_dict = {}\n",
    "count = 1\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-5\\\\LATIMES\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file\n",
    "        xml = f.read()\n",
    " \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "\n",
    "    root = ElementTree.fromstring(xml)\n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    \n",
    "    for doc in root:\n",
    "        print(root[0][1].text)\n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        \n",
    "        doc_json = {doc.find('DOCNO').text.strip(): doc.text} #doc.attrib}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "    \n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delujoca\n",
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"L\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-5/LATIMES/' + dir_name\n",
    "directory_dict = {}\n",
    "count = 1\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-5\\\\LATIMES\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file\n",
    "        xml = f.read()\n",
    " \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "\n",
    "    root = ElementTree.fromstring(xml)\n",
    "    \n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    \n",
    "    for doc in root:\n",
    "        \n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        \n",
    "        doc_json = {doc.find('DOCNO').text.strip(): ' '.join(doc.itertext()).replace('\\n','')} #doc.attrib}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "\n",
    "\n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delujoca\n",
    "import json\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "dir_name = \"F\"\n",
    "directory = '/Users/sarab/work/try/TREC/TREC-Disk-5/FBIS/' + dir_name\n",
    "directory_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\TREC-Disk-5\\\\FBIS\\\\'+dir_name+'\\\\'+filename, 'r') as f:   # Reading file Reading file\n",
    "        xml = f.read()\n",
    " \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # Let's add a root tag\n",
    "\n",
    "    root = ElementTree.fromstring(xml)\n",
    "    \n",
    "\n",
    "    # Simple loop through each document\n",
    "    docs_json = {}\n",
    "    \n",
    "    for doc in root:\n",
    "        \n",
    "        #doc_json = {'DOCNO': doc.find('DOCNO').text.strip(), 'TEXT': doc.find('TEXT').text.strip()}\n",
    "        \n",
    "        doc_json = {doc.find('DOCNO').text.strip(): ' '.join(doc.itertext()).replace('\\n','')} #doc.attrib}\n",
    "        docs_json.update(doc_json)\n",
    "    directory_dict = {**directory_dict, **docs_json}\n",
    "    continue\n",
    "\n",
    "\n",
    "file = open(\"written_\"+dir_name,\"w+\") \n",
    "dict_to_write = json.dumps(directory_dict)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(directory_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions (300 - 450 ad hoc)\n",
    "import re\n",
    "directory = '/Users/sarab/work/try/TREC/topics/' \n",
    "#directory_list = []\n",
    "question_dictionary = {}\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\topics\\\\'+filename) as f:\n",
    "        file = f.read()\n",
    "        numbers = re.findall('<num>\\sNumber:\\s\\d+',file,re.MULTILINE)\n",
    "        questions = re.findall('<title>\\s(.*?)<desc>',file,re.DOTALL)\n",
    "        for i in range(len(numbers)):\n",
    "            number = re.search('\\d+', numbers[i])\n",
    "            number = number.group(0)\n",
    "            question = questions[i].replace('\\n','')\n",
    "            question_dictionary.update({number: question})\n",
    "\n",
    "file = open(\"titles_topics\",\"w+\") \n",
    "dict_to_write = json.dumps(question_dictionary)\n",
    "file.write(dict_to_write)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# qrels for questions 300 - 450\n",
    "\n",
    "import re\n",
    "directory = '/Users/sarab/work/try/TREC/qrels/' \n",
    "tuple_list= []\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\qrels\\\\'+filename) as f:\n",
    "        file = f.read()\n",
    "       # questions = re.findall(r'(\\d+)\\s0\\s.+\\s\\d\\n',file)\n",
    "        documents = re.findall(r'\\d+\\s0\\s(.+)\\s\\d\\n',file)\n",
    "        relavances = re.findall(r'(\\d+)\\s0\\s.+\\s(\\d)\\n',file)\n",
    "        if len(relavances) == len(documents) :\n",
    "            for i in range(len(relavances)):\n",
    "                tuple_list.append((relavances[i][0], documents[i], relavances[i][1]))\n",
    "                \n",
    "        else:\n",
    "            print('Error')\n",
    "            \n",
    "#write in form for pytrec_eval:\n",
    "main_dict = {}\n",
    "for nb in range(301,451):\n",
    "    main_dict[nb] = {}\n",
    "for tupl in tuple_list:\n",
    "    main_dict[int(float(tupl[0]))][tupl[1]] = tupl[2]\n",
    "\n",
    "file = open(\"qrels\",\"w+\") \n",
    "dict_to_write = json.dumps(main_dict)\n",
    "file.write(dict_to_write)\n",
    "file.close()\n",
    "    \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(json.JSONDecoder):\n",
    "    def decode(self, s):\n",
    "        result = super().decode(s)  # result = super(Decoder, self).decode(s) for Python 2.x\n",
    "        return self._decode(result)\n",
    "\n",
    "    def _decode(self, o):\n",
    "        if isinstance(o, str):\n",
    "            try:\n",
    "                return int(o)\n",
    "            except ValueError:\n",
    "                return o\n",
    "        elif isinstance(o, dict):\n",
    "            return {k: self._decode(v) for k, v in o.items()}\n",
    "        elif isinstance(o, list):\n",
    "            return [self._decode(v) for v in o]\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_path):\n",
    "    file_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            dicti = json.loads(line, cls=Decoder)\n",
    "            file_dict = {**file_dict, **dicti}\n",
    "    return file_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qrel = import_file(\"/Users/sarab/work/try/TREC/processed_data/qrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATIMES = import_file(\"/Users/sarab/work/try/enviroLENS/word-embeddings/notebooks/written_L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATIMES.get(\"LA010189-0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel.get(\"301\").get('CR93E-10279')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysing TREC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "directory = \"/Users/sarab/work/try/TREC/processed_data/eval_results\"\n",
    "map_results = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    map_results[filename] = {}\n",
    "    q301_350 = 0\n",
    "    q351_400 = 0\n",
    "    q401_450 = 0\n",
    "    map_val = 0\n",
    "    count = 0\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "\n",
    "    with open(directory + '/' + filename, 'r') as file:\n",
    "        results = json.loads(file.read(), cls=Decoder)\n",
    "    for key,value in results.items():\n",
    "        key = int(key)\n",
    "        val = value.get(\"map\")\n",
    "        if 301 <= key <=350:\n",
    "            c1 += 1\n",
    "            q301_350 = q301_350 + val\n",
    "                \n",
    "        elif 351 <= key <=400:\n",
    "            c2 += 1\n",
    "            q351_400 = q351_400 + val\n",
    "            \n",
    "        elif 401 <= key <=450:\n",
    "            c3 += 1\n",
    "            q401_450 = q401_450 + val\n",
    "            \n",
    "        else:\n",
    "            print('error')\n",
    "            \n",
    "        map_val = map_val + val\n",
    "        count += 1\n",
    "    map_final = map_val / count\n",
    "    q301_350 = q301_350 / c1\n",
    "    q351_400 = q351_400 / c2\n",
    "    q401_450 = q401_450 / c3\n",
    "    map_results[filename]['q301_450'] = map_final\n",
    "    map_results[filename]['q301_350'] = q301_350\n",
    "    map_results[filename]['q351_400'] = q401_450\n",
    "    map_results[filename]['q401_450'] = q401_450\n",
    "    if c1 !=50 or c2 !=50 or c1 !=50 or count!=150 :\n",
    "        print('error_c')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame.from_dict(map_results,'index')\n",
    "df.sort_values(by=['q301_450'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
