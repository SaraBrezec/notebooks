{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion\n",
    "### Using FastText Word Embedding\n",
    "Based on this paper: https://arxiv.org/pdf/1606.07608.pdf\n",
    "\n",
    "Pre-made vector models: https://fasttext.cc/docs/en/aligned-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:16.676369Z",
     "start_time": "2019-06-12T13:31:15.880009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "\n",
    "# import natural language toolkit\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:16.676369Z",
     "start_time": "2019-06-12T13:31:15.880009Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare stopword list\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'document_embeddings.ipynb',\n",
       " 'enviroLENS-deliverable-D4.2-images.ipynb',\n",
       " 'query-expansion.ipynb',\n",
       " 'query_modules.ipynb',\n",
       " 'testing db functions.ipynb',\n",
       " 'testing.ipynb',\n",
       " 'titles_topics',\n",
       " 'trectesting2.ipynb',\n",
       " 't_tfidf_wsum006',\n",
       " 't_tfidf_wsum007',\n",
       " 't_tfidf_wsum008',\n",
       " 't_tfidf_wsum009',\n",
       " 't_tfidf_wsum01',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T07:16:14.874574Z",
     "start_time": "2019-06-12T07:06:52.184229Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words 2519370\n"
     ]
    }
   ],
   "source": [
    "wiki_en_align = './../data/fasttext/wiki.en.align.vec' #'../../data/fasttext/wiki.en.align.vec'\n",
    "# get fasttext wiki embeddings for english\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n",
    "print('english words {}'.format(len(list(wv_wiki_en.vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv_wiki_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-retrieval kNN Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:17.961109Z",
     "start_time": "2019-06-12T13:31:17.954389Z"
    }
   },
   "outputs": [],
   "source": [
    "#list of terms\n",
    "def tokenize(text, stopwords):\n",
    "    \"\"\"Tokenizes and removes stopwords from the document\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [w.lower() for w in tokens if not w in stopwords]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extended list of terms ###\n",
    "def extend_tokens(token_list, wv):\n",
    "    \"\"\"Extends token list summing vector pairs\"\"\"\n",
    "    tokens = []\n",
    "    for token in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            tokens.append(token)\n",
    "    extention = set()\n",
    "    for i in range(len(tokens)-1):\n",
    "        new_token = wv.most_similar(positive=[tokens[i], tokens[i+1]])[0][0]\n",
    "        extention.add(new_token)\n",
    "    extention = list(extention)\n",
    "    return extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'pollution', 'underground']\n",
      "['undergrounding', 'pollutions']\n"
     ]
    }
   ],
   "source": [
    "test = tokenize('water pollution underground', stop_words)\n",
    "print(test)\n",
    "ext = extend_tokens(test,wv_wiki_en)\n",
    "print(ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annex', 'fishing', 'agreement', 'europe']\n",
      "['agreements', 'flwfishing']\n"
     ]
    }
   ],
   "source": [
    "test1 = tokenize('annex fishing agreement europe', stop_words)\n",
    "print(test1)\n",
    "ext1 = extend_tokens(test1,wv_wiki_en)\n",
    "print(ext1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:18.473214Z",
     "start_time": "2019-06-12T13:31:18.465157Z"
    }
   },
   "outputs": [],
   "source": [
    "# knn nearest\n",
    "def get_candidate_expansion_terms(tokens, k, wv):\n",
    "    \"\"\"Gets the candidate expansion terms\"\"\"\n",
    "    candidates = set()\n",
    "    for token in tokens:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            result = wv.similar_by_word(token)\n",
    "            limit = k if len(result) > k else len(result)\n",
    "            # iterate through the most similar words\n",
    "            for i in range(limit):\n",
    "                candidates.add(result[i][0])\n",
    "    # return list of candidates\n",
    "    candidates = list(candidates)\n",
    "    return candidates\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baitfish', 'fishes', 'milkfishes', 'gamefish', 'shellfishes']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidate_expansion_terms([\"fish\"], 5, wv_wiki_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:31:20.226899Z",
     "start_time": "2019-06-12T13:31:19.569959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['undergrounded', 'undergrounder', 'sewage', 'undergrounding', 'pollution,', 'undergrounds', 'undergrounders', '#pollution', 'water—', 'undergroung', 'seawater', 'biopollution', 'potable', 'groundwater', 'pollutions', 'pollution', 'earpollution', 'undergroun', 'pollutants']\n",
      "['undergrounded', '#pollution', 'water—', 'sewage', 'groundwater', 'undergroung', 'pollutions', 'undergrounding', 'seawater', 'undergrounds', 'earpollution', 'undergroun', 'biopollution', 'potable', 'pollution,']\n"
     ]
    }
   ],
   "source": [
    "candidates = get_candidate_expansion_terms(test+ext, 5, wv_wiki_en)\n",
    "print(candidates)\n",
    "witout = get_candidate_expansion_terms(test, 5, wv_wiki_en)\n",
    "print(witout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between word and list of words\n",
    "def similarity(token, token_list, wv ):\n",
    "    \"\"\"calculates the similarity between word and list of words\"\"\"\n",
    "    # calculate the similarity of the token to all tokens\n",
    "    similarity = 0\n",
    "    num_of_tokens = 0\n",
    "    for toks in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if toks in wv.vocab.keys():\n",
    "            num_of_tokens += 1\n",
    "            similarity += wv.similarity(toks, token)\n",
    "    return similarity/num_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:13.725242Z",
     "start_time": "2019-06-12T13:33:13.716880Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculates similarity and sorts\n",
    "def get_top_expansion_terms(tokens, candidates, wv):\n",
    "    \"\"\"Gets the actual expansion terms\"\"\"\n",
    "    similarity_pairs = []\n",
    "    for candidate in candidates:\n",
    "        sim = similarity(candidate, tokens, wv)\n",
    "        similarity_pairs.append((candidate, sim))\n",
    "    # return the list of expansion terms with their similarities\n",
    "    return similarity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:14.317627Z",
     "start_time": "2019-06-12T13:33:14.308832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pollution', 0.6260276407951795), ('pollutions', 0.5989783010567384), ('undergrounding', 0.5868486694404182), ('earpollution', 0.5791309410113682), ('pollution,', 0.5584437076938438), ('pollutants', 0.5473505877035749), ('groundwater', 0.5472185974670385), ('sewage', 0.5438533174483885), ('undergrounds', 0.5373294534380543), ('#pollution', 0.5307803259871704), ('biopollution', 0.5239432296935493), ('undergrounded', 0.5166503920961191), ('undergrounders', 0.5048834686641379), ('undergroun', 0.4968187167322256), ('undergrounder', 0.4930006599112927), ('undergroung', 0.489668825408463), ('seawater', 0.48062588166948944), ('potable', 0.47213486261999227), ('water—', 0.4339509338693007)]\n",
      "[('pollution', 0.6073097696558459), ('groundwater', 0.572225614085378), ('sewage', 0.5672777675468184), ('earpollution', 0.5479508482970045), ('pollutions', 0.5392823009654663), ('pollution,', 0.5361565112564385), ('pollutants', 0.5332377192240293), ('seawater', 0.5219145986771884), ('undergrounding', 0.5190662482715993), ('potable', 0.5137930541544308), ('#pollution', 0.5053773015402238), ('biopollution', 0.504703593451706), ('undergrounds', 0.48430795952141487), ('water—', 0.47646895745070106), ('undergrounded', 0.46550011202029024), ('undergrounders', 0.4524525797301971), ('undergroun', 0.4511787727249055), ('undergroung', 0.4484747924647392), ('undergrounder', 0.44571173169707395)]\n"
     ]
    }
   ],
   "source": [
    "# get actual expansion terms for test set; with and without extension\n",
    "top = get_top_expansion_terms(test+ext, candidates,  wv_wiki_en)\n",
    "topwithout = get_top_expansion_terms(test, candidates,  wv_wiki_en)\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "top = sorted(top, key=takeSecond)[::-1]\n",
    "topw = sorted(topwithout, key=takeSecond)[::-1]\n",
    "print((top))\n",
    "print((topw))\n",
    "top = top[0:5]\n",
    "topw = topw[0:5]\n",
    "top_list = []\n",
    "for tupl in top:\n",
    "    top_list.append(tupl[0])\n",
    "topw_list = []\n",
    "for tupl in topw:\n",
    "    topw_list.append(tupl[0])\n",
    "\n",
    "# get actual expansion terms for test1 set; with and without extension\n",
    "top1 = get_top_expansion_terms(test1+ext1, candidates,  wv_wiki_en)\n",
    "topwithout1 = get_top_expansion_terms(test1, candidates,  wv_wiki_en)\n",
    "\n",
    "top1 = sorted(top1, key=takeSecond)[::-1]\n",
    "topw1 = sorted(topwithout1, key=takeSecond)[::-1]\n",
    "top1 = top1[0:5]\n",
    "topw1 = topw1[0:5]\n",
    "top_list1 = []\n",
    "for tupl in top1:\n",
    "    top_list1.append(tupl[0])\n",
    "topw_list1 = []\n",
    "for tupl in topw1:\n",
    "    topw_list1.append(tupl[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:17.453260Z",
     "start_time": "2019-06-12T13:33:17.445772Z"
    }
   },
   "outputs": [],
   "source": [
    "# all functions together, finds k nearest for each term, returns top n\n",
    "def pre_retrieval_KNN(string, k, wv, n):\n",
    "    \"\"\"Find the most similar tokens to the given query\"\"\"\n",
    "    tokens = tokenize(string, stop_words)\n",
    "    candidates = get_candidate_expansion_terms(tokens, k, wv)\n",
    "    candidates_sim = get_top_expansion_terms(tokens, candidates, wv)\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "    sort = sorted(candidates_sim, key=takeSecond)[::-1]\n",
    "    return sort[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T13:33:18.114793Z",
     "start_time": "2019-06-12T13:33:17.927772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deeper', 0.7391939305931274),\n",
       " ('deepest', 0.6943081694334498),\n",
       " ('shallow', 0.6193613248537374),\n",
       " ('deeps', 0.6047575334790283),\n",
       " ('depths', 0.6011057073807676),\n",
       " ('deepe', 0.5938460628287918),\n",
       " ('deep,', 0.5859666598255622),\n",
       " ('shallowed', 0.583433385064088),\n",
       " ('deepers', 0.5757998018436017),\n",
       " ('deepesh', 0.570137993578689)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('deep', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pollutions', 0.6271618716914852),\n",
       " ('—fishing', 0.6187853691901083),\n",
       " ('shellfishing', 0.6062972260940005),\n",
       " ('‘fishing', 0.6062562445167072),\n",
       " ('earpollution', 0.6010677191786318),\n",
       " ('flwfishing', 0.5958800042388847),\n",
       " ('pollution,', 0.5895427243095528),\n",
       " ('fishing,\\u3000', 0.5847228653899453),\n",
       " ('#pollution', 0.5825691062808801),\n",
       " ('biopollution', 0.5803963372819559),\n",
       " ('fishings', 0.579665719484446),\n",
       " ('polluting', 0.5758477844891869),\n",
       " ('billfishing', 0.5739594348713932),\n",
       " ('sollution', 0.5724698912220131),\n",
       " ('gamefishing', 0.5716465085305145)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('fishing and pollution', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('undergrounds', 0.9179600940200481),\n",
       " ('undergroun', 0.8998362698613858),\n",
       " ('undergroung', 0.8908817840768637),\n",
       " ('undergrounded', 0.8905654223562418),\n",
       " ('undergrounding', 0.8774652255413747),\n",
       " ('underground,', 0.864975977115042),\n",
       " ('undergrounder', 0.863371207446057),\n",
       " ('‘underground', 0.8486077380180744),\n",
       " ('undergrounders', 0.8468026308820736),\n",
       " ('undergroud', 0.8240030184734265)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('underground', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pollutions', 0.9166142395275622),\n",
       " ('pollution,', 0.8794701382056926),\n",
       " ('#pollution', 0.8381261902767326),\n",
       " ('earpollution', 0.8319100859328133),\n",
       " ('biopollution', 0.8275063703183863),\n",
       " ('pollutants', 0.811238387992271),\n",
       " ('antipollution', 0.8036468757467374),\n",
       " ('pollut', 0.7968592254436011),\n",
       " ('polluting', 0.7878968364118702),\n",
       " ('sollution', 0.7834910856693191)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('pollution', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deforestations', 0.9543662655883809),\n",
       " ('deforestated', 0.8770535809858295),\n",
       " ('forestation', 0.8603426012583533),\n",
       " ('reforestation', 0.8474253223194705),\n",
       " ('deforestator', 0.8424493806368417),\n",
       " ('rainforestation', 0.8416130982869243),\n",
       " ('deforested', 0.840407202191005),\n",
       " ('deforesting', 0.837824533688038),\n",
       " ('deforestion', 0.8273205199154788),\n",
       " ('afforestation', 0.7714247425497361)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('deforestation', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('—fishing', 0.8224531025707755),\n",
       " ('fishing,\\u3000', 0.8175126643141182),\n",
       " ('fishings', 0.7937207347071881),\n",
       " ('‘fishing', 0.7848391574115785),\n",
       " ('fishing,', 0.7788249395298493),\n",
       " ('codfishing', 0.7764357551424427),\n",
       " ('gamefishing', 0.7726032676663466),\n",
       " ('shellfishing', 0.7702744225755196),\n",
       " ('billfishing', 0.7617344350893832),\n",
       " ('flwfishing', 0.757403056207396)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('fishing', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fishes', 0.8091816234334873),\n",
       " ('baitfish', 0.7484850728532674),\n",
       " ('milkfishes', 0.7438243794721577),\n",
       " ('shellfishes', 0.737795091718359),\n",
       " ('gamefish', 0.728902819872663),\n",
       " ('mudfishes', 0.7288577509453343),\n",
       " ('beakfish', 0.7276970772384057),\n",
       " ('goldfishes', 0.7234903439609776),\n",
       " ('billfishes', 0.7199549836696606),\n",
       " ('fishwater', 0.7198671529918804)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('fish', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('annexes', 0.716713979048553),\n",
       " ('annexet', 0.7042431294278589),\n",
       " ('mannex', 0.6553835809055547),\n",
       " ('annexe', 0.6403126926256983),\n",
       " ('annexy', 0.6340643399412271),\n",
       " ('annext', 0.592006018305464),\n",
       " ('brannex', 0.5619266512662793),\n",
       " ('annexed', 0.5303272978298281),\n",
       " ('building', 0.5265608624567143),\n",
       " ('gannex', 0.5140592604975713)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('annex', 15, wv_wiki_en, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Users\\\\sarab\\\\work\\\\enviroLens\\\\final\\\\enviroLENS\\\\word-embeddings\\\\notebooks', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\python37.zip', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\DLLs', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens', '', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\modules-1.0-py3.7.egg', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\win32', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'D:\\\\Users\\\\sarab\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Users\\\\sarab\\\\work\\\\enviroLens\\\\final\\\\enviroLENS\\\\word-embeddings', 'D:\\\\Users\\\\sarab\\\\work\\\\enviroLens\\\\final\\\\enviroLENS\\\\word-embeddings\\\\notebooks', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\python37.zip', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\DLLs', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens', '', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\modules-1.0-py3.7.egg', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\win32', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Users\\\\sarab\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\EnviroLens\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'D:\\\\Users\\\\sarab\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "# import postgresql\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "print(sys.path)\n",
    "from modules.library.postgresql import PostgresQL\n",
    "# connect to the postgresql database\n",
    "pg = PostgresQL() \n",
    "pg.connect(database=\"eurlex_environment_only\", user=\"postgres\", password=\"dbpass\") #\"eurlex_env_only\" \"solata.2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import documents \n",
    "# documents = pg.execute(\"\"\"\n",
    "#     SELECT * FROM documents WHERE LOWER(document_text) ~ '(LOWER({}))';\n",
    "# \"\"\".format('|'.join(['fishing'])))\n",
    "documents = pg.execute(\"\"\"\n",
    "    SELECT * FROM documents;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[ \"agreement\", \"fish\"]\n",
    "output = '|'.join(words)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta dela ampak pocasi, pocakaj na Samota\n",
    "def db_query(query_words):\n",
    "    \"\"\" Vrne seznam dokumentov iz baze, ki vsebujejo vsaj eno od besed iz seznama(list) query_words\"\"\"\n",
    "    output = '|'.join(query_words)\n",
    "    SQL = \"\"\"\n",
    "        SELECT * FROM documents\n",
    "        WHERE document_text @@ to_tsquery(\"\"\" + '\\''+ output + '\\'' + \"\"\");\"\"\"\n",
    "    documents = pg.execute(SQL)\n",
    "    return(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[ \"agreement\", \"fish\"]\n",
    "docs = db_query(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23722\n"
     ]
    }
   ],
   "source": [
    "print(len(dos\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 33,\n",
       " 'document_celex_num': '21981A0916(01)',\n",
       " 'document_title': 'Exchange of letters between the European Economic Community and the Peoples Republic of Poland on trade in sheepmeat and goatmeat - Exchange of letters relevant to the consultations foreseen in clause 8 of the exchange of letters - Exchange of letters relevant to clause 2 of the exchange of letters',\n",
       " 'document_author': 'European Economic Community',\n",
       " 'document_form': 'Exchange of letters',\n",
       " 'document_date': datetime.date(1981, 4, 28),\n",
       " 'document_text': 'Avis juridique important|21981A0916(01)Exchange of letters between the European Economic Community and the Peoples Republic of Poland on trade in sheepmeat and goatmeat - Exchange of letters relevant to the consultations foreseen in clause 8 of the exchange of letters - Exchange of letters relevant to clause 2 of the exchange of lettersOfficial Journal L 137 , 23/05/1981 P. 0013 - 0020 EXCHANGE OF LETTERS between the European Economic Community and the Peoples Republic of Poland on trade in sheepmeat and goatmeat Letter No 1Sir,I have the honour to refer to the negotiations recently undertaken between our respective delegations for the purpose of drawing up provisions concerning import into the European Economic Community of mutton, lamb and goatmeat, and live sheep and goats other than pure-bred breeding animals from the Peoples Republic of Poland, in connection with implementation by the Community of the common organization of the market in sheepmeat and goatmeat.During these negotiations which took place between the two Parties, who are participants in GATT, our delegations agreed as follows:1. This arrangement covers:- live sheep and goats other than pure-bred breeding animals (subheading 01.04 B of the Common Customs Tariff), - fresh or chilled mutton, lamb and goatmeat (subheading 02.01 A IV a) of the Common Customs Tariff), - frozen mutton, lamb and goatmeat (subheading 02.01 A IV b) of the Common Customs Tariff).The two Parties agree that steps should be taken to ensure that the smooth operation of the agreement should not be upset by delivery of sheepmeat and goatmeat products falling under tariff headings not covered by the arrangement. 2. Within this arrangement, the scope for imports of sheepmeat and goatmeat and for live sheep and goats from Poland into the Community shall be fixed at the following annual quantities:- 5 800 tonnes of live animals, expressed in carcase weight bone-in (1), - 200 tonnes of fresh or chilled meat, expressed in carcase weight bone-in (2).In order to ensure the smooth operation of the arrangement, the competent Polish authorities undertake to implement the appropriate procedures to ensure that the quantities actually exported do not exceed the abovementioned figures. 3. In the case of imports of the products covered by this arrangement and up to the quantity limits therein laid down, the Community will not apply new quantitative restrictions or measures of equivalent effect nor levy customs duties or taxes of equivalent effect to levies or customs duties exceeding those agreed in clause 5.Were the Community to invoke the protective clause, it is hereby agreed that the provisions of this arrangement would not be affected. (1) 100 kilograms live weight shall correspond to 47 kilograms carcase weight (bone-in equivalent weight).(2) Carcase weight (bone-in equivalent weight). By this term is understood the weight of bone-in meat presented as such as well as boned meat converted by a coefficient into bone-in weight. For this purpose 55 kilograms of boned mutton corresponds to 100 kilograms of bone-in mutton and 60 kilograms of boned lamb corresponds to 100 kilograms of bone-in lamb. 4. If imports from Poland exceed the agreed quantities, the Community reserves the right to suspend further imports from that country until the end of the current year. However, in any event, quantities beyond those agreed for the current year shall be deducted from the quantities agreed for the following year. 5. The Community undertakes, on import of products covered by this arrangement, to limit the amounts levied to the following ad valorem levels:- 10 % for live animals, - 10 % for meat. 6. On accession of new Member States to the Community, the quantities referred to in clause 2 shall be, as appropriate, adapted by the Community, in consultation between the two Parties, to reflect the trade between the Peoples Republic of Poland and each such new Member State. The quantities shall not be reduced.The charges to be levied on imports in respect of the said new Member States shall be fixed in accordance with the rules in the Treaty of Accession ; the limit on the levy set out in clause 5 shall be taken into account. 7. The competent Polish authorities shall ensure compliance with this arrangement, in particular through issue by the competent agency, which they shall designate for the purpose, of export licences applicable to the products referred to at clause 1, up to the maximum agreed quantity.The Community agrees to take all the necessary measures to make automatic issue of an import licence, no later than release from customs bond, for the abovementioned products originating in Poland subject to production of an export licence issued by the competent Polish authority.Detailed rules for the implementation of this system shall be drawn up so that the lodging of a security for the issue of import licences in respect of the products in question shall be unnecessary. These detailed rules shall also provide that the competent Polish authorities and the competent Community authorities shall undertake periodical exchanges of information in respect of the quantities in respect of which export and import licences have been issued, broken down according to destination as appropriate, as well as in respect of quantities which have actually been shipped.It is hereby agreed that export licences will be valid for three months with effect from their date of issue. The corresponding import licences shall be valid until the date of expiry of the export licences.Quantities delivered under an export licence shall be deducted from the quantity agreed in respect of the year during which the export licence was issued. 8. In order to ensure the smooth operation of this arrangement, the two Parties shall take the appropriate measures and agree to remain in close contact and to be ready to undertake consultation in respect of any question which might arise while this arrangement applies. Consultation must commence within a maximum of 14 days after a request by one of the Parties. 9. The annual quantity fixed in clause 2 shall cover the period 1 January to 31 December.The quantity applicable as from the date of implementation of this arrangement up to 1 January of the following year shall be fixed by consultation between the two Parties in proportion to the total annual quantity, adjusted to reflect the seasonal trend in Polish deliveries of the products in question during the year.10. This Agreement shall apply, on the one hand, to the territories in which the Treaty establishing the European Economic Community is applied and under the conditions laid down in that Treaty and, on the other hand, to the territory of the Peoples Republic of Poland. 11. This arrangement shall enter into force on 1 January 1981. It shall apply until 31 March 1984. It shall thereafter be automatically extended for periods of one year, subject to the right of either Party to terminate it by notice in writing given six months before the date of expiry of any one of the said periods. In case of termination, the arrangement shall expire at the date of expiry of the period in question. In any case, the provisions of this arrangement shall be reviewed by the two Parties before 1 April 1984 for the purpose of incorporating any adaptation which might seem necessary for its extension.I would be grateful to you if you would confirm to me that the foregoing correctly expresses what our two delegations agree in this respect.Please accept, Sir, the assurance of my highest consideration.For the Council of the European CommunitiesLetter No 2Sir,I have the honour to acknowledge receipt of your letter of todays date which reads as follows:\"I have the honour to refer to the negotiations recently undertaken between our respective delegations for the purpose of drawing up provisions concerning import into the European Economic Community of mutton, lamb and goatmeat, and live sheep and goats other than pure-bred breeding animals from the Peoples Republic of Poland, in connection with implementation by the Community of the common organization of the market in sheepmeat and goatmeat.During these negotiations which took place between the two Parties, who are participants in GATT, our delegations agreed as follows:1. This arrangement covers:- live sheep and goats other than pure-bred breeding animals (subheading 01.04 B of the Common Customs Tariff), - fresh or chilled mutton, lamb and goatmeat (subheading 02.01 A IV a) of the Common Customs Tariff), - frozen mutton, lamb and goatmeat (subheading 02.01 A IV b) of the Common Customs Tariff).The two Parties agree that steps should be taken to ensure that the smooth operation of the agreement should not be upset by delivery of sheepmeat and goatmeat products falling under tariff headings not covered by the arrangement. 2. Within this arrangement, the scope for imports of sheepmeat and goatmeat and for live sheep and goats from Poland into the Community shall be fixed at the following annual quantities: - 5 800 tonnes of live animals, expressed in carcase weight bone-in (1), - 200 tonnes of fresh or chilled meat, expressed in carcase weight bone-in (2).In order to ensure the smooth operation of the arrangement, the competent Polish authorities undertake to implement the appropriate procedures to ensure that the quantities actually exported do not exceed the abovementioned figures. 3. In the case of imports of the products covered by this arrangement and up to the quantity limits therein laid down, the Community will not apply new quantitative restrictions or measures of equivalent effect nor levy customs duties or taxes of equivalent effect to levies or customs duties exceeding those agreed in clause 5.Were the Community to invoke the protective clause, it is hereby agreed that the provisions of this arrangement would not be affected. 4. If imports from Poland exceed the agreed quantities, the Community reserves the right to suspend further imports from that country until the end of the current year. However, in any event, quantities beyond those agreed for the current year shall be deducted from the quantities agreed for the following year. 5. The Community undertakes, on import of products covered by this arrangement, to limit the amounts levied to the following ad valorem levels:- 10 % for live animals, - 10 % for meat. 6. On accession of new Member States to the Community, the quantities referred to in clause 2 shall be, as appropriate, adapted by the Community, in consultation between the two Parties, to reflect the trade between the Peoples Republic of Poland and each such new Member State. The quantities shall not be reduced.The charges to be levied on imports in respect of the said new Member States shall be fixed in accordance with the rules in the Treaty of Accession ; the limit on the levy set out in clause 5 shall be taken into account. 7. The competent Polish authorities shall ensure compliance with this arrangement, in particular through issue by the competent agency, which they shall designate for the purpose, of export licences applicable to the products referred to at clause 1, up to the maximum agreed quantity.The Community agrees to take all the necessary measures to make automatic issue of an import licence, no later than release from customs bond, for the abovementioned products originating in Poland subject to production of an export licence issued by the competent Polish authority.Detailed rules for the implementation of this system shall be drawn up so that the lodging of a security for the issue of import licences in respect of the products in question shall be unnecessary. These detailed rules shall also provide that the competent Polish authorities and the competent Community authorities shall undertake periodical exchanges of information in respect of the quantities in respect of which export and import licences have been issued, broken down according to destination as appropriate, as well as in respect of quantities which have actually been shipped. (1) 100 kilograms live weight shall correspond to 47 kilograms carcase weight (bone-in equivalent weight).(2) Carcase weight (bone-in equivalent weight). By this term is understood the weight of bone-in meat presented as such as well as boned meat converted by a coefficient into bone-in weight. For this purpose 55 kilograms of boned mutton corresponds to 100 kilograms of bone-in mutton and 60 kilograms of boned lamb corresponds to 100 kilograms of bone-in lamb.It is hereby agreed that export licences will be valid for three months with effect from their date of issue. The corresponding import licences shall be valid until the date of expiry of the export licences.Quantities delivered under an export licence shall be deducted from the quantity agreed in respect of the year during which the export licence was issued. 8. In order to ensure the smooth operation of this arrangement, the two Parties shall take the appropriate measures and agree to remain in close contact and to be ready to undertake consultation in respect of any question which might arise while this arrangement applies. Consultation must commence within a maximum of 14 days after a request by one of the Parties. 9. The annual quantity fixed in clause 2 shall cover the period 1 January to 31 December.The quantity applicable as from the date of implementation of this arrangement up to 1 January of the following year shall be fixed by consultation between the two Parties in proportion to the total annual quantity, adjusted to reflect the seasonal trend in Polish deliveries of the products in question during the year. 10. This Agreement shall apply, on the one hand, to the territories in which the Treaty establishing the European Economic Community is applied and under the conditions laid down in that Treaty and, on the other hand, to the territory of the Peoples Republic of Poland. 11. This arrangement shall enter into force on 1 January 1981. It shall apply until 31 March 1984. It shall thereafter be automatically extended for periods of one year, subject to the right of either Party to terminate it by notice in writing given six month before the date of expiry of any one of the said periods. In the case of termination, the arrangement shall expire at the date of expiry of the period in question. In any case, the provisions of this arrangement shall be reviewed by the two Parties before 1 April 1984 for the purpose of incorporating any adaptation which might seem necessary for its extension.I would be grateful to you if you would confirm to me that the foregoing correctly expresses what our two delegations agree in this respect.\"I have the honour to confirm to you that the foregoing correctly expresses what our two delegations agree in this respect.Please accept, Sir, the assurance of my highest consideration.For the Government of the Peoples Republic of PolandEXCHANGE OF LETTERS relevant to the consultations foreseen in clause 8 of the exchange of letters between the European Economic Community and the Peoples Republic of Poland on trade in sheepmeat and goatmeatLetter No 1Sir,Since it was not possible to reach sufficiently precise solutions to some questions raised by Poland during the negotiations for this arrangement between the European Economic Community and the Peoples Republic of Poland on trade in sheepmeat and goatmeat, it was agreed during the negotiations, that, during the consultations provided for in clause 8 of the arrangement, if Poland were to raise any concrete problems, the following points could be covered by the said consultations without prejudice to the terms of clause 8:1. cases of force majeure; 2. supply of live animals within the quantity agreed for meat; 3. in cases where the quantity agreed for a given year was entirely taken up, advance use, at the end of the current year, of a limited proportion of the quantity agreed for the following year; 4. use of the agreed quantities for the purposes of export of frozen meat of Polish origin to the Community; 5. the possibility of allowing imports of quantities over and above those fixed in clause 2 of the arrangement if the Community market were to require additional imports; 6. the possibility of delivering export and import certificates for quantities above those agreed, in so far as the quantities actually imported are below those for which import certificates have been delivered.For its part, the Community would be prepared to undertake the said consultations in a spirit of cooperation in respect of any requests put forward by Poland.Furthermore, I have the honour hereby to confirm to you the following declaration made by the Community during the abovementioned negotiations:- imports into the Community of products covered by the arrangements shall not be subject to the quantitative limits set out in point 2 of that arrangement provided that these products are re-exported from the Community either without further processing or after inward processing under the relevant Community system.I should be obliged if you would kindly acknowledge receipt of this letter.Please accept, Sir, the assurance of my highest consideration.For the Council of the European Communities Top'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(document_text <> '') IS NOT TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123157\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = documents\n",
    "# some docs are empty !!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# for doc in docs:\n",
    "#     idd= doc.get('document_id')\n",
    "#     if idd == 39722:\n",
    "#         print(doc)\n",
    "\n",
    "delete = []\n",
    "for doc in docs:\n",
    "    n = len(doc.get('document_text'))\n",
    "    if n == 0:\n",
    "        id_doc = doc.get('document_id')\n",
    "        delete.append(id_doc)\n",
    "            \n",
    "# remove empty docs\n",
    "for doc in docs:\n",
    "    id_doc = doc.get('document_id')\n",
    "    if id_doc in delete:\n",
    "        docs.remove(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99369\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokenzized documents (texts), texts, tokenzized titles and titles #use postgress\n",
    "tokenized_docs = {}\n",
    "tokenized_titles = {}\n",
    "texts = {}\n",
    "titles = {}\n",
    "for document in docs:\n",
    "    doc_id = document.get('document_id')\n",
    "    text = document.get('document_text')\n",
    "    texts.update({doc_id: text})\n",
    "    title = document.get('document_title')\n",
    "    titles.update({doc_id: title})\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokenized = tokenize(text, stop_words)\n",
    "    title = title.translate(str.maketrans('','',string.punctuation))\n",
    "    tokenized_title = tokenize(title, stop_words)\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            if token.isalpha():\n",
    "                tokenized.remove(token)\n",
    "    tokenized_docs.update({doc_id: tokenized})\n",
    "    for title in tokenized_title:\n",
    "        if len(title) == 1:\n",
    "            if title.isalpha():\n",
    "                tokenized_title.remove(title)\n",
    "    tokenized_titles.update({doc_id: tokenized_title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agreement', 'relating', 'principally', 'chemicals', 'supplementary', 'geneva', '1967', 'protocol', 'general', 'agreement', 'tariffs', 'trade', 'negotiated', 'geneva', '30', 'june', '1967']\n",
      "/* Agreement relating principally to chemicals, supplementary to the Geneva (1967) Protocol to the General Agreement on Tariffs and Trade, negotiated in Geneva on 30 June 1967 */\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenized_titles.get(3))\n",
    "print(titles.get(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs.get(39703))\n",
    "print(texts.get(39703))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99412\n",
      "23151\n"
     ]
    }
   ],
   "source": [
    "# still some empty documents\n",
    "print(len(tokenized_docs))\n",
    "#print(take(1, tokenized_docs.items()))\n",
    "empt=[]\n",
    "for k,v in tokenized_docs.items():\n",
    "    l =len(v)\n",
    "    if l==0:\n",
    "        empt.append(k)\n",
    "print(len(empt))\n",
    "#print((empt))\n",
    "\n",
    "for k in empt:\n",
    "    del tokenized_docs[k]\n",
    "    del tokenized_titles[k]\n",
    "    del texts[k]\n",
    "    del titles[k]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76261\n",
      "76261\n",
      "76261\n",
      "76261\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_docs))\n",
    "print(len(tokenized_titles))\n",
    "print(len(texts))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-68b8bbc9534d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtitles1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mvtd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_docs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvtt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_titles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# just making smaller document set for faster testing, can delete later\n",
    "tokenized_docs1 = {}\n",
    "tokenized_titles1 = {}\n",
    "texts1= {}\n",
    "titles1={}\n",
    "for k in range(1,1000):\n",
    "    vtd = tokenized_docs.get(k)\n",
    "    vtt = tokenized_titles.get(k)\n",
    "    vx = texts.get(k)\n",
    "    vt =titles.get(k)\n",
    "    tokenized_docs1.update({k:vtd})\n",
    "    tokenized_titles1.update({k:vtt})\n",
    "    texts1.update({k:vx})\n",
    "    titles1.update({k:vt})\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_docs1))\n",
    "print(len(tokenized_titles1))\n",
    "print(len(texts1))\n",
    "print(len(titles1))\n",
    "tokenized_docs1 = {k:v for k,v in tokenized_docs1.items() if v is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### search full words (not lemmatized), search as substrings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. probability scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability scoring\n",
    "### all query words have to be in the document (multiplying)\n",
    "\n",
    "def probab_score(tokens,tokenized_docs,texts):\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 1\n",
    "        text = texts.get(k)\n",
    "        for token in tokens:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability*(token_frequency/n)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(120739, 2.2778939503692463e-06), (90952, 1.4623827626019007e-06), (98346, 9.60986510312026e-07), (100875, 9.116079910329329e-07), (96934, 7.795624765214125e-07), (98891, 4.7381201508301593e-07), (99214, 4.374815382790846e-07), (53800, 2.5508770567849454e-07), (101429, 2.4695734439852766e-07), (101872, 2.433802400069899e-07)]\n",
      "[]\n",
      "[(72, 4.866706628529354e-11), (4098, 3.9980157168428714e-11), (31030, 1.0620609744608273e-11), (85, 6.56566844179315e-12), (47616, 4.413577090077561e-12), (59689, 3.662677012225006e-12), (2146, 2.529433698703274e-12), (25303, 2.2094496103487563e-12), (57414, 2.1926876323265894e-12), (31131, 1.3631354904463345e-12)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# only original query\n",
    "score =probab_score(test,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "print(sorted_positives_top)\n",
    "\n",
    "# original query + extension\n",
    "score =probab_score(test+ext,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "print(([(k,v) for k,v in score.items() if v > 0]))\n",
    "\n",
    "\n",
    "# only original query\n",
    "score =probab_score(test1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "print(sorted_positives_top)\n",
    "\n",
    "# original query + extension\n",
    "score =probab_score(test1+ext1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "print(([(k,v) for k,v in score.items() if v > 0]))\n",
    "\n",
    "\n",
    "# no point having an extention, empty results are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def probability_score(tokens,texts):\n",
    "        \"\"\"Assigns score to document based on multiplication of probabilities. Used only for query searches which demand that all query words \n",
    "        should be in doucuments returned.\n",
    "        Args:\n",
    "            tokens (list): List of tokens (tokenized query).\n",
    "            tokenized_docs (dict): Keys represent document ids, values are lists of tokenized text (content) of documents.\n",
    "            texts (dict):  Keys represent document ids, values are document text.\n",
    "        Returns:\n",
    "            document_probability (dict): Keys represent document ids, values are scores that measure adequacy of the document.\n",
    "        \"\"\"\n",
    "        document_probability = {}\n",
    "        for k, v in texts.items():\n",
    "            n = len(v)\n",
    "            probability = 1\n",
    "            for token in tokens:\n",
    "                token_frequency = v.count(token)\n",
    "                probability = probability*(token_frequency/n)\n",
    "            document_probability.update({k: probability})\n",
    "        return document_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0,\n",
       " 2: 5.682684467100214e-12,\n",
       " 3: 0.0,\n",
       " 5: 0.0,\n",
       " 6: 0.0,\n",
       " 7: 0.0,\n",
       " 8: 0.0,\n",
       " 9: 0.0,\n",
       " 10: 0.0,\n",
       " 11: 0.0,\n",
       " 12: 0.0,\n",
       " 13: 0.0,\n",
       " 14: 0.0,\n",
       " 15: 0.0,\n",
       " 16: 0.0,\n",
       " 17: 0.0,\n",
       " 18: 0.0,\n",
       " 19: 0.0,\n",
       " 20: 0.0,\n",
       " 21: 0.0,\n",
       " 22: 0.0,\n",
       " 23: 0.0,\n",
       " 24: 0.0,\n",
       " 25: 0.0,\n",
       " 26: 0.0,\n",
       " 27: 0.0,\n",
       " 28: 0.0,\n",
       " 29: 0.0,\n",
       " 30: 0.0,\n",
       " 31: 0.0,\n",
       " 32: 0.0,\n",
       " 33: 0.0,\n",
       " 34: 0.0,\n",
       " 35: 0.0,\n",
       " 36: 0.0,\n",
       " 37: 0.0,\n",
       " 38: 0.0,\n",
       " 39: 0.0,\n",
       " 58: 0.0,\n",
       " 116: 0.0,\n",
       " 2624: 0.0,\n",
       " 40: 0.0,\n",
       " 41: 0.0,\n",
       " 42: 0.0,\n",
       " 43: 0.0,\n",
       " 3021: 0.0,\n",
       " 44: 0.0,\n",
       " 45: 0.0,\n",
       " 46: 0.0,\n",
       " 47: 0.0,\n",
       " 48: 0.0,\n",
       " 49: 0.0,\n",
       " 50: 0.0,\n",
       " 51: 1.1509197350428859e-11,\n",
       " 52: 0.0,\n",
       " 53: 0.0,\n",
       " 54: 0.0,\n",
       " 55: 0.0,\n",
       " 56: 0.0,\n",
       " 57: 0.0,\n",
       " 59: 0.0,\n",
       " 60: 0.0,\n",
       " 61: 0.0,\n",
       " 62: 0.0,\n",
       " 63: 0.0,\n",
       " 64: 0.0,\n",
       " 65: 0.0,\n",
       " 66: 0.0,\n",
       " 67: 0.0,\n",
       " 3022: 0.0,\n",
       " 68: 0.0,\n",
       " 69: 0.0,\n",
       " 70: 0.0,\n",
       " 71: 0.0,\n",
       " 72: 0.0,\n",
       " 73: 0.0,\n",
       " 74: 0.0,\n",
       " 75: 0.0,\n",
       " 76: 0.0,\n",
       " 77: 0.0,\n",
       " 78: 0.0,\n",
       " 79: 0.0,\n",
       " 80: 1.6387162322777285e-11,\n",
       " 81: 0.0,\n",
       " 82: 0.0,\n",
       " 83: 0.0,\n",
       " 91: 0.0,\n",
       " 92: 0.0,\n",
       " 84: 0.0,\n",
       " 85: 0.0,\n",
       " 86: 0.0,\n",
       " 87: 0.0,\n",
       " 88: 0.0,\n",
       " 89: 0.0,\n",
       " 90: 0.0,\n",
       " 93: 0.0,\n",
       " 3023: 0.0,\n",
       " 94: 0.0,\n",
       " 95: 0.0,\n",
       " 96: 0.0,\n",
       " 97: 0.0,\n",
       " 98: 0.0,\n",
       " 99: 0.0,\n",
       " 100: 0.0,\n",
       " 101: 0.0,\n",
       " 115: 0.0,\n",
       " 2625: 0.0,\n",
       " 102: 0.0,\n",
       " 103: 0.0,\n",
       " 104: 0.0,\n",
       " 105: 0.0,\n",
       " 106: 0.0,\n",
       " 107: 0.0,\n",
       " 108: 0.0,\n",
       " 109: 0.0,\n",
       " 110: 0.0,\n",
       " 111: 0.0,\n",
       " 112: 0.0,\n",
       " 113: 0.0,\n",
       " 114: 0.0,\n",
       " 117: 0.0,\n",
       " 118: 0.0,\n",
       " 119: 0.0,\n",
       " 120: 0.0,\n",
       " 121: 0.0,\n",
       " 122: 0.0,\n",
       " 123: 0.0,\n",
       " 124: 0.0,\n",
       " 125: 0.0,\n",
       " 126: 0.0,\n",
       " 127: 0.0,\n",
       " 128: 0.0,\n",
       " 3024: 0.0,\n",
       " 129: 0.0,\n",
       " 130: 0.0,\n",
       " 131: 0.0,\n",
       " 132: 0.0,\n",
       " 133: 0.0,\n",
       " 134: 0.0,\n",
       " 135: 0.0,\n",
       " 136: 0.0,\n",
       " 137: 0.0,\n",
       " 138: 0.0,\n",
       " 146: 0.0,\n",
       " 158: 0.0,\n",
       " 159: 0.0,\n",
       " 3025: 0.0,\n",
       " 139: 0.0,\n",
       " 140: 0.0,\n",
       " 141: 0.0,\n",
       " 142: 0.0,\n",
       " 143: 0.0,\n",
       " 144: 0.0,\n",
       " 145: 0.0,\n",
       " 157: 0.0,\n",
       " 147: 0.0,\n",
       " 148: 0.0,\n",
       " 149: 0.0,\n",
       " 150: 0.0,\n",
       " 151: 0.0,\n",
       " 152: 0.0,\n",
       " 153: 0.0,\n",
       " 154: 0.0,\n",
       " 155: 0.0,\n",
       " 156: 0.0,\n",
       " 160: 0.0,\n",
       " 161: 0.0,\n",
       " 162: 0.0,\n",
       " 163: 0.0,\n",
       " 164: 0.0,\n",
       " 165: 0.0,\n",
       " 166: 0.0,\n",
       " 167: 0.0,\n",
       " 186: 0.0,\n",
       " 168: 0.0,\n",
       " 169: 0.0,\n",
       " 170: 0.0,\n",
       " 171: 0.0,\n",
       " 172: 0.0,\n",
       " 173: 0.0,\n",
       " 205: 0.0,\n",
       " 206: 0.0,\n",
       " 524: 0.0,\n",
       " 174: 0.0,\n",
       " 175: 0.0,\n",
       " 176: 0.0,\n",
       " 177: 0.0,\n",
       " 178: 0.0,\n",
       " 179: 0.0,\n",
       " 180: 0.0,\n",
       " 181: 0.0,\n",
       " 182: 0.0,\n",
       " 183: 0.0,\n",
       " 184: 0.0,\n",
       " 185: 0.0,\n",
       " 187: 0.0,\n",
       " 188: 0.0,\n",
       " 189: 0.0,\n",
       " 190: 0.0,\n",
       " 191: 0.0,\n",
       " 192: 0.0,\n",
       " 193: 0.0,\n",
       " 194: 0.0,\n",
       " 195: 0.0,\n",
       " 196: 0.0,\n",
       " 197: 0.0,\n",
       " 198: 0.0,\n",
       " 525: 0.0,\n",
       " 199: 0.0,\n",
       " 200: 0.0,\n",
       " 201: 0.0,\n",
       " 202: 0.0,\n",
       " 203: 0.0,\n",
       " 204: 0.0,\n",
       " 207: 0.0,\n",
       " 208: 0.0,\n",
       " 209: 0.0,\n",
       " 210: 0.0,\n",
       " 211: 0.0,\n",
       " 212: 0.0,\n",
       " 213: 0.0,\n",
       " 214: 0.0,\n",
       " 215: 0.0,\n",
       " 216: 0.0,\n",
       " 237: 0.0,\n",
       " 217: 0.0,\n",
       " 218: 0.0,\n",
       " 219: 0.0,\n",
       " 220: 0.0,\n",
       " 221: 0.0,\n",
       " 222: 0.0,\n",
       " 223: 0.0,\n",
       " 224: 0.0,\n",
       " 225: 0.0,\n",
       " 226: 0.0,\n",
       " 238: 0.0,\n",
       " 239: 0.0,\n",
       " 240: 0.0,\n",
       " 227: 0.0,\n",
       " 228: 0.0,\n",
       " 229: 0.0,\n",
       " 230: 0.0,\n",
       " 231: 0.0,\n",
       " 232: 0.0,\n",
       " 233: 0.0,\n",
       " 234: 0.0,\n",
       " 235: 0.0,\n",
       " 236: 0.0,\n",
       " 241: 0.0,\n",
       " 242: 0.0,\n",
       " 243: 0.0,\n",
       " 244: 0.0,\n",
       " 245: 0.0,\n",
       " 246: 0.0,\n",
       " 247: 0.0,\n",
       " 248: 0.0,\n",
       " 249: 0.0,\n",
       " 250: 0.0,\n",
       " 260: 0.0,\n",
       " 261: 0.0,\n",
       " 526: 0.0,\n",
       " 655: 0.0,\n",
       " 656: 0.0,\n",
       " 3026: 0.0,\n",
       " 251: 0.0,\n",
       " 252: 0.0,\n",
       " 253: 0.0,\n",
       " 254: 0.0,\n",
       " 255: 0.0,\n",
       " 256: 0.0,\n",
       " 257: 0.0,\n",
       " 258: 0.0,\n",
       " 259: 0.0,\n",
       " 262: 0.0,\n",
       " 263: 0.0,\n",
       " 264: 0.0,\n",
       " 265: 0.0,\n",
       " 266: 0.0,\n",
       " 267: 0.0,\n",
       " 268: 0.0,\n",
       " 269: 0.0,\n",
       " 3027: 0.0,\n",
       " 270: 0.0,\n",
       " 271: 0.0,\n",
       " 272: 0.0,\n",
       " 273: 0.0,\n",
       " 274: 0.0,\n",
       " 275: 0.0,\n",
       " 276: 0.0,\n",
       " 277: 0.0,\n",
       " 278: 0.0,\n",
       " 326: 0.0,\n",
       " 4845: 0.0,\n",
       " 279: 0.0,\n",
       " 280: 0.0,\n",
       " 281: 0.0,\n",
       " 282: 0.0,\n",
       " 283: 0.0,\n",
       " 284: 0.0,\n",
       " 285: 0.0,\n",
       " 286: 0.0,\n",
       " 287: 0.0,\n",
       " 288: 0.0,\n",
       " 289: 0.0,\n",
       " 290: 0.0,\n",
       " 291: 0.0,\n",
       " 292: 0.0,\n",
       " 293: 0.0,\n",
       " 294: 0.0,\n",
       " 295: 0.0,\n",
       " 327: 0.0,\n",
       " 328: 0.0,\n",
       " 296: 0.0,\n",
       " 297: 0.0,\n",
       " 298: 0.0,\n",
       " 299: 0.0,\n",
       " 300: 0.0,\n",
       " 301: 0.0,\n",
       " 302: 0.0,\n",
       " 303: 0.0,\n",
       " 304: 0.0,\n",
       " 305: 0.0,\n",
       " 329: 0.0,\n",
       " 330: 0.0,\n",
       " 331: 0.0,\n",
       " 557: 0.0,\n",
       " 306: 0.0,\n",
       " 307: 0.0,\n",
       " 308: 0.0,\n",
       " 309: 0.0,\n",
       " 310: 0.0,\n",
       " 311: 0.0,\n",
       " 312: 0.0,\n",
       " 313: 0.0,\n",
       " 314: 0.0,\n",
       " 315: 0.0,\n",
       " 316: 0.0,\n",
       " 386: 0.0,\n",
       " 387: 0.0,\n",
       " 388: 0.0,\n",
       " 317: 0.0,\n",
       " 318: 0.0,\n",
       " 319: 0.0,\n",
       " 320: 0.0,\n",
       " 321: 0.0,\n",
       " 322: 0.0,\n",
       " 323: 0.0,\n",
       " 324: 0.0,\n",
       " 325: 0.0,\n",
       " 332: 0.0,\n",
       " 333: 0.0,\n",
       " 334: 0.0,\n",
       " 335: 0.0,\n",
       " 336: 0.0,\n",
       " 337: 0.0,\n",
       " 338: 0.0,\n",
       " 558: 0.0,\n",
       " 3028: 0.0,\n",
       " 339: 0.0,\n",
       " 340: 0.0,\n",
       " 341: 0.0,\n",
       " 342: 0.0,\n",
       " 343: 0.0,\n",
       " 344: 0.0,\n",
       " 345: 0.0,\n",
       " 346: 0.0,\n",
       " 347: 0.0,\n",
       " 348: 0.0,\n",
       " 349: 0.0,\n",
       " 657: 0.0,\n",
       " 764: 0.0,\n",
       " 350: 0.0,\n",
       " 351: 0.0,\n",
       " 352: 0.0,\n",
       " 353: 0.0,\n",
       " 354: 0.0,\n",
       " 355: 0.0,\n",
       " 356: 0.0,\n",
       " 357: 0.0,\n",
       " 358: 0.0,\n",
       " 765: 0.0,\n",
       " 359: 0.0,\n",
       " 360: 0.0,\n",
       " 361: 0.0,\n",
       " 362: 0.0,\n",
       " 363: 0.0,\n",
       " 364: 0.0,\n",
       " 365: 0.0,\n",
       " 366: 0.0,\n",
       " 367: 0.0,\n",
       " 766: 0.0,\n",
       " 767: 0.0,\n",
       " 368: 0.0,\n",
       " 369: 0.0,\n",
       " 370: 0.0,\n",
       " 371: 0.0,\n",
       " 372: 0.0,\n",
       " 373: 0.0,\n",
       " 374: 0.0,\n",
       " 375: 0.0,\n",
       " 376: 0.0,\n",
       " 377: 0.0,\n",
       " 378: 0.0,\n",
       " 379: 0.0,\n",
       " 768: 0.0,\n",
       " 3029: 0.0,\n",
       " 380: 0.0,\n",
       " 381: 0.0,\n",
       " 382: 0.0,\n",
       " 383: 0.0,\n",
       " 384: 0.0,\n",
       " 385: 0.0,\n",
       " 389: 0.0,\n",
       " 390: 0.0,\n",
       " 391: 0.0,\n",
       " 392: 0.0,\n",
       " 393: 0.0,\n",
       " 394: 0.0,\n",
       " 769: 0.0,\n",
       " 770: 0.0,\n",
       " 395: 0.0,\n",
       " 396: 0.0,\n",
       " 397: 0.0,\n",
       " 398: 0.0,\n",
       " 399: 0.0,\n",
       " 400: 0.0,\n",
       " 771: 0.0,\n",
       " 401: 0.0,\n",
       " 402: 0.0,\n",
       " 403: 0.0,\n",
       " 404: 0.0,\n",
       " 409: 0.0,\n",
       " 405: 0.0,\n",
       " 406: 0.0,\n",
       " 407: 0.0,\n",
       " 408: 0.0,\n",
       " 697: 0.0,\n",
       " 410: 0.0,\n",
       " 411: 0.0,\n",
       " 412: 0.0,\n",
       " 413: 0.0,\n",
       " 414: 0.0,\n",
       " 415: 0.0,\n",
       " 436: 0.0,\n",
       " 416: 0.0,\n",
       " 417: 0.0,\n",
       " 418: 0.0,\n",
       " 419: 0.0,\n",
       " 420: 0.0,\n",
       " 437: 0.0,\n",
       " 438: 0.0,\n",
       " 730: 0.0,\n",
       " 772: 0.0,\n",
       " 3030: 0.0,\n",
       " 421: 0.0,\n",
       " 422: 0.0,\n",
       " 423: 0.0,\n",
       " 424: 0.0,\n",
       " 425: 0.0,\n",
       " 426: 0.0,\n",
       " 427: 0.0,\n",
       " 428: 0.0,\n",
       " 429: 0.0,\n",
       " 430: 0.0,\n",
       " 431: 0.0,\n",
       " 432: 0.0,\n",
       " 433: 0.0,\n",
       " 434: 0.0,\n",
       " 435: 0.0,\n",
       " 485: 0.0,\n",
       " 486: 0.0,\n",
       " 5145: 0.0,\n",
       " 439: 0.0,\n",
       " 440: 0.0,\n",
       " 441: 0.0,\n",
       " 442: 0.0,\n",
       " 443: 0.0,\n",
       " 444: 0.0,\n",
       " 6694: 0.0,\n",
       " 445: 0.0,\n",
       " 446: 0.0,\n",
       " 447: 0.0,\n",
       " 448: 0.0,\n",
       " 449: 0.0,\n",
       " 487: 0.0,\n",
       " 488: 0.0,\n",
       " 489: 0.0,\n",
       " 4846: 0.0,\n",
       " 450: 0.0,\n",
       " 451: 0.0,\n",
       " 452: 0.0,\n",
       " 453: 0.0,\n",
       " 490: 0.0,\n",
       " 773: 0.0,\n",
       " 774: 0.0,\n",
       " 454: 0.0,\n",
       " 455: 0.0,\n",
       " 456: 0.0,\n",
       " 457: 0.0,\n",
       " 775: 0.0,\n",
       " 906: 0.0,\n",
       " 458: 0.0,\n",
       " 459: 0.0,\n",
       " 460: 0.0,\n",
       " 461: 0.0,\n",
       " 462: 0.0,\n",
       " 907: 0.0,\n",
       " 5513: 0.0,\n",
       " 463: 0.0,\n",
       " 464: 0.0,\n",
       " 465: 0.0,\n",
       " 466: 0.0,\n",
       " 467: 0.0,\n",
       " 908: 0.0,\n",
       " 468: 0.0,\n",
       " 469: 0.0,\n",
       " 470: 0.0,\n",
       " 471: 0.0,\n",
       " 472: 0.0,\n",
       " 473: 0.0,\n",
       " 909: 0.0,\n",
       " 474: 0.0,\n",
       " 475: 0.0,\n",
       " 476: 0.0,\n",
       " 477: 0.0,\n",
       " 478: 0.0,\n",
       " 501: 0.0,\n",
       " 5514: 0.0,\n",
       " 479: 0.0,\n",
       " 480: 0.0,\n",
       " 481: 0.0,\n",
       " 482: 0.0,\n",
       " 483: 0.0,\n",
       " 484: 0.0,\n",
       " 491: 0.0,\n",
       " 492: 0.0,\n",
       " 493: 0.0,\n",
       " 494: 0.0,\n",
       " 495: 0.0,\n",
       " 496: 0.0,\n",
       " 497: 0.0,\n",
       " 498: 0.0,\n",
       " 499: 0.0,\n",
       " 500: 0.0,\n",
       " 1338: 0.0,\n",
       " 502: 0.0,\n",
       " 503: 0.0,\n",
       " 504: 0.0,\n",
       " 505: 0.0,\n",
       " 506: 0.0,\n",
       " 507: 0.0,\n",
       " 508: 0.0,\n",
       " 509: 0.0,\n",
       " 510: 0.0,\n",
       " 511: 0.0,\n",
       " 512: 0.0,\n",
       " 1912: 0.0,\n",
       " 513: 0.0,\n",
       " 514: 0.0,\n",
       " 515: 0.0,\n",
       " 516: 0.0,\n",
       " 517: 0.0,\n",
       " 518: 0.0,\n",
       " 519: 0.0,\n",
       " 520: 0.0,\n",
       " 521: 0.0,\n",
       " 522: 0.0,\n",
       " 523: 0.0,\n",
       " 527: 0.0,\n",
       " 528: 0.0,\n",
       " 529: 0.0,\n",
       " 530: 0.0,\n",
       " 531: 0.0,\n",
       " 2441: 0.0,\n",
       " 532: 0.0,\n",
       " 533: 0.0,\n",
       " 534: 0.0,\n",
       " 535: 0.0,\n",
       " 536: 0.0,\n",
       " 923: 0.0,\n",
       " 924: 0.0,\n",
       " 7605: 0.0,\n",
       " 537: 0.0,\n",
       " 538: 0.0,\n",
       " 539: 0.0,\n",
       " 540: 0.0,\n",
       " 541: 0.0,\n",
       " 542: 0.0,\n",
       " 543: 0.0,\n",
       " 544: 0.0,\n",
       " 545: 0.0,\n",
       " 546: 0.0,\n",
       " 938: 0.0,\n",
       " 1913: 0.0,\n",
       " 547: 0.0,\n",
       " 548: 0.0,\n",
       " 549: 0.0,\n",
       " 550: 0.0,\n",
       " 551: 0.0,\n",
       " 552: 0.0,\n",
       " 553: 0.0,\n",
       " 554: 0.0,\n",
       " 555: 0.0,\n",
       " 556: 0.0,\n",
       " 7606: 0.0,\n",
       " 559: 0.0,\n",
       " 560: 0.0,\n",
       " 561: 0.0,\n",
       " 562: 0.0,\n",
       " 563: 0.0,\n",
       " 564: 0.0,\n",
       " 565: 0.0,\n",
       " 1194: 0.0,\n",
       " 566: 0.0,\n",
       " 567: 0.0,\n",
       " 568: 0.0,\n",
       " 569: 0.0,\n",
       " 570: 0.0,\n",
       " 571: 0.0,\n",
       " 572: 0.0,\n",
       " 573: 0.0,\n",
       " 574: 0.0,\n",
       " 575: 6.268530494248657e-12,\n",
       " 576: 0.0,\n",
       " 577: 0.0,\n",
       " 578: 0.0,\n",
       " 579: 0.0,\n",
       " 580: 0.0,\n",
       " 581: 0.0,\n",
       " 1195: 0.0,\n",
       " 582: 0.0,\n",
       " 583: 0.0,\n",
       " 584: 0.0,\n",
       " 585: 0.0,\n",
       " 586: 0.0,\n",
       " 587: 0.0,\n",
       " 1914: 0.0,\n",
       " 588: 0.0,\n",
       " 589: 0.0,\n",
       " 590: 0.0,\n",
       " 591: 0.0,\n",
       " 592: 0.0,\n",
       " 605: 0.0,\n",
       " 593: 0.0,\n",
       " 594: 0.0,\n",
       " 595: 0.0,\n",
       " 596: 0.0,\n",
       " 637: 0.0,\n",
       " 9881: 0.0,\n",
       " 597: 0.0,\n",
       " 598: 0.0,\n",
       " 599: 0.0,\n",
       " 600: 0.0,\n",
       " 601: 0.0,\n",
       " 602: 0.0,\n",
       " 603: 0.0,\n",
       " 604: 0.0,\n",
       " 638: 0.0,\n",
       " 639: 0.0,\n",
       " 606: 0.0,\n",
       " 607: 0.0,\n",
       " 608: 0.0,\n",
       " 609: 0.0,\n",
       " 610: 0.0,\n",
       " 640: 0.0,\n",
       " 23962: 0.0,\n",
       " 611: 0.0,\n",
       " 612: 0.0,\n",
       " 613: 0.0,\n",
       " 614: 0.0,\n",
       " 615: 0.0,\n",
       " 641: 0.0,\n",
       " 23963: 0.0,\n",
       " 616: 0.0,\n",
       " 617: 0.0,\n",
       " 618: 0.0,\n",
       " 619: 0.0,\n",
       " 620: 0.0,\n",
       " 642: 0.0,\n",
       " 621: 0.0,\n",
       " 622: 0.0,\n",
       " 623: 0.0,\n",
       " 624: 0.0,\n",
       " 625: 0.0,\n",
       " 626: 0.0,\n",
       " 627: 0.0,\n",
       " 628: 0.0,\n",
       " 643: 0.0,\n",
       " 629: 0.0,\n",
       " 630: 0.0,\n",
       " 631: 0.0,\n",
       " 632: 0.0,\n",
       " 633: 0.0,\n",
       " 634: 0.0,\n",
       " 635: 0.0,\n",
       " 636: 0.0,\n",
       " 644: 0.0,\n",
       " 645: 0.0,\n",
       " 646: 0.0,\n",
       " 647: 0.0,\n",
       " 648: 0.0,\n",
       " 649: 0.0,\n",
       " 650: 0.0,\n",
       " 651: 0.0,\n",
       " 652: 0.0,\n",
       " 653: 0.0,\n",
       " 654: 0.0,\n",
       " 658: 0.0,\n",
       " 659: 0.0,\n",
       " 660: 0.0,\n",
       " 661: 0.0,\n",
       " 662: 0.0,\n",
       " 663: 0.0,\n",
       " 664: 0.0,\n",
       " 665: 0.0,\n",
       " 666: 0.0,\n",
       " 667: 0.0,\n",
       " 2626: 0.0,\n",
       " 668: 0.0,\n",
       " 669: 0.0,\n",
       " 670: 0.0,\n",
       " 671: 0.0,\n",
       " 672: 0.0,\n",
       " 2627: 0.0,\n",
       " 673: 0.0,\n",
       " 674: 0.0,\n",
       " 675: 0.0,\n",
       " 676: 0.0,\n",
       " 677: 0.0,\n",
       " 678: 0.0,\n",
       " 679: 0.0,\n",
       " 680: 0.0,\n",
       " 681: 0.0,\n",
       " 682: 0.0,\n",
       " 683: 0.0,\n",
       " 684: 0.0,\n",
       " 685: 0.0,\n",
       " 690: 0.0,\n",
       " 2628: 0.0,\n",
       " 2629: 0.0,\n",
       " 686: 0.0,\n",
       " 687: 0.0,\n",
       " 688: 0.0,\n",
       " 689: 0.0,\n",
       " 1196: 0.0,\n",
       " 691: 0.0,\n",
       " 692: 0.0,\n",
       " 693: 0.0,\n",
       " 694: 0.0,\n",
       " 695: 0.0,\n",
       " 696: 0.0,\n",
       " 1206: 0.0,\n",
       " 698: 0.0,\n",
       " 699: 0.0,\n",
       " 700: 0.0,\n",
       " 701: 0.0,\n",
       " 702: 0.0,\n",
       " 703: 0.0,\n",
       " 704: 0.0,\n",
       " 705: 0.0,\n",
       " 957: 0.0,\n",
       " 706: 0.0,\n",
       " 707: 0.0,\n",
       " 708: 0.0,\n",
       " 709: 0.0,\n",
       " 710: 0.0,\n",
       " 1187: 0.0,\n",
       " 711: 0.0,\n",
       " 712: 0.0,\n",
       " 713: 0.0,\n",
       " 714: 0.0,\n",
       " 715: 0.0,\n",
       " 716: 0.0,\n",
       " 717: 0.0,\n",
       " 958: 0.0,\n",
       " 718: 0.0,\n",
       " 719: 0.0,\n",
       " 720: 0.0,\n",
       " 721: 0.0,\n",
       " 1207: 0.0,\n",
       " 1208: 0.0,\n",
       " 1209: 0.0,\n",
       " 722: 0.0,\n",
       " 723: 0.0,\n",
       " 724: 0.0,\n",
       " 725: 0.0,\n",
       " 726: 0.0,\n",
       " 727: 0.0,\n",
       " 728: 0.0,\n",
       " 729: 0.0,\n",
       " 731: 0.0,\n",
       " 732: 0.0,\n",
       " 733: 0.0,\n",
       " 734: 0.0,\n",
       " 784: 0.0,\n",
       " 735: 0.0,\n",
       " 736: 0.0,\n",
       " 737: 0.0,\n",
       " 738: 0.0,\n",
       " 762: 0.0,\n",
       " 739: 0.0,\n",
       " 740: 0.0,\n",
       " 741: 0.0,\n",
       " 742: 0.0,\n",
       " 743: 0.0,\n",
       " 744: 0.0,\n",
       " 745: 0.0,\n",
       " 746: 0.0,\n",
       " 747: 0.0,\n",
       " 748: 0.0,\n",
       " 749: 0.0,\n",
       " 750: 0.0,\n",
       " 751: 0.0,\n",
       " 752: 0.0,\n",
       " 753: 0.0,\n",
       " 754: 0.0,\n",
       " 755: 0.0,\n",
       " 756: 0.0,\n",
       " 757: 0.0,\n",
       " 758: 0.0,\n",
       " 759: 0.0,\n",
       " 760: 0.0,\n",
       " 761: 0.0,\n",
       " 763: 0.0,\n",
       " 776: 0.0,\n",
       " 777: 0.0,\n",
       " 778: 0.0,\n",
       " 779: 0.0,\n",
       " 780: 0.0,\n",
       " 781: 0.0,\n",
       " 782: 0.0,\n",
       " 783: 0.0,\n",
       " 785: 0.0,\n",
       " 786: 0.0,\n",
       " 787: 0.0,\n",
       " 788: 0.0,\n",
       " 789: 0.0,\n",
       " 1210: 0.0,\n",
       " 12679: 0.0,\n",
       " 790: 0.0,\n",
       " 791: 0.0,\n",
       " 792: 0.0,\n",
       " 793: 0.0,\n",
       " 794: 0.0,\n",
       " 3682: 0.0,\n",
       " 795: 0.0,\n",
       " 796: 0.0,\n",
       " 797: 0.0,\n",
       " 798: 0.0,\n",
       " 799: 0.0,\n",
       " 2630: 0.0,\n",
       " 800: 0.0,\n",
       " 801: 0.0,\n",
       " 802: 0.0,\n",
       " 803: 0.0,\n",
       " 804: 0.0,\n",
       " 805: 0.0,\n",
       " 810: 0.0,\n",
       " 3031: 0.0,\n",
       " 806: 0.0,\n",
       " 807: 0.0,\n",
       " 808: 0.0,\n",
       " 809: 0.0,\n",
       " 1188: 0.0,\n",
       " 811: 0.0,\n",
       " 812: 0.0,\n",
       " 813: 0.0,\n",
       " 814: 0.0,\n",
       " 815: 0.0,\n",
       " 1211: 0.0,\n",
       " 26138: 0.0,\n",
       " 816: 0.0,\n",
       " 817: 0.0,\n",
       " 818: 0.0,\n",
       " 819: 0.0,\n",
       " 820: 0.0,\n",
       " 821: 0.0,\n",
       " 1212: 0.0,\n",
       " 822: 0.0,\n",
       " 823: 0.0,\n",
       " 824: 0.0,\n",
       " 825: 0.0,\n",
       " 826: 0.0,\n",
       " 827: 0.0,\n",
       " 1213: 0.0,\n",
       " 828: 0.0,\n",
       " 829: 0.0,\n",
       " 830: 0.0,\n",
       " 831: 0.0,\n",
       " 832: 0.0,\n",
       " 3032: 0.0,\n",
       " 833: 0.0,\n",
       " 834: 0.0,\n",
       " 835: 0.0,\n",
       " 836: 0.0,\n",
       " 837: 0.0,\n",
       " 852: 0.0,\n",
       " 838: 0.0,\n",
       " 839: 0.0,\n",
       " 840: 0.0,\n",
       " 841: 0.0,\n",
       " 842: 0.0,\n",
       " 882: 0.0,\n",
       " 843: 0.0,\n",
       " 844: 0.0,\n",
       " 845: 0.0,\n",
       " 846: 0.0,\n",
       " 883: 0.0,\n",
       " 3033: 0.0,\n",
       " 847: 0.0,\n",
       " 848: 0.0,\n",
       " 849: 0.0,\n",
       " 850: 0.0,\n",
       " 851: 0.0,\n",
       " 853: 0.0,\n",
       " 854: 0.0,\n",
       " 855: 0.0,\n",
       " 856: 0.0,\n",
       " 857: 0.0,\n",
       " 1214: 0.0,\n",
       " 1215: 0.0,\n",
       " 858: 0.0,\n",
       " 859: 0.0,\n",
       " 860: 0.0,\n",
       " 861: 0.0,\n",
       " 862: 0.0,\n",
       " 1217: 0.0,\n",
       " 863: 0.0,\n",
       " 864: 0.0,\n",
       " 865: 0.0,\n",
       " 866: 0.0,\n",
       " 867: 0.0,\n",
       " 1218: 0.0,\n",
       " 868: 0.0,\n",
       " 869: 0.0,\n",
       " 870: 0.0,\n",
       " 871: 0.0,\n",
       " 872: 0.0,\n",
       " 1219: 0.0,\n",
       " 1220: 0.0,\n",
       " 873: 0.0,\n",
       " 874: 0.0,\n",
       " 875: 0.0,\n",
       " 876: 0.0,\n",
       " 884: 0.0,\n",
       " 26791: 0.0,\n",
       " 877: 0.0,\n",
       " 878: 0.0,\n",
       " 879: 0.0,\n",
       " 880: 0.0,\n",
       " 881: 0.0,\n",
       " 904: 0.0,\n",
       " 905: 0.0,\n",
       " 885: 0.0,\n",
       " 886: 0.0,\n",
       " 887: 0.0,\n",
       " 888: 0.0,\n",
       " 889: 0.0,\n",
       " 890: 0.0,\n",
       " 891: 0.0,\n",
       " 892: 0.0,\n",
       " 893: 0.0,\n",
       " 894: 0.0,\n",
       " 895: 0.0,\n",
       " 3034: 0.0,\n",
       " 896: 0.0,\n",
       " 897: 0.0,\n",
       " 898: 0.0,\n",
       " 899: 0.0,\n",
       " 900: 0.0,\n",
       " 901: 0.0,\n",
       " 902: 0.0,\n",
       " 903: 0.0,\n",
       " 3035: 0.0,\n",
       " 910: 0.0,\n",
       " 911: 0.0,\n",
       " 912: 0.0,\n",
       " 913: 0.0,\n",
       " 914: 0.0,\n",
       " 915: 0.0,\n",
       " 916: 0.0,\n",
       " 917: 0.0,\n",
       " 918: 0.0,\n",
       " 919: 0.0,\n",
       " 920: 0.0,\n",
       " 921: 0.0,\n",
       " 922: 0.0,\n",
       " 1221: 0.0,\n",
       " 925: 0.0,\n",
       " 926: 0.0,\n",
       " 927: 0.0,\n",
       " 928: 0.0,\n",
       " 929: 0.0,\n",
       " 930: 0.0,\n",
       " 931: 0.0,\n",
       " 932: 0.0,\n",
       " 933: 0.0,\n",
       " 934: 0.0,\n",
       " 935: 0.0,\n",
       " 936: 0.0,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query words summation\n",
    "def probab_score_sum(tokens,tokenized_docs,texts):\n",
    "    '''assigns score to document based on summation of probabilities'''\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 0\n",
    "        text = texts.get(k)\n",
    "        for token in tokens:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability+(token_frequency/n)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zavedaj se:\n",
    "\"gabla is bla2\".count(\"bla\")\n",
    "# kar pomeni da bi bilo bolje uporabiti lemmatized words! popravi!! neke stvari zdaj 2x stejes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_positives(dictionary,n):\n",
    "    \"\"\"Takes dict and returns first n tuples of k,v sorted by v\"\"\"\n",
    "    positives = {} \n",
    "    for k,v in dictionary.items():\n",
    "        if v > 0:\n",
    "            positives.update({k: v})\n",
    "    sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "    sorted_positives_top = sorted_positives[0:n]\n",
    "    return sorted_positives_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32476, 0.11656671664167916), (33546, 0.11182108626198083), (37122, 0.10662729658792651), (34869, 0.10542476970317298), (30565, 0.103206106870229), (36068, 0.10318609703113686), (3867, 0.10294117647058823), (95196, 0.0990990990990991), (94467, 0.0984251968503937), (38921, 0.0975609756097561)]\n"
     ]
    }
   ],
   "source": [
    "# only original query\n",
    "score_sum =probab_score_sum(test,tokenized_docs,texts)\n",
    "\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sum_original</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>0.116567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33546</td>\n",
       "      <td>0.111821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37122</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34869</td>\n",
       "      <td>0.105425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30565</td>\n",
       "      <td>0.103206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36068</td>\n",
       "      <td>0.103186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3867</td>\n",
       "      <td>0.102941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95196</td>\n",
       "      <td>0.099099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94467</td>\n",
       "      <td>0.098425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38921</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sum_original     score\n",
       "0            32476  0.116567\n",
       "1            33546  0.111821\n",
       "2            37122  0.106627\n",
       "3            34869  0.105425\n",
       "4            30565  0.103206\n",
       "5            36068  0.103186\n",
       "6             3867  0.102941\n",
       "7            95196  0.099099\n",
       "8            94467  0.098425\n",
       "9            38921  0.097561"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "df_sum_original = pd.DataFrame(sorted_positives_top, columns =['id_sum_original', 'score'])\n",
    "df_sum_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titles.get(565))\n",
    "# print(texts.get(565))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32476, 0.11656671664167916), (33546, 0.11182108626198083), (37122, 0.10662729658792651), (34869, 0.10542476970317298), (30565, 0.103206106870229), (36068, 0.10318609703113686), (3867, 0.10294117647058823), (95196, 0.0990990990990991), (94467, 0.0984251968503937), (38921, 0.0975609756097561)]\n"
     ]
    }
   ],
   "source": [
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test+ext,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df_sum_original_ext = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(72, 4.866706628529354e-11), (4098, 3.9980157168428714e-11), (31030, 1.0620609744608273e-11), (85, 6.56566844179315e-12), (47616, 4.413577090077561e-12), (59689, 3.662677012225006e-12), (2146, 2.529433698703274e-12), (25303, 2.2094496103487563e-12), (57414, 2.1926876323265894e-12), (31131, 1.3631354904463345e-12)]\n",
      "[(94870, 0.1276595744680851), (11790, 0.09090909090909091), (100258, 0.08527131782945736), (97436, 0.08280254777070063), (28599, 0.08163833937029813), (92702, 0.07514450867052024), (97265, 0.07430340557275542), (50112, 0.07235621521335807), (62459, 0.07103825136612021), (49539, 0.07056113902847572)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sum_original1</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>0.127660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11790</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100258</td>\n",
       "      <td>0.085271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97436</td>\n",
       "      <td>0.082803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28599</td>\n",
       "      <td>0.081638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92702</td>\n",
       "      <td>0.075145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97265</td>\n",
       "      <td>0.074303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50112</td>\n",
       "      <td>0.072356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62459</td>\n",
       "      <td>0.071038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49539</td>\n",
       "      <td>0.070561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sum_original1     score\n",
       "0             94870  0.127660\n",
       "1             11790  0.090909\n",
       "2            100258  0.085271\n",
       "3             97436  0.082803\n",
       "4             28599  0.081638\n",
       "5             92702  0.075145\n",
       "6             97265  0.074303\n",
       "7             50112  0.072356\n",
       "8             62459  0.071038\n",
       "9             49539  0.070561"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only original query\n",
    "score =probab_score(test1,tokenized_docs,texts)\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score,10)\n",
    "print(sorted_positives_top)\n",
    "\n",
    "\n",
    "\n",
    "## original query\n",
    "score_sum =probab_score_sum(test1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original1', 'score'])\n",
    "df_sum_original1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sum_original_ext1</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>0.127660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92702</td>\n",
       "      <td>0.121387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93604</td>\n",
       "      <td>0.116959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97781</td>\n",
       "      <td>0.114583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93549</td>\n",
       "      <td>0.104348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14571</td>\n",
       "      <td>0.102041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97436</td>\n",
       "      <td>0.101911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100258</td>\n",
       "      <td>0.100775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44382</td>\n",
       "      <td>0.100151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>98394</td>\n",
       "      <td>0.099099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sum_original_ext1     score\n",
       "0                 94870  0.127660\n",
       "1                 92702  0.121387\n",
       "2                 93604  0.116959\n",
       "3                 97781  0.114583\n",
       "4                 93549  0.104348\n",
       "5                 14571  0.102041\n",
       "6                 97436  0.101911\n",
       "7                100258  0.100775\n",
       "8                 44382  0.100151\n",
       "9                 98394  0.099099"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test1+ext1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "positives = dict([(k,v) for k,v in score_sum.items() if v > 0])\n",
    "sorted_positives = sorted(positives.items(), key=lambda x: x[1],reverse=True)\n",
    "sorted_positives_top = sorted_positives[0:10]\n",
    "#dataframe\n",
    "df_sum_original_ext1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext1', 'score'])\n",
    "df_sum_original_ext1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12744, 0.15163934426229508), (92986, 0.1473429951690821), (92988, 0.14669421487603304), (9649, 0.13286713286713286), (93921, 0.11695906432748537), (32476, 0.11656671664167916), (62937, 0.11578947368421053), (94936, 0.11494252873563218), (97319, 0.11363636363636363), (95196, 0.11261261261261261)]\n",
      "[(94936, 0.12643678160919541), (3453, 0.11875000000000001), (32476, 0.11656671664167916), (97319, 0.11363636363636363), (33546, 0.11182108626198083), (95196, 0.10810810810810811), (37122, 0.10662729658792651), (34869, 0.10542476970317298), (30565, 0.103206106870229), (36068, 0.10318609703113686)]\n",
      "[(94870, 0.1276595744680851), (11790, 0.09090909090909091), (100258, 0.08527131782945736), (97436, 0.08280254777070063), (28599, 0.08163833937029813), (92702, 0.07514450867052024), (97265, 0.07430340557275542), (50112, 0.07235621521335807), (62459, 0.07103825136612021), (49539, 0.07056113902847572)]\n",
      "[(94870, 0.1276595744680851), (92702, 0.12138728323699421), (93604, 0.11695906432748537), (97781, 0.11458333333333334), (93549, 0.10434782608695653), (14571, 0.10204081632653061), (97436, 0.1019108280254777), (100258, 0.10077519379844961), (44382, 0.10015060240963855), (98394, 0.0990990990990991)]\n"
     ]
    }
   ],
   "source": [
    "# adding candidates\n",
    "## without weights\n",
    "# no point using multiplication\n",
    "\n",
    "# summation:\n",
    "## original query\n",
    "score_sum =probab_score_sum(test+topw_list,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_cand = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_cand', 'score'])\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test+ext+top_list,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_ext_cand = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext_cand', 'score'])\n",
    "\n",
    "\n",
    "## original query\n",
    "score_sum =probab_score_sum(test1+topw_list1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_cand1', 'score'])\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "score_sum =probab_score_sum(test1+ext1+top_list1,tokenized_docs,texts)\n",
    "#original + ext gives same score, add global and state gives different score, same order\n",
    "#how many docs have positive score?\n",
    "sorted_positives_top = top_positives(score_sum,10)\n",
    "print(sorted_positives_top)\n",
    "#dataframe\n",
    "df_sum_original_ext_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_sum_original_ext_cand1', 'score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_value(word, alpha, original_tokens, top_expansion, wv):\n",
    "    \"\"\"values word based on whether is in original token set or expanded, if alpha -1 value equals to cosine similarity\"\"\"\n",
    "    only_expanded = []\n",
    "    for token in top_expansion:\n",
    "        if token not in original_tokens:\n",
    "            only_expanded.append(token)\n",
    "            \n",
    "    sum_similarity = 0\n",
    "    for exp_token in only_expanded:\n",
    "            sum_similarity += similarity(exp_token,original_tokens, wv)\n",
    "            \n",
    "    if alpha == -1:\n",
    "        if word in original_tokens:\n",
    "            value = 1\n",
    "        else:\n",
    "            value = similarity(word, original_tokens, wv)/sum_similarity\n",
    "\n",
    "\n",
    "    else:\n",
    "        if word in original_tokens:\n",
    "            value = alpha\n",
    "        else:\n",
    "            value = (1-alpha)*similarity(word, original_tokens, wv)/sum_similarity\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.28172557133554\n",
      "0.7\n",
      "0.27141179615367744\n"
     ]
    }
   ],
   "source": [
    "# ce ni ext zraven je so cudni rezultati, zamenja vrstni red pomembnsti med sewage in undergrounding??\n",
    "top = top[0:4]\n",
    "top_words = [i[0] for i in top]\n",
    "print(word_value(\"water\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"sewage\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"undergrounding\", 0.7, test+ext ,top_words, wv_wiki_en))\n",
    "print(word_value(\"biopollution\", 0.7, test+ext ,top_words, wv_wiki_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probab_score_sum_weights(original_tokens, top_expansion,tokenized_docs,texts, wv, alpha): \n",
    "    '''As probab_score_sum only weighted; usually extention added to original tokens, candidates = top_expansion - have weights'''\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        probability = 0\n",
    "        text = texts.get(k)\n",
    "        for token in original_tokens+top_expansion:\n",
    "            token_frequency = text.count(token)\n",
    "            probability = probability+(token_frequency/n)*word_value(token, alpha, original_tokens, top_expansion, wv)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.33091025022915893,\n",
       " 0.31908974977084104]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check word values\n",
    "wvals = []\n",
    "for token in test+ext+top_list:\n",
    "    wvals.append(word_value(token, 0.35, test+ext, top_list, wv_wiki_en))\n",
    "wvals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pollution', 'pollutions', 'undergrounding', 'earpollution', 'pollution,']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "#check word frequences\n",
    "n = len(tokenized_docs1)\n",
    "print(n)\n",
    "t_freqs = []\n",
    "for k, v in tokenized_docs1.items():\n",
    "    n = len(v)\n",
    "    text = texts1.get(k)\n",
    "    for token in test+ext+top_list:\n",
    "        token_frequency = text.count(token)\n",
    "        t_freqs.append(token_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with weights\n",
    "# summation \n",
    "original_query_cand = []\n",
    "for alpha in [-1,0.35,0.4,0.45,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    ## original query\n",
    "    score_sum = probab_score_sum_weights(test, topw_list,tokenized_docs, texts, wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_cand = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_cand'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_cand.append(df_wsum_original_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original query plus ext\n",
    "original_query_ext_cand = []\n",
    "for alpha in [-1,0.35,0.4,0.45,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum =probab_score_sum_weights(test+ext, top_list,tokenized_docs,texts,  wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_ext_cand = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_ext_cand'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_ext_cand.append(df_wsum_original_ext_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for each alpha\n",
    "doubleframes =[]\n",
    "for i in range(len(original_query_cand)):\n",
    "    frst = original_query_cand[i].take([0], axis=1)\n",
    "    snd = original_query_ext_cand[i].take([0], axis=1)\n",
    "    con = pd.concat([frst,snd], axis=1)\n",
    "    doubleframes.append(con)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_wsum_original_cand0.4</th>\n",
       "      <th>id_wsum_original_ext_cand0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12744</td>\n",
       "      <td>32476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92988</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94936</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97319</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92986</td>\n",
       "      <td>95196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33546</td>\n",
       "      <td>37122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95196</td>\n",
       "      <td>34869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9649</td>\n",
       "      <td>30565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37122</td>\n",
       "      <td>36068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_wsum_original_cand0.4  id_wsum_original_ext_cand0.4\n",
       "0                     32476                         94936\n",
       "1                     12744                         32476\n",
       "2                     92988                          3453\n",
       "3                     94936                         97319\n",
       "4                     97319                         33546\n",
       "5                     92986                         95196\n",
       "6                     33546                         37122\n",
       "7                     95196                         34869\n",
       "8                      9649                         30565\n",
       "9                     37122                         36068"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleframes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_wsum_original_cand-1</th>\n",
       "      <th>id_wsum_original_cand0.35</th>\n",
       "      <th>id_wsum_original_cand0.4</th>\n",
       "      <th>id_wsum_original_cand0.45</th>\n",
       "      <th>id_wsum_original_cand0.5</th>\n",
       "      <th>id_wsum_original_cand0.6</th>\n",
       "      <th>id_wsum_original_cand0.7</th>\n",
       "      <th>id_wsum_original_cand0.8</th>\n",
       "      <th>id_wsum_original_cand0.9</th>\n",
       "      <th>id_wsum_original_cand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>12744</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94936</td>\n",
       "      <td>92988</td>\n",
       "      <td>12744</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97319</td>\n",
       "      <td>92986</td>\n",
       "      <td>92988</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33546</td>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "      <td>92988</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92988</td>\n",
       "      <td>94936</td>\n",
       "      <td>97319</td>\n",
       "      <td>12744</td>\n",
       "      <td>92988</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95196</td>\n",
       "      <td>97319</td>\n",
       "      <td>92986</td>\n",
       "      <td>33546</td>\n",
       "      <td>95196</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12744</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>95196</td>\n",
       "      <td>12744</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37122</td>\n",
       "      <td>9649</td>\n",
       "      <td>95196</td>\n",
       "      <td>92986</td>\n",
       "      <td>37122</td>\n",
       "      <td>92988</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3453</td>\n",
       "      <td>95196</td>\n",
       "      <td>9649</td>\n",
       "      <td>37122</td>\n",
       "      <td>3453</td>\n",
       "      <td>34869</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92986</td>\n",
       "      <td>93921</td>\n",
       "      <td>37122</td>\n",
       "      <td>3453</td>\n",
       "      <td>92986</td>\n",
       "      <td>12744</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_wsum_original_cand-1  id_wsum_original_cand0.35  \\\n",
       "0                    32476                      12744   \n",
       "1                    94936                      92988   \n",
       "2                    97319                      92986   \n",
       "3                    33546                      32476   \n",
       "4                    92988                      94936   \n",
       "5                    95196                      97319   \n",
       "6                    12744                      33546   \n",
       "7                    37122                       9649   \n",
       "8                     3453                      95196   \n",
       "9                    92986                      93921   \n",
       "\n",
       "   id_wsum_original_cand0.4  id_wsum_original_cand0.45  \\\n",
       "0                     32476                      32476   \n",
       "1                     12744                      94936   \n",
       "2                     92988                      97319   \n",
       "3                     94936                      92988   \n",
       "4                     97319                      12744   \n",
       "5                     92986                      33546   \n",
       "6                     33546                      95196   \n",
       "7                     95196                      92986   \n",
       "8                      9649                      37122   \n",
       "9                     37122                       3453   \n",
       "\n",
       "   id_wsum_original_cand0.5  id_wsum_original_cand0.6  \\\n",
       "0                     32476                     32476   \n",
       "1                     94936                     94936   \n",
       "2                     97319                     97319   \n",
       "3                     33546                     33546   \n",
       "4                     92988                     95196   \n",
       "5                     95196                     37122   \n",
       "6                     12744                      3453   \n",
       "7                     37122                     92988   \n",
       "8                      3453                     34869   \n",
       "9                     92986                     12744   \n",
       "\n",
       "   id_wsum_original_cand0.7  id_wsum_original_cand0.8  \\\n",
       "0                     32476                     32476   \n",
       "1                     94936                     94936   \n",
       "2                     97319                     97319   \n",
       "3                     33546                     33546   \n",
       "4                     95196                     95196   \n",
       "5                     37122                     37122   \n",
       "6                      3453                      3453   \n",
       "7                     34869                     34869   \n",
       "8                     30565                     30565   \n",
       "9                     36068                     36068   \n",
       "\n",
       "   id_wsum_original_cand0.9  id_wsum_original_cand1  \n",
       "0                     32476                   32476  \n",
       "1                     94936                   94936  \n",
       "2                     97319                   97319  \n",
       "3                     33546                   33546  \n",
       "4                     95196                   95196  \n",
       "5                     37122                   37122  \n",
       "6                      3453                    3453  \n",
       "7                     34869                   34869  \n",
       "8                     30565                   30565  \n",
       "9                     36068                   36068  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing sorting for different alphas, original + cand\n",
    "frames =[]\n",
    "for i in range(len(original_query_cand)):\n",
    "    dataf = original_query_cand[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "con = pd.concat(frames, axis=1)\n",
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alpha 1 - 0.5 first 4 docs the same, 0.45 first 3 the same, 0.4 1st the same\n",
    "- alpha -1 same as alpha 0.5\n",
    "- alpha 0.35 even 1st doc different than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({32476: 10, 94936: 10, 97319: 10, 33546: 10, 95196: 10, 37122: 9, 3453: 8, 12744: 6, 92988: 6, 92986: 5, 34869: 5, 30565: 4, 36068: 4, 9649: 2, 93921: 1})\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#counting number of occurances of  documents\n",
    "values = con.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter=collections.Counter(flat_vals)\n",
    "print((counter))\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 docs in every column \n",
    "- 11 of 15 different docs appear at least in half of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_wsum_original_ext_cand-1</th>\n",
       "      <th>id_wsum_original_ext_cand0.35</th>\n",
       "      <th>id_wsum_original_ext_cand0.4</th>\n",
       "      <th>id_wsum_original_ext_cand0.45</th>\n",
       "      <th>id_wsum_original_ext_cand0.5</th>\n",
       "      <th>id_wsum_original_ext_cand0.6</th>\n",
       "      <th>id_wsum_original_ext_cand0.7</th>\n",
       "      <th>id_wsum_original_ext_cand0.8</th>\n",
       "      <th>id_wsum_original_ext_cand0.9</th>\n",
       "      <th>id_wsum_original_ext_cand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32476</td>\n",
       "      <td>3453</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97319</td>\n",
       "      <td>32476</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3453</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>3453</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>37122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_wsum_original_ext_cand-1  id_wsum_original_ext_cand0.35  \\\n",
       "0                        94936                          94936   \n",
       "1                        32476                           3453   \n",
       "2                        97319                          32476   \n",
       "3                         3453                          97319   \n",
       "4                        33546                          33546   \n",
       "5                        95196                          95196   \n",
       "6                        37122                          37122   \n",
       "7                        34869                          34869   \n",
       "8                        30565                          30565   \n",
       "9                        36068                          36068   \n",
       "\n",
       "   id_wsum_original_ext_cand0.4  id_wsum_original_ext_cand0.45  \\\n",
       "0                         94936                          94936   \n",
       "1                         32476                          32476   \n",
       "2                          3453                           3453   \n",
       "3                         97319                          97319   \n",
       "4                         33546                          33546   \n",
       "5                         95196                          95196   \n",
       "6                         37122                          37122   \n",
       "7                         34869                          34869   \n",
       "8                         30565                          30565   \n",
       "9                         36068                          36068   \n",
       "\n",
       "   id_wsum_original_ext_cand0.5  id_wsum_original_ext_cand0.6  \\\n",
       "0                         94936                         94936   \n",
       "1                         32476                         32476   \n",
       "2                         97319                         97319   \n",
       "3                          3453                         33546   \n",
       "4                         33546                          3453   \n",
       "5                         95196                         95196   \n",
       "6                         37122                         37122   \n",
       "7                         34869                         34869   \n",
       "8                         30565                         30565   \n",
       "9                         36068                         36068   \n",
       "\n",
       "   id_wsum_original_ext_cand0.7  id_wsum_original_ext_cand0.8  \\\n",
       "0                         94936                         32476   \n",
       "1                         32476                         94936   \n",
       "2                         97319                         97319   \n",
       "3                         33546                         33546   \n",
       "4                          3453                         95196   \n",
       "5                         95196                          3453   \n",
       "6                         37122                         37122   \n",
       "7                         34869                         34869   \n",
       "8                         30565                         30565   \n",
       "9                         36068                         36068   \n",
       "\n",
       "   id_wsum_original_ext_cand0.9  id_wsum_original_ext_cand1  \n",
       "0                         32476                       32476  \n",
       "1                         94936                       94936  \n",
       "2                         97319                       97319  \n",
       "3                         33546                       33546  \n",
       "4                         95196                       95196  \n",
       "5                          3453                       37122  \n",
       "6                         37122                        3453  \n",
       "7                         34869                       34869  \n",
       "8                         30565                       30565  \n",
       "9                         36068                       36068  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing sorting for different alphas, original +ext + cand\n",
    "frames =[]\n",
    "for i in range(len(original_query_ext_cand)):\n",
    "    dataf = original_query_ext_cand[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "conex = pd.concat(frames, axis=1)\n",
    "conex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.8 - 1 1st doc 32476, rest 1st doc 94936\n",
    "- 0.8 - 1 first 5 docs the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({94936: 10, 32476: 10, 3453: 10, 97319: 10, 33546: 10, 95196: 10, 37122: 10, 34869: 10, 30565: 10, 36068: 10})\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#counting number of occurances of  documents\n",
    "values = conex.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter=collections.Counter(flat_vals)\n",
    "print((counter))\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all columns contain same documents, just order is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'pollution', 'underground']\n",
      "['pollutions', 'undergrounding']\n",
      "['pollution', 'pollutions', 'undergrounding', 'earpollution', 'pollution,']\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(ext)\n",
    "print(top_list)\n",
    "# 94936 light pollution -, 32476 fishing quotas o, 97319 light pollution - , 3453 marine pollution o\n",
    "# without weighted sum (below): \n",
    "# 32476 already appeared, 33546 fishing quotas o,37122 fishing quotas o,12744 groundwater protection +,92988 groundwater protection +\n",
    "# 33546 fishing quotas o, 92986 groundwater protection +\n",
    "# id_sum_original_cand best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avis juridique important|91997E1892WRITTEN QUESTION No. 1892/97 by Amedeo AMADEO to the Commission. Integrated groundwater protection and managementOfficial Journal C 045 , 10/02/1998 P. 0120 WRITTEN QUESTION E-1892/97 by Amedeo Amadeo (NI) to the Commission (4 June 1997)Subject: Integrated groundwater protection and managementWith reference to the Commission Proposal for a European Parliament and Council Decision on an action programme for integrated groundwater protection and management (COM(96) 0315 final - 96/0181 COD) ((OJ C 355, 25.11.1996, p. 1.)),the proposed action programme comprises four main lines of action: planning and management of groundwater protection, creating a regulatory framework for fresh water abstraction, development of instruments for control of groundwater polution from diffuse sources and development of instruments for control of point source emissions and discharges.With regard to the first line of action, will the Commission:1. Classify waters according to their use, their different characteristics and the original water table?2. Take measures to reduce the waterproofing of the ground in suburban areas and take preventive measures against earth creep in areas intended for agricultural use?3. Restrict pumping capacities as far as possible to replenishing the water table and encourage the rational use and the re-use of water?4. Cooperate with the third countries preparing to accede to the European Union with the aim of helping them adapt to European quality standards?Answer given by Mrs Bjerregaard on behalf of the Commission (10 July 1997)The objective of the proposal for a Parliament and Council decision on an action programme for integrated groundwater protection and management is to ensure that protection and use of groundwater takes place as part of an integrated management of fresh water resources and that groundwaters on a long term basis will be managed with surface waters within a river basin management approach.In February 1997, the Commission adopted a proposal for a Council directive establishing a framework for Community action in the field of water policy ((COM(97) 49 final. )) which lays down the basic principles for integrated protection and use of groundwaters and surface waters within such a river basin management approach. Whilst part of the actions presented in the proposed groundwater action programme may be pursued through a variety of instruments, some may be strengthened through appropriate legal provisions. The proposed water framework directive is intended as a framework within which inter alia the protection and sustainable use of groundwater may be ensured through such legal provisions.1. It is the objective of the proposed groundwater action programme to ensure protection of all groundwaters and in line with this the proposed groundwater action programme does not classify groundwaters according to their use. On the contrary, in order to ensure appropriate protection and sustainable use the characteristics of a particular groundwater have to be taken into account and therefore the proposed groundwater action programme requires Member States to identify the environmental pressures as well as the particular vulnerability of groundwaters. Such designation of zones therefore is to be used to decide upon the nature of the measures which should be taken to protect a particular groundwater in a particular area.2. The proposed groundwater action programme requests Member States to take into consideration in the planning process that urban development may impair natural replenishment of groundwaters. In order to be targeted, measures to reduce the waterproofing of the ground in suburban areas and to prevent land subsidence due to a lowering of the water table should be adopted as appropriate in response to local circumstances.3. The proposed groundwater action programme requires a sustainable use of groundwaters in order to avoid overexploitation. Controls on abstraction as well as water saving, reuse and general good house-keeping of fresh water resources are encouraged as important elements in achieving this objective. Where appropriate either transiently or permanently restrictions on abstraction could be introduced depending on the local availability and characteristics of the aquifers.4. Approximation of legislation and standards is an integral part of the process within which the Community is preparing the potential enlargement. A wide range of activities and programmes target these questions in the approximation process. Top'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.get(92986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10124015377562287,\n",
       " 0.15817081118234286,\n",
       " 0.07172285285022235,\n",
       " 0.11798534515333482,\n",
       " 0.18960660975970187,\n",
       " 0.1010925155782609,\n",
       " 0.33091025022915893,\n",
       " 0.35,\n",
       " 0.3127512400283096,\n",
       " 0.29937648460119626,\n",
       " 0.26977364610954957]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check word values\n",
    "wvals = []\n",
    "for token in test1+ext1+top_list1:\n",
    "    wvals.append(word_value(token, 0.35, test+ext, top_list, wv_wiki_en))\n",
    "wvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1:\n",
    "## original query\n",
    "original_query_cand1 = []\n",
    "for alpha in [-1,0.35,0.4,0.45,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum = probab_score_sum_weights(test1, topw_list1,tokenized_docs,texts,  wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_cand1'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_cand1.append(df_wsum_original_cand1)\n",
    "\n",
    "\n",
    "## original query plus ext\n",
    "original_query_ext_cand1 = []\n",
    "for alpha in [-1,0.35,0.4,0.45,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    score_sum =probab_score_sum_weights(test1+ext1, top_list1,tokenized_docs,texts,  wv_wiki_en, alpha)\n",
    "    #original + ext gives same score, add global and state gives different score, same order\n",
    "    #how many docs have positive score?\n",
    "    sorted_positives_top = top_positives(score_sum,10)\n",
    "    #dataframe\n",
    "    df_wsum_original_ext_cand1 = pd.DataFrame(sorted_positives_top, columns =['id_wsum_original_ext_cand1'+str(alpha), 'score'+str(alpha)])\n",
    "    original_query_ext_cand1.append(df_wsum_original_ext_cand1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sorting for each alpha\n",
    "doubleframes1 =[]\n",
    "for i in range(len(original_query_cand1)):\n",
    "    frst = original_query_cand1[i].take([0], axis=1)\n",
    "    snd = original_query_ext_cand1[i].take([0], axis=1)\n",
    "    con = pd.concat([frst,snd], axis=1)\n",
    "    doubleframes1.append(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_wsum_original_cand1-1</th>\n",
       "      <th>id_wsum_original_ext_cand1-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11790</td>\n",
       "      <td>92702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100258</td>\n",
       "      <td>93604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97436</td>\n",
       "      <td>97781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28599</td>\n",
       "      <td>93549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92702</td>\n",
       "      <td>14571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97265</td>\n",
       "      <td>97436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50112</td>\n",
       "      <td>100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62459</td>\n",
       "      <td>44382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49539</td>\n",
       "      <td>98394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_wsum_original_cand1-1  id_wsum_original_ext_cand1-1\n",
       "0                     94870                         94870\n",
       "1                     11790                         92702\n",
       "2                    100258                         93604\n",
       "3                     97436                         97781\n",
       "4                     28599                         93549\n",
       "5                     92702                         14571\n",
       "6                     97265                         97436\n",
       "7                     50112                        100258\n",
       "8                     62459                         44382\n",
       "9                     49539                         98394"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleframes1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_wsum_original_cand1-1</th>\n",
       "      <th>id_wsum_original_cand10.35</th>\n",
       "      <th>id_wsum_original_cand10.4</th>\n",
       "      <th>id_wsum_original_cand10.45</th>\n",
       "      <th>id_wsum_original_cand10.5</th>\n",
       "      <th>id_wsum_original_cand10.6</th>\n",
       "      <th>id_wsum_original_cand10.7</th>\n",
       "      <th>id_wsum_original_cand10.8</th>\n",
       "      <th>id_wsum_original_cand10.9</th>\n",
       "      <th>id_wsum_original_cand11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_wsum_original_cand1-1  id_wsum_original_cand10.35  \\\n",
       "0                     94870                       94870   \n",
       "1                     11790                       11790   \n",
       "2                    100258                      100258   \n",
       "3                     97436                       97436   \n",
       "4                     28599                       28599   \n",
       "5                     92702                       92702   \n",
       "6                     97265                       97265   \n",
       "7                     50112                       50112   \n",
       "8                     62459                       62459   \n",
       "9                     49539                       49539   \n",
       "\n",
       "   id_wsum_original_cand10.4  id_wsum_original_cand10.45  \\\n",
       "0                      94870                       94870   \n",
       "1                      11790                       11790   \n",
       "2                     100258                      100258   \n",
       "3                      97436                       97436   \n",
       "4                      28599                       28599   \n",
       "5                      92702                       92702   \n",
       "6                      97265                       97265   \n",
       "7                      50112                       50112   \n",
       "8                      62459                       62459   \n",
       "9                      49539                       49539   \n",
       "\n",
       "   id_wsum_original_cand10.5  id_wsum_original_cand10.6  \\\n",
       "0                      94870                      94870   \n",
       "1                      11790                      11790   \n",
       "2                     100258                     100258   \n",
       "3                      97436                      97436   \n",
       "4                      28599                      28599   \n",
       "5                      92702                      92702   \n",
       "6                      97265                      97265   \n",
       "7                      50112                      50112   \n",
       "8                      62459                      62459   \n",
       "9                      49539                      49539   \n",
       "\n",
       "   id_wsum_original_cand10.7  id_wsum_original_cand10.8  \\\n",
       "0                      94870                      94870   \n",
       "1                      11790                      11790   \n",
       "2                     100258                     100258   \n",
       "3                      97436                      97436   \n",
       "4                      28599                      28599   \n",
       "5                      92702                      92702   \n",
       "6                      97265                      97265   \n",
       "7                      50112                      50112   \n",
       "8                      62459                      62459   \n",
       "9                      49539                      49539   \n",
       "\n",
       "   id_wsum_original_cand10.9  id_wsum_original_cand11  \n",
       "0                      94870                    94870  \n",
       "1                      11790                    11790  \n",
       "2                     100258                   100258  \n",
       "3                      97436                    97436  \n",
       "4                      28599                    28599  \n",
       "5                      92702                    92702  \n",
       "6                      97265                    97265  \n",
       "7                      50112                    50112  \n",
       "8                      62459                    62459  \n",
       "9                      49539                    49539  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing sorting for different alphas, original + cand\n",
    "frames =[]\n",
    "for i in range(len(original_query_cand1)):\n",
    "    dataf = original_query_cand1[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "con1 = pd.concat(frames, axis=1)\n",
    "con1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all columns the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annex', 'fishing', 'agreement', 'europe']\n",
      "['flwfishing', 'agreements']\n",
      "['earpollution', 'pollution', 'pollutants', 'biopollution', 'potable']\n"
     ]
    }
   ],
   "source": [
    "print(test1)\n",
    "print(ext1)\n",
    "print(top_list1)\n",
    "#94870 fishing agreement EU +, 11790 fishing agreement EU +, 100258 fishing agreement +\n",
    "#94870 11790, 100258 already, 92702 fishing agreement +, 93604 environmental agreement o ;better without extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avis juridique important|91997E3072WRITTEN QUESTION No. 3072/97 by Amedeo AMADEO to the Commission. Environmental agreementsOfficial Journal C 117 , 16/04/1998 P. 0146 WRITTEN QUESTION E-3072/97 by Amedeo Amadeo (NI) to the Commission (2 October 1997)Subject: Environmental agreementsThe main objective of the Commissions communication on environmental agreements (COM(96) 561 final) is to promote and facilitate the use of effective and acceptable environmental agreements. These agreements are instruments for the integration or implementation of environment law in the Community. The communication should be seen in the light of the strategy outlined in the fifth action programme to extend the range of environment policy instruments and put into practice the concept of shared responsibility.The communication also seeks to clarify certain aspects of how environmental agreements can be used to implement certain provisions of Community directives in the Member States and how environmental agreements can be used at Community level.Does the Commission recommendation on environmental agreements refer only to their transposal into national law or does it also cover the application of the provisions once transposed?Answer given by Mrs Bjerregaard on behalf of the Commission (31 October 1997)The Commission Recommendation concerning environmental agreements implementing Community directives ((OJ L 333, 21.12.1996. )) not only concerns the transposition of certain provisions of directives into national law but also the implementation of the transposed law. The word implementation includes both the transposition into national law and the application of the transposed provisions. Clearly, where national legislation is in place to ensure compliance with a directive, the form of implementing these rules is less important. Nevertheless, the Commission considers the legal status of agreements as an important element of their success and therefore recommends that they be concluded in a legally-binding form. Top'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.get(93604)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sum_original</th>\n",
       "      <th>id_sum_original_cand</th>\n",
       "      <th>id_sum_original_ext</th>\n",
       "      <th>id_sum_original_ext_cand</th>\n",
       "      <th>id_wsum_original_cand0.4</th>\n",
       "      <th>id_wsum_original_ext_cand0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>12744</td>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33546</td>\n",
       "      <td>92986</td>\n",
       "      <td>33546</td>\n",
       "      <td>3453</td>\n",
       "      <td>12744</td>\n",
       "      <td>32476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37122</td>\n",
       "      <td>92988</td>\n",
       "      <td>37122</td>\n",
       "      <td>32476</td>\n",
       "      <td>92988</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34869</td>\n",
       "      <td>9649</td>\n",
       "      <td>34869</td>\n",
       "      <td>97319</td>\n",
       "      <td>94936</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30565</td>\n",
       "      <td>93921</td>\n",
       "      <td>30565</td>\n",
       "      <td>33546</td>\n",
       "      <td>97319</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36068</td>\n",
       "      <td>32476</td>\n",
       "      <td>36068</td>\n",
       "      <td>95196</td>\n",
       "      <td>92986</td>\n",
       "      <td>95196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3867</td>\n",
       "      <td>62937</td>\n",
       "      <td>3867</td>\n",
       "      <td>37122</td>\n",
       "      <td>33546</td>\n",
       "      <td>37122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95196</td>\n",
       "      <td>94936</td>\n",
       "      <td>95196</td>\n",
       "      <td>34869</td>\n",
       "      <td>95196</td>\n",
       "      <td>34869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94467</td>\n",
       "      <td>97319</td>\n",
       "      <td>94467</td>\n",
       "      <td>30565</td>\n",
       "      <td>9649</td>\n",
       "      <td>30565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38921</td>\n",
       "      <td>95196</td>\n",
       "      <td>38921</td>\n",
       "      <td>36068</td>\n",
       "      <td>37122</td>\n",
       "      <td>36068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sum_original  id_sum_original_cand  id_sum_original_ext  \\\n",
       "0            32476                 12744                32476   \n",
       "1            33546                 92986                33546   \n",
       "2            37122                 92988                37122   \n",
       "3            34869                  9649                34869   \n",
       "4            30565                 93921                30565   \n",
       "5            36068                 32476                36068   \n",
       "6             3867                 62937                 3867   \n",
       "7            95196                 94936                95196   \n",
       "8            94467                 97319                94467   \n",
       "9            38921                 95196                38921   \n",
       "\n",
       "   id_sum_original_ext_cand  id_wsum_original_cand0.4  \\\n",
       "0                     94936                     32476   \n",
       "1                      3453                     12744   \n",
       "2                     32476                     92988   \n",
       "3                     97319                     94936   \n",
       "4                     33546                     97319   \n",
       "5                     95196                     92986   \n",
       "6                     37122                     33546   \n",
       "7                     34869                     95196   \n",
       "8                     30565                      9649   \n",
       "9                     36068                     37122   \n",
       "\n",
       "   id_wsum_original_ext_cand0.4  \n",
       "0                         94936  \n",
       "1                         32476  \n",
       "2                          3453  \n",
       "3                         97319  \n",
       "4                         33546  \n",
       "5                         95196  \n",
       "6                         37122  \n",
       "7                         34869  \n",
       "8                         30565  \n",
       "9                         36068  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison of summation method versions on test set\n",
    "frms = [df_sum_original[\"id_sum_original\"], df_sum_original_cand[\"id_sum_original_cand\"], df_sum_original_ext['id_sum_original_ext'],df_sum_original_ext_cand['id_sum_original_ext_cand'],doubleframes[2]]\n",
    "sum_result = pd.concat(frms, axis=1)\n",
    "sum_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({32476: 6, 95196: 6, 33546: 5, 37122: 5, 94936: 4, 34869: 4, 97319: 4, 30565: 4, 36068: 4, 12744: 2, 92986: 2, 3453: 2, 92988: 2, 9649: 2, 3867: 2, 94467: 2, 38921: 2, 93921: 1, 62937: 1})\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "values = sum_result.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter=collections.Counter(flat_vals)\n",
    "print((counter))\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same results for sum original and sum original_ext\n",
    "- slight diff. between original ext cand and original ext cand wsum  \n",
    "- ..\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sum_original1</th>\n",
       "      <th>id_sum_original_cand1</th>\n",
       "      <th>id_sum_original_ext1</th>\n",
       "      <th>id_sum_original_ext_cand1</th>\n",
       "      <th>id_wsum_original_cand10.4</th>\n",
       "      <th>id_wsum_original_ext_cand10.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>11790</td>\n",
       "      <td>92702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>93604</td>\n",
       "      <td>93604</td>\n",
       "      <td>100258</td>\n",
       "      <td>93604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97781</td>\n",
       "      <td>97781</td>\n",
       "      <td>97436</td>\n",
       "      <td>97781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28599</td>\n",
       "      <td>28599</td>\n",
       "      <td>93549</td>\n",
       "      <td>93549</td>\n",
       "      <td>28599</td>\n",
       "      <td>93549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>14571</td>\n",
       "      <td>14571</td>\n",
       "      <td>92702</td>\n",
       "      <td>14571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97265</td>\n",
       "      <td>97265</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>97265</td>\n",
       "      <td>97436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50112</td>\n",
       "      <td>50112</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>50112</td>\n",
       "      <td>100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62459</td>\n",
       "      <td>62459</td>\n",
       "      <td>44382</td>\n",
       "      <td>44382</td>\n",
       "      <td>62459</td>\n",
       "      <td>44382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49539</td>\n",
       "      <td>49539</td>\n",
       "      <td>98394</td>\n",
       "      <td>98394</td>\n",
       "      <td>49539</td>\n",
       "      <td>98394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sum_original1  id_sum_original_cand1  id_sum_original_ext1  \\\n",
       "0             94870                  94870                 94870   \n",
       "1             11790                  11790                 92702   \n",
       "2            100258                 100258                 93604   \n",
       "3             97436                  97436                 97781   \n",
       "4             28599                  28599                 93549   \n",
       "5             92702                  92702                 14571   \n",
       "6             97265                  97265                 97436   \n",
       "7             50112                  50112                100258   \n",
       "8             62459                  62459                 44382   \n",
       "9             49539                  49539                 98394   \n",
       "\n",
       "   id_sum_original_ext_cand1  id_wsum_original_cand10.4  \\\n",
       "0                      94870                      94870   \n",
       "1                      92702                      11790   \n",
       "2                      93604                     100258   \n",
       "3                      97781                      97436   \n",
       "4                      93549                      28599   \n",
       "5                      14571                      92702   \n",
       "6                      97436                      97265   \n",
       "7                     100258                      50112   \n",
       "8                      44382                      62459   \n",
       "9                      98394                      49539   \n",
       "\n",
       "   id_wsum_original_ext_cand10.4  \n",
       "0                          94870  \n",
       "1                          92702  \n",
       "2                          93604  \n",
       "3                          97781  \n",
       "4                          93549  \n",
       "5                          14571  \n",
       "6                          97436  \n",
       "7                         100258  \n",
       "8                          44382  \n",
       "9                          98394  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison of summation method versions on test1 set\n",
    "frames = [df_sum_original1[\"id_sum_original1\"], df_sum_original_cand1[\"id_sum_original_cand1\"], df_sum_original_ext1['id_sum_original_ext1'],df_sum_original_ext_cand1['id_sum_original_ext_cand1'],doubleframes1[2]]\n",
    "sum_result1 = pd.concat(frames, axis=1)\n",
    "sum_result1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1st place the same\n",
    "- sum original, sum original cand, wsum original cand same\n",
    "- sum original ext, sum original ext cand, wsum original ext  cand the same\n",
    "  --> having ext or not gives different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({94870: 6, 92702: 6, 100258: 6, 97436: 6, 11790: 3, 93604: 3, 97781: 3, 28599: 3, 93549: 3, 14571: 3, 97265: 3, 50112: 3, 62459: 3, 44382: 3, 49539: 3, 98394: 3})\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "values = sum_result1.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "counter1=collections.Counter(flat_vals)\n",
    "print((counter1))\n",
    "print(len(counter1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 values appear in all columns, rest of the values appear in half columns (ext/not ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first 5 returned docs no difference between weighted and unweighted for alpha = 0.6,  alpha = 0.8, 1 #if all weight the same is same as\n",
    "# if expansion would not exist\n",
    "# only tokens / tokens + ext\n",
    "# [(99, 0.048582995951417005), (380, 0.046610169491525424), (244, 0.04477611940298507), (89, 0.04371584699453552), (376, 0.04034065441506051)]\n",
    "# [(244, 0.08955223880597014), (243, 0.08), (903, 0.04964539007092198), (99, 0.048582995951417005), (380, 0.046610169491525424)]\n",
    "# unweighted with candidate exp:\n",
    "# [(565, 0.048730964467005075), (1219, 0.0461864406779661), (12, 0.04042348411934552), (226, 0.039756782039289056), (22, 0.03749147920927062)]\n",
    "# [(565, 0.05177664974619289), (1219, 0.04745762711864407), (12, 0.04138594802694899), (226, 0.04069223573433115), (22, 0.03953646898432175)]\n",
    "# [(99, 0.048582995951417005), (380, 0.046610169491525424), (244, 0.04477611940298507), (89, 0.04371584699453552), (376, 0.04034065441506051)]\n",
    "# [(244, 0.08955223880597014), (243, 0.08), (903, 0.04964539007092198), (99, 0.048582995951417005), (380, 0.046610169491525424)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 10\n",
    "# alpha 0.8, 1\n",
    "# [(161, 0.027947874459039665), (313, 0.027129979796553974), (73, 0.025445292620865142), (402, 0.022429906542056073)]\n",
    "# [(161, 0.027915369391449767), (313, 0.02712754175646111), (73, 0.025570205421714953), (402, 0.022429906542056073)]\n",
    "# [(243, 0.032), (925, 0.031578947368421054), (1212, 0.03118536197295147), (910, 0.031168831168831172), (108, 0.03114754098360656)]\n",
    "# [(1212, 0.036276849642004776), (89, 0.034972677595628415), (376, 0.032989690721649485), (925, 0.031578947368421054), (910, 0.031168831168831172)]\n",
    "# for alpha 0.6 one change in one case\n",
    "# only tokens/tokens+ext\n",
    "# [(243, 0.04), (925, 0.039473684210526314), (1212, 0.03898170246618934), (910, 0.03896103896103896), (108, 0.0389344262295082)]\n",
    "# [(1212, 0.045346062052505964), (89, 0.04371584699453552), (376, 0.041237113402061855), (925, 0.039473684210526314), (910, 0.03896103896103896)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 2.TFIDF evaluation\n",
    "# texts_keys = []\n",
    "# texts_values = []\n",
    "# for key in sorted(texts.keys()) :\n",
    "#     texts_keys.append(key)\n",
    "#     texts_values.append(texts[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "# vectors_t = vectorizer.fit_transform(texts_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the first vector out (for the first document)\n",
    "# vector_t = vectors_t[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # place tf-idf values in a pandas data frame\n",
    "# vector_dframe_t = pd.DataFrame(vector_t.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# vector_dframe_t = vector_dframe_t.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_dframe_t.head(50) #treaty ne sesteje lepo, lahko bi text olepsal preden gre v vectorizer, a pojavitev pomeni istost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to solve with transformation into string of tokenized text:\n",
    "# strings_keys = []\n",
    "# strings = []\n",
    "# for key in sorted(tokenized_docs.keys()) :\n",
    "#     strings_keys.append(key)\n",
    "#     list_tokens = tokenized_docs[key]\n",
    "#     corrected = \" \".join(list_tokens)\n",
    "#     strings.append(corrected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = vectorizer.fit_transform(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the first vector out (for the first document)\n",
    "# vector = vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # place tf-idf values in a pandas data frame\n",
    "# vector_dframe = pd.DataFrame(vector.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# vector_dframe = vector_dframe.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_dframe.head(50) #not much difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tfidf only for query words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_docs_tokens_appear(tokensI,tokenized_docsI,textsI):\n",
    "    '''For each token in tokensI counts the number of documents the token has appeared'''\n",
    "    docs_per_token = []\n",
    "    for i in range(len(tokensI)):\n",
    "        docs_per_token.append(0)\n",
    "    for k, v in tokenized_docsI.items():\n",
    "        content = tokenized_docsI.get(k)\n",
    "        text = textsI.get(k)\n",
    "        for i in range(len(tokensI)):\n",
    "            token = tokensI[i]\n",
    "            if token in text:\n",
    "                docs_per_token[i] = docs_per_token[i]+1\n",
    "    return docs_per_token\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_sum(tokensI,tokenized_docsI, textsI):\n",
    "    '''First tuple argument similar to probab_score_sum function but different metric - tfidf, second returns words that did not occure in any document'''\n",
    "    nb_docs_tokens_appeared = nb_docs_tokens_appear(tokensI,tokenized_docsI,textsI)\n",
    "    filtered_nb_docs_tokens_appeared = [elt for elt in nb_docs_tokens_appeared if not elt == 0]\n",
    "    not_appear = []\n",
    "    appear = []\n",
    "    for i in range(len(nb_docs_tokens_appeared)):\n",
    "        if nb_docs_tokens_appeared[i] == 0:\n",
    "            not_appear.append(tokensI[i])\n",
    "        else:\n",
    "            appear.append(tokensI[i])    \n",
    "    l = len(tokenized_docsI)\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docsI.items():\n",
    "        n = len(v)\n",
    "        text = textsI.get(k)\n",
    "        probability = 0\n",
    "        for i in range(len(appear)):\n",
    "            token_frequency = text.count(appear[i])\n",
    "            idf = math.log(l/filtered_nb_docs_tokens_appeared[i])\n",
    "            probability = probability+((token_frequency/n)*idf)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab, not_appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tfidf_sum_original</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>0.197593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33546</td>\n",
       "      <td>0.189548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37122</td>\n",
       "      <td>0.180744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34869</td>\n",
       "      <td>0.178706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95196</td>\n",
       "      <td>0.178279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30565</td>\n",
       "      <td>0.174945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36068</td>\n",
       "      <td>0.174911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3867</td>\n",
       "      <td>0.174496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63388</td>\n",
       "      <td>0.167434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94467</td>\n",
       "      <td>0.166841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_tfidf_sum_original     score\n",
       "0                  32476  0.197593\n",
       "1                  33546  0.189548\n",
       "2                  37122  0.180744\n",
       "3                  34869  0.178706\n",
       "4                  95196  0.178279\n",
       "5                  30565  0.174945\n",
       "6                  36068  0.174911\n",
       "7                   3867  0.174496\n",
       "8                  63388  0.167434\n",
       "9                  94467  0.166841"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf metric for test set\n",
    "tf = tfidf_sum(test,tokenized_docs, texts)\n",
    "df_tfidf_sum_original = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original', 'score'])\n",
    "tf[1] #query words that did not appear in any doc\n",
    "df_tfidf_sum_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tfidf_sum(test+ext,tokenized_docs, texts)\n",
    "df_tfidf_sum_original_ext = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_ext', 'score'])\n",
    "tf[1]\n",
    "#df_tfidf_sum_original_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earpollution']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tfidf_sum(test+ext+top_list,tokenized_docs, texts)\n",
    "df_tfidf_sum_original_ext_cand = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_ext_cand', 'score'])\n",
    "tf[1]\n",
    "#df_tfidf_sum_original_ext_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earpollution']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tfidf_sum(test+topw_list,tokenized_docs, texts)\n",
    "df_tfidf_sum_original_cand = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_cand', 'score'])\n",
    "tf[1]\n",
    "#df_tfidf_sum_original_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tfidf_sum_original</th>\n",
       "      <th>id_tfidf_sum_original_ext</th>\n",
       "      <th>id_tfidf_sum_original_ext_cand</th>\n",
       "      <th>id_tfidf_sum_original_cand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32476</td>\n",
       "      <td>32476</td>\n",
       "      <td>94936</td>\n",
       "      <td>92988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33546</td>\n",
       "      <td>33546</td>\n",
       "      <td>3453</td>\n",
       "      <td>12744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37122</td>\n",
       "      <td>37122</td>\n",
       "      <td>97319</td>\n",
       "      <td>92986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34869</td>\n",
       "      <td>34869</td>\n",
       "      <td>93533</td>\n",
       "      <td>9649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95196</td>\n",
       "      <td>95196</td>\n",
       "      <td>97320</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30565</td>\n",
       "      <td>30565</td>\n",
       "      <td>4505</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36068</td>\n",
       "      <td>36068</td>\n",
       "      <td>94953</td>\n",
       "      <td>92987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3867</td>\n",
       "      <td>3867</td>\n",
       "      <td>59175</td>\n",
       "      <td>93921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63388</td>\n",
       "      <td>63388</td>\n",
       "      <td>93921</td>\n",
       "      <td>47944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94467</td>\n",
       "      <td>94467</td>\n",
       "      <td>92988</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_tfidf_sum_original  id_tfidf_sum_original_ext  \\\n",
       "0                  32476                      32476   \n",
       "1                  33546                      33546   \n",
       "2                  37122                      37122   \n",
       "3                  34869                      34869   \n",
       "4                  95196                      95196   \n",
       "5                  30565                      30565   \n",
       "6                  36068                      36068   \n",
       "7                   3867                       3867   \n",
       "8                  63388                      63388   \n",
       "9                  94467                      94467   \n",
       "\n",
       "   id_tfidf_sum_original_ext_cand  id_tfidf_sum_original_cand  \n",
       "0                           94936                       92988  \n",
       "1                            3453                       12744  \n",
       "2                           97319                       92986  \n",
       "3                           93533                        9649  \n",
       "4                           97320                       94936  \n",
       "5                            4505                       97319  \n",
       "6                           94953                       92987  \n",
       "7                           59175                       93921  \n",
       "8                           93921                       47944  \n",
       "9                           92988                        3453  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results of tfidf sum for different input sets\n",
    "frames = [df_tfidf_sum_original['id_tfidf_sum_original'], df_tfidf_sum_original_ext['id_tfidf_sum_original_ext'], df_tfidf_sum_original_ext_cand['id_tfidf_sum_original_ext_cand'],df_tfidf_sum_original_cand['id_tfidf_sum_original_cand']]\n",
    "tfidf_sum_result = pd.concat(frames, axis=1)\n",
    "tfidf_sum_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tfifd sum original and tfidf sum original ext the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({32476: 2, 94936: 2, 92988: 2, 33546: 2, 3453: 2, 37122: 2, 97319: 2, 34869: 2, 95196: 2, 30565: 2, 36068: 2, 3867: 2, 93921: 2, 63388: 2, 94467: 2, 12744: 1, 92986: 1, 93533: 1, 9649: 1, 97320: 1, 4505: 1, 94953: 1, 92987: 1, 59175: 1, 47944: 1})\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#count number of appearances for each doument in upper dataframe\n",
    "values = tfidf_sum_result.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "countertf=collections.Counter(flat_vals)\n",
    "print((countertf))\n",
    "print(len(countertf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'pollution', 'underground']\n",
      "['pollutions', 'undergrounding']\n",
      "['pollution', 'pollutions', 'undergrounding', 'earpollution', 'pollution,']\n"
     ]
    }
   ],
   "source": [
    "#evaluating different merics\n",
    "print(test)\n",
    "print(ext)\n",
    "print(top_list)\n",
    "##############################################\n",
    "#SUM\n",
    "###############################################\n",
    "# 94936 light pollution -, 32476 fishing quotas o, 97319 light pollution - , 3453 marine pollution o\n",
    "# without weighted sum (below): \n",
    "# 32476 already appeared, 33546 fishing quotas o,37122 fishing quotas o,12744 groundwater protection +,92988 groundwater protection +\n",
    "# 33546 fishing quotas o, 92986 groundwater protection +\n",
    "# id_sum_original_cand best choice\n",
    "##############################################\n",
    "# TFIDF\n",
    "##########################################\n",
    "# 32476, 94936, 92988, 33546, 3453, 12744, ... \n",
    "# id_tfidf_sum_original_ext_cand -, id_tfidf_sum_original_cand +, id_tfidf_sum_original and id_tfidf_sum_original_ext o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avis juridique important|91998E2572WRITTEN QUESTION No. 2572/98 by John McCARTIN Fishing agreement with the Comores 1994-1997Official Journal C 320 , 06/11/1999 P. 0008 WRITTEN QUESTION E-2572/98by John McCartin (PPE) to the Commission(1 September 1998)Subject: Fishing agreement with the Comores 1994-1997Can the Commission state how many fishing vessels were involved in the 1994-1997 EU fishing agreement with the Comores, what was the tonnage of these vessels and how many days they fished under the agreement? Top'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.get(94870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tfidf_sum_original_cand</th>\n",
       "      <th>id_sum_original_cand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92988</td>\n",
       "      <td>12744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12744</td>\n",
       "      <td>92986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92986</td>\n",
       "      <td>92988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9649</td>\n",
       "      <td>9649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94936</td>\n",
       "      <td>93921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97319</td>\n",
       "      <td>32476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92987</td>\n",
       "      <td>62937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93921</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47944</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3453</td>\n",
       "      <td>95196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_tfidf_sum_original_cand  id_sum_original_cand\n",
       "0                       92988                 12744\n",
       "1                       12744                 92986\n",
       "2                       92986                 92988\n",
       "3                        9649                  9649\n",
       "4                       94936                 93921\n",
       "5                       97319                 32476\n",
       "6                       92987                 62937\n",
       "7                       93921                 94936\n",
       "8                       47944                 97319\n",
       "9                        3453                 95196"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing best sum and best tfidf metric/combination of input data\n",
    "best_test = pd.concat([tfidf_sum_result['id_tfidf_sum_original_cand'], sum_result['id_sum_original_cand']], axis=1)\n",
    "best_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({92988: 2, 12744: 2, 92986: 2, 9649: 2, 94936: 2, 93921: 2, 97319: 2, 32476: 1, 92987: 1, 62937: 1, 47944: 1, 3453: 1, 95196: 1})\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "values = best_test.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "countbestTest=collections.Counter(flat_vals)\n",
    "print((countbestTest))\n",
    "print(len(countbestTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different docs in dataframe best_test:\n",
    "# sum probab:32476 o, 62937 +, 95196 o+\n",
    "# tfidf sum :92987 +,47944 +, 3453 o,\n",
    "# tfidf better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avis juridique important|91998E3492WRITTEN QUESTION No. 3492/98 by Luigi MORETTI to the Commission. Pollution of surface waterOfficial Journal C 207 , 21/07/1999 P. 0077 WRITTEN QUESTION E-3492/98by Luigi Moretti (NI) to the Commission(25 November 1998)Subject: Pollution of surface waterThe drainage systems in built-up areas are often not designed to convey surface water, or water from recent rainfall, to water treatment plants. As a result, these waters flow into rivers, streams and lakes.To my knowledge there are currently no laws or provisions requiring these waters to be treated before they enter waterways.In view of the fact that surface water and water from recent rainfall are more polluted than sewage, since they contain over 2010 exhaust gases and heavy metals, can the Commission say what measures it intends to adopt in this area?Answer given by Mrs Bjerregaard on behalf of the Commission(12 January 1999)Rainwater on impermeable urban surfaces can be collected either separately from the domestic and industrial waste water of the built-up area (this is known as a separate sewer system) or in the same sewer as the waste water (a combined system). Two-thirds of urban areas have combined systems. Council Directive 91/271/EEC of 21 May 1991 concerning urban waste-water treatment(1) requires Member States to provide for the collection and treatment of the mixture of waste water and run-off rainwater in a combined sewer system. However, given that it is not possible in practice to construct collecting systems and treatment plants in a way such that all waste water can be treated during situations such as unusually heavy rainfall, the Directive lays down that Member States must decide on measures to limit pollution from storm water overflows. Such measures could be based on dilution rates or capacity in relation to dry weather flow, or could specify a certain acceptable number of overflows per year.As regards direct discharges of run-off water from separate sewer systems, the proposal for a Directive establishing a framework for Community action in the field of water policy(2), in providing that account should be taken of all major sources of pollution in each catchment area, will make it possible to regulate such discharges.(1) OJ L 135, 30.5.1991.(2) OJ C 108, 7.4.1998. Top'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.get(95196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['flwfishing']\n",
      "['flwfishing', 'earpollution', 'biopollution']\n",
      "['earpollution', 'biopollution']\n"
     ]
    }
   ],
   "source": [
    "# test1 set, query words that do not occure in any of documents\n",
    "tf = tfidf_sum(test1,tokenized_docs, texts)\n",
    "print(tf[1])\n",
    "df_tfidf_sum_original1 = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original1', 'score'])\n",
    "tf = tfidf_sum(test1+ext1,tokenized_docs, texts)\n",
    "print(tf[1])\n",
    "df_tfidf_sum_original_ext1 = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_ext1', 'score'])\n",
    "tf = tfidf_sum(test1+ext1+top_list1,tokenized_docs, texts)\n",
    "print(tf[1])\n",
    "df_tfidf_sum_original_ext_cand1 = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_ext_cand1', 'score'])\n",
    "tf = tfidf_sum(test1+topw_list1,tokenized_docs, texts)\n",
    "print(tf[1])\n",
    "df_tfidf_sum_original_cand1 = pd.DataFrame(top_positives(tf[0],10), columns =['id_tfidf_sum_original_cand1', 'score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tfidf_sum_original1</th>\n",
       "      <th>id_tfidf_sum_original_ext1</th>\n",
       "      <th>id_tfidf_sum_original_ext_cand1</th>\n",
       "      <th>id_tfidf_sum_original_cand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94870</td>\n",
       "      <td>92702</td>\n",
       "      <td>92702</td>\n",
       "      <td>94870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11790</td>\n",
       "      <td>94870</td>\n",
       "      <td>94870</td>\n",
       "      <td>11790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97436</td>\n",
       "      <td>93604</td>\n",
       "      <td>93604</td>\n",
       "      <td>97436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100258</td>\n",
       "      <td>97781</td>\n",
       "      <td>97781</td>\n",
       "      <td>100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28599</td>\n",
       "      <td>97436</td>\n",
       "      <td>97436</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50112</td>\n",
       "      <td>93549</td>\n",
       "      <td>93549</td>\n",
       "      <td>28599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97265</td>\n",
       "      <td>100258</td>\n",
       "      <td>100258</td>\n",
       "      <td>3455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39238</td>\n",
       "      <td>14571</td>\n",
       "      <td>14571</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>49539</td>\n",
       "      <td>96942</td>\n",
       "      <td>96942</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96093</td>\n",
       "      <td>11790</td>\n",
       "      <td>11790</td>\n",
       "      <td>50112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_tfidf_sum_original1  id_tfidf_sum_original_ext1  \\\n",
       "0                   94870                       92702   \n",
       "1                   11790                       94870   \n",
       "2                   97436                       93604   \n",
       "3                  100258                       97781   \n",
       "4                   28599                       97436   \n",
       "5                   50112                       93549   \n",
       "6                   97265                      100258   \n",
       "7                   39238                       14571   \n",
       "8                   49539                       96942   \n",
       "9                   96093                       11790   \n",
       "\n",
       "   id_tfidf_sum_original_ext_cand1  id_tfidf_sum_original_cand1  \n",
       "0                            92702                        94870  \n",
       "1                            94870                        11790  \n",
       "2                            93604                        97436  \n",
       "3                            97781                       100258  \n",
       "4                            97436                        38528  \n",
       "5                            93549                        28599  \n",
       "6                           100258                         3455  \n",
       "7                            14571                        94936  \n",
       "8                            96942                        97319  \n",
       "9                            11790                        50112  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [df_tfidf_sum_original1['id_tfidf_sum_original1'], df_tfidf_sum_original_ext1['id_tfidf_sum_original_ext1'], df_tfidf_sum_original_ext_cand1['id_tfidf_sum_original_ext_cand1'],df_tfidf_sum_original_cand1['id_tfidf_sum_original_cand1']]\n",
    "tfidf_sum_result1 = pd.concat(frames, axis=1)\n",
    "tfidf_sum_result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sum_original_ext and sum_original_ext_cand same, the other two columns same in first 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({94870: 4, 11790: 4, 97436: 4, 100258: 4, 92702: 2, 93604: 2, 97781: 2, 28599: 2, 50112: 2, 93549: 2, 14571: 2, 96942: 2, 38528: 1, 97265: 1, 3455: 1, 39238: 1, 94936: 1, 49539: 1, 97319: 1, 96093: 1})\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "values = tfidf_sum_result1.values.tolist()\n",
    "flat_vals = []\n",
    "for sublist in values:\n",
    "    for item in sublist:\n",
    "        flat_vals.append(item)\n",
    "countertf1=collections.Counter(flat_vals)\n",
    "print((countertf1))\n",
    "print(len(countertf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annex', 'fishing', 'agreement', 'europe']\n",
      "['flwfishing', 'agreements']\n",
      "['earpollution', 'pollution', 'pollutants', 'biopollution', 'potable']\n"
     ]
    }
   ],
   "source": [
    "# comparing results for test1 set\n",
    "print(test1)\n",
    "print(ext1)\n",
    "print(top_list1)\n",
    "############\n",
    "#SUM\n",
    "############################################################\n",
    "#94870 fishing agreement EU +, 11790 fishing agreement EU +, 100258 fishing agreement +\n",
    "#94870 11790, 100258 already, 92702 fishing agreement +, 93604 environmental agreement o ;better without extention\n",
    "############################################################\n",
    "# TFIDF\n",
    "#94870, 92702, 11790, 94870,97436 overfishing, 93604\n",
    "# better without ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avis juridique important|92000E3661WRITTEN QUESTION E-3661/00 by Glenys Kinnock (PSE) to the Commission. Coastal fishing in ACP countries.Official Journal 174 E , 19/06/2001 P. 0104 - 0105 WRITTEN QUESTION E-3661/00by Glenys Kinnock (PSE) to the Commission(27 November 2000)Subject: Coastal fishing in ACP countriesWould the Commission outline what measures it is taking to ensure that Community fishing vessels, operating under EU-ACP fishing agreements, respect the needs and rights of small-scale, coastal fishing communities in ACP countries and do not damage the local ACP fisheries sector?What action is the Commission taking to improve the capacity of ACP countries to patrol the waters under their jurisdiction, so as to control the activities of both Community and ACP fishing fleets and thereby prevent overfishing?Answer given by Mr Fischler on behalf of the Commission(5 January 2001)The Commission thanks the Honourable Member for her question and informs her that, in order to avoid clashes with small-scale traditional fishermen, protocols to fisheries agreements specify fishing zones for the Community which differ from those for local vessels.As for improving the capacity of African, Caribbean and Pacific (ACP) countries to control fishing activities, the Commission is pleased to inform the Honourable Member that, since 1997 the majority of fisheries agreements concluded between the Community and third countries (namely ACP countries) stipulate that an important part of the financial compensation paid in exchange for fishing possibilities be devoted to targeted actions, amongst which are monitoring, control and surveillance activities, including the setting up of satellite based vessel monitoring systems. Top'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.get(97436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_sum_weights(original_tokens, top_expansion,tokenized_docs,texts, wv, alpha): \n",
    "    tokens_together = original_tokens+top_expansion\n",
    "    nb_docs_tokens_appeared = nb_docs_tokens_appear(tokens_together,tokenized_docs,texts)\n",
    "    filtered_nb_docs_tokens_appeared = [elt for elt in nb_docs_tokens_appeared if not elt == 0]\n",
    "    not_appear = []\n",
    "    appear = []\n",
    "    for i in range(len(nb_docs_tokens_appeared)):\n",
    "        if nb_docs_tokens_appeared[i] == 0:\n",
    "            not_appear.append(tokens_together[i])\n",
    "        else:\n",
    "            appear.append(tokens_together[i])  \n",
    "    l = len(tokenized_docs)\n",
    "    doc_probab = {}\n",
    "    for k, v in tokenized_docs.items():\n",
    "        n = len(v)\n",
    "        text = texts.get(k)\n",
    "        probability = 0\n",
    "        for i in range(len(appear)):\n",
    "            token_frequency = text.count(appear[i])\n",
    "            idf = math.log(l/filtered_nb_docs_tokens_appeared[i])\n",
    "            probability = probability+((token_frequency/n)*idf)*word_value(appear[i], alpha, original_tokens, top_expansion, wv)\n",
    "        doc_probab.update({k: probability})\n",
    "    return doc_probab,not_appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_original_query_ext_cand = []\n",
    "for alpha in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    tfw = tfidf_sum_weights(test+ext, top_list,tokenized_docs,texts, wv_wiki_en, alpha)\n",
    "    df_tfidf_wsum_original_ext_cand = pd.DataFrame(top_positives(tfw[0],10), columns =['id_tfidf_wsum_original_ext_cand'+str(alpha), 'score'+str(alpha)])\n",
    "    tfidf_original_query_ext_cand.append(df_tfidf_wsum_original_ext_cand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand0.5</th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand0.6</th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand0.7</th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand0.8</th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand0.9</th>\n",
       "      <th>id_tfidf_wsum_original_ext_cand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "      <td>94936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "      <td>97319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "      <td>3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93533</td>\n",
       "      <td>93533</td>\n",
       "      <td>93533</td>\n",
       "      <td>93533</td>\n",
       "      <td>93533</td>\n",
       "      <td>93533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97320</td>\n",
       "      <td>97320</td>\n",
       "      <td>97320</td>\n",
       "      <td>97320</td>\n",
       "      <td>97320</td>\n",
       "      <td>97320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4505</td>\n",
       "      <td>59175</td>\n",
       "      <td>59175</td>\n",
       "      <td>59175</td>\n",
       "      <td>59175</td>\n",
       "      <td>59175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59175</td>\n",
       "      <td>4505</td>\n",
       "      <td>93921</td>\n",
       "      <td>93921</td>\n",
       "      <td>93921</td>\n",
       "      <td>93921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93921</td>\n",
       "      <td>93921</td>\n",
       "      <td>4505</td>\n",
       "      <td>4505</td>\n",
       "      <td>4505</td>\n",
       "      <td>4505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94953</td>\n",
       "      <td>94953</td>\n",
       "      <td>94953</td>\n",
       "      <td>94953</td>\n",
       "      <td>94953</td>\n",
       "      <td>94953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62286</td>\n",
       "      <td>62286</td>\n",
       "      <td>62286</td>\n",
       "      <td>62286</td>\n",
       "      <td>62286</td>\n",
       "      <td>62286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_tfidf_wsum_original_ext_cand0.5  id_tfidf_wsum_original_ext_cand0.6  \\\n",
       "0                               94936                               94936   \n",
       "1                               97319                               97319   \n",
       "2                                3453                                3453   \n",
       "3                               93533                               93533   \n",
       "4                               97320                               97320   \n",
       "5                                4505                               59175   \n",
       "6                               59175                                4505   \n",
       "7                               93921                               93921   \n",
       "8                               94953                               94953   \n",
       "9                               62286                               62286   \n",
       "\n",
       "   id_tfidf_wsum_original_ext_cand0.7  id_tfidf_wsum_original_ext_cand0.8  \\\n",
       "0                               94936                               94936   \n",
       "1                               97319                               97319   \n",
       "2                                3453                                3453   \n",
       "3                               93533                               93533   \n",
       "4                               97320                               97320   \n",
       "5                               59175                               59175   \n",
       "6                               93921                               93921   \n",
       "7                                4505                                4505   \n",
       "8                               94953                               94953   \n",
       "9                               62286                               62286   \n",
       "\n",
       "   id_tfidf_wsum_original_ext_cand0.9  id_tfidf_wsum_original_ext_cand1  \n",
       "0                               94936                             94936  \n",
       "1                               97319                             97319  \n",
       "2                                3453                              3453  \n",
       "3                               93533                             93533  \n",
       "4                               97320                             97320  \n",
       "5                               59175                             59175  \n",
       "6                               93921                             93921  \n",
       "7                                4505                              4505  \n",
       "8                               94953                             94953  \n",
       "9                               62286                             62286  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing sorting for different alphas, original + ext + cand\n",
    "frames =[]\n",
    "for i in range(len(tfidf_original_query_ext_cand)):\n",
    "    dataf = tfidf_original_query_ext_cand[i].take([0], axis=1)\n",
    "    frames.append(dataf)\n",
    "tfidfcon = pd.concat(frames, axis=1)\n",
    "tfidfcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########################\n",
    "# #set json format in readable form (starting wih multiple objec format, need list)\n",
    "# annotation_dir='D:/Users/sarab/work/enviroLens/files/'\n",
    "# print('Loading annotations')\n",
    "# annotations=[]\n",
    "# for filename in os.listdir(annotation_dir):\n",
    "#     print('loading file ',filename)\n",
    "#     lines = [line.rstrip('\\n') for line in open(annotation_dir+filename,encoding='utf-8')]\n",
    "#     for line in lines:\n",
    "#         js=json.loads(line)\n",
    "#         annotations.append(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
