{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from modules.library import documentRetrieval as dr\n",
    "from modules.library import queryExp as qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words 2519370\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wiki_en_align = './../data/fasttext/wiki.en.align.vec' #'../../data/fasttext/wiki.en.align.vec'\n",
    "# get fasttext wiki embeddings for english\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n",
    "print('english words {}'.format(len(list(wv_wiki_en.vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Decoder(json.JSONDecoder):\n",
    "    def decode(self, s):\n",
    "        result = super().decode(s)  # result = super(Decoder, self).decode(s) for Python 2.x\n",
    "        return self._decode(result)\n",
    "\n",
    "    def _decode(self, o):\n",
    "        if isinstance(o, str):\n",
    "            try:\n",
    "                return int(o)\n",
    "            except ValueError:\n",
    "                return o\n",
    "        elif isinstance(o, dict):\n",
    "            return {k: self._decode(v) for k, v in o.items()}\n",
    "        elif isinstance(o, list):\n",
    "            return [self._decode(v) for v in o]\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_path):\n",
    "    file_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            dicti = json.loads(line, cls=Decoder)\n",
    "            file_dict = {**file_dict, **dicti}\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "directory = \"/Users/sarab/work/try/TREC/processed_data/t_eval_results\"\n",
    "map_results = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    map_results[filename] = {}\n",
    "    q301_350 = 0\n",
    "    q351_400 = 0\n",
    "    q401_450 = 0\n",
    "    map_val = 0\n",
    "    count = 0\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "\n",
    "    with open(directory + '/' + filename, 'r') as file:\n",
    "        results = json.loads(file.read(), cls=Decoder)\n",
    "    for key,value in results.items():\n",
    "        key = int(key)\n",
    "        val = value.get(\"map\")\n",
    "        if 301 <= key <=350:\n",
    "            c1 += 1\n",
    "            q301_350 = q301_350 + val\n",
    "                \n",
    "        elif 351 <= key <=400:\n",
    "            c2 += 1\n",
    "            q351_400 = q351_400 + val\n",
    "            \n",
    "        elif 401 <= key <=450:\n",
    "            c3 += 1\n",
    "            q401_450 = q401_450 + val\n",
    "            \n",
    "        else:\n",
    "            print('error')\n",
    "            \n",
    "        map_val = map_val + val\n",
    "        count += 1\n",
    "    map_final = map_val / count\n",
    "    q301_350 = q301_350 / c1\n",
    "    q351_400 = q351_400 / c2\n",
    "    q401_450 = q401_450 / c3\n",
    "    map_results[filename]['q301_450'] = map_final\n",
    "    map_results[filename]['q301_350'] = q301_350\n",
    "    map_results[filename]['q351_400'] = q401_450\n",
    "    map_results[filename]['q401_450'] = q401_450\n",
    "    if c1 !=50 or c2 !=50 or c1 !=50 or count!=150 :\n",
    "        print('error_c')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q301_450</th>\n",
       "      <th>q301_350</th>\n",
       "      <th>q351_400</th>\n",
       "      <th>q401_450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_eval_t_t_tfidf_sum2</th>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.017023</td>\n",
       "      <td>0.017023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum3</th>\n",
       "      <td>0.022818</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>0.016374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum1</th>\n",
       "      <td>0.021488</td>\n",
       "      <td>0.038904</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.012390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum0</th>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.038280</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum009</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum3</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum01</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum006</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum007</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum008</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum109</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum108</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum107</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum106</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum11</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum2</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum1</th>\n",
       "      <td>0.016947</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.008897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum0</th>\n",
       "      <td>0.016576</td>\n",
       "      <td>0.031240</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.008114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply1</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.012352</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.003850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply2</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply3</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       q301_450  q301_350  q351_400  q401_450\n",
       "t_eval_t_t_tfidf_sum2  0.023011  0.035387  0.017023  0.017023\n",
       "t_eval_t_tfidf_sum3    0.022818  0.033636  0.016374  0.016374\n",
       "t_eval_t_tfidf_sum1    0.021488  0.038904  0.012390  0.012390\n",
       "t_eval_t_tfidf_sum0    0.021015  0.038280  0.011436  0.011436\n",
       "t_eval_t_wsum009       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_sum3          0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum01        0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum006       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum007       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum008       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum109       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum108       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum107       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum106       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum11        0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_sum2          0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_sum1          0.016947  0.032057  0.008897  0.008897\n",
       "t_eval_t_sum0          0.016576  0.031240  0.008114  0.008114\n",
       "t_eval_t_multiply1     0.006923  0.012352  0.003850  0.003850\n",
       "t_eval_t_multiply2     0.000102  0.000110  0.000100  0.000100\n",
       "t_eval_t_multiply3     0.000102  0.000110  0.000100  0.000100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame.from_dict(map_results,'index')\n",
    "df.sort_values(by=['q301_450'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import documents\n",
    "directory = '/Users/sarab/work/try/TREC/processed_data/processed_documents/wFT/' \n",
    "text = {}\n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\processed_documents\\\\wFT\\\\'+filename, 'r') as file:\n",
    "        for line in file: \n",
    "            dicti = json.loads(line)\n",
    "            text = {**text, **dicti}\n",
    "            \n",
    "directory = '/Users/sarab/work/try/TREC/processed_data/processed_documents/w/' \n",
    "for filename in os.listdir(directory):\n",
    "    with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\processed_documents\\\\w\\\\'+filename, 'r') as file:\n",
    "        for line in file: \n",
    "            dicti = json.loads(line)\n",
    "            text = {**text, **dicti}\n",
    "# import questions\n",
    "topics = {} \n",
    "with open('C:\\\\Users\\\\sarab\\\\work\\\\try\\\\TREC\\\\processed_data\\\\titles_topics', 'r') as file:\n",
    "        for line in file: \n",
    "            dicti = json.loads(line)\n",
    "            topics = {**topics, **dicti}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.get(\"FR940104-0-00002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = {}\n",
    "for k,v in text.items():\n",
    "    if len(v) != 0:\n",
    "        cleaned_text.update({k:v})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hubble Telescope Achievements \n"
     ]
    }
   ],
   "source": [
    "print(topics.get(\"303\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3126.750829935074\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "   \n",
    "  \n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "    \n",
    "#     tfidf_sum0 = dr.tfidf_score(tokenized, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_sum0:\n",
    "#         main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "#     tfidf_sum1 = dr.tfidf_score(tokenized+ext, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_sum1:\n",
    "#         main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "#     tfidf_sum2 = dr.tfidf_score(tokenized+ext+ext_candidates, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_sum2:\n",
    "#         main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_sum3 = dr.tfidf_score(tokenized+candidates, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_sum3:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "# file = open('wholet_tfidf_sum0',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti0)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "        \n",
    "# file = open('wholet_tfidf_sum1',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti1)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "# file = open('wholet_t_tfidf_sum2',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti2)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "file = open('wholet_tfidf_sum3',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16991.71778512001\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "main_dicti4 = {}\n",
    "for key in topics:\n",
    "    main_dicti4[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    wsum01 = dr.probability_score_sum_weights(tokenized, candidates, text,0, -1, wv_wiki_en)\n",
    "    for tupl in wsum01:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum006 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.6, wv_wiki_en)\n",
    "    for tupl in wsum006:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum007 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.7, wv_wiki_en)\n",
    "    for tupl in wsum007:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum008 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.8, wv_wiki_en)\n",
    "    for tupl in wsum008:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    wsum009 = dr.probability_score_sum_weights(tokenized, candidates, text,0, 0.9, wv_wiki_en)\n",
    "    for tupl in wsum009:\n",
    "        main_dicti4[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "        \n",
    "file = open('wholet_wsum01',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('wholet_wsum006',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('wholet_wsum007',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('wholet_wsum008',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('wholet_wsum009',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti4)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057.222836971283\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "    \n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "    \n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "\n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    ext = qe.extend_tokens(tokenized, wv_wiki_en)\n",
    "    candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    ext_candidates = qe.pre_retrieval_KNN(topics.get(key), 5, wv_wiki_en, 10, stopWords,extension=True)\n",
    "    \n",
    "\n",
    "    \n",
    "#     sum0 = dr.probability_score(tokenized,text, dr.probability_sum,0)\n",
    "#     for tupl in sum0:\n",
    "#         main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "#     sum1 = dr.probability_score(tokenized+ext,text, dr.probability_sum,0)\n",
    "#     for tupl in sum1:\n",
    "#         main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "#     sum2 = dr.probability_score(tokenized+ext+ext_candidates,text, dr.probability_sum,0)\n",
    "#     for tupl in sum2:\n",
    "#         main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    sum3 = dr.probability_score(tokenized+candidates,text, dr.probability_sum,0)\n",
    "    for tupl in sum3:\n",
    "        main_dicti3[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "# file = open('t_sum0',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti0)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "        \n",
    "# file = open('t_sum1',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti1)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "# file = open('t_sum2',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti2)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()\n",
    "\n",
    "file = open('tt_sum3',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti3)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the optimal number of candidates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dicti0 = {}\n",
    "for key in topics:\n",
    "    main_dicti0[key] = {}\n",
    "\n",
    "main_dicti1 = {}\n",
    "for key in topics:\n",
    "    main_dicti1[key] = {}\n",
    "    \n",
    "main_dicti2 = {}\n",
    "for key in topics:\n",
    "    main_dicti2[key] = {}\n",
    "\n",
    "main_dicti3 = {}\n",
    "for key in topics:\n",
    "    main_dicti3[key] = {}\n",
    "\n",
    "    \n",
    "\n",
    "for key in topics:\n",
    "  \n",
    "    tokenized = qe.tokenize(topics.get(key), stopWords)\n",
    "    candidates10 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    candidates20 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 20, stopWords,extension=False)\n",
    "    candidates30 = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 30, stopWords,extension=False)\n",
    "#    candidates = qe.pre_retrieval_KNN(topics.get(key), 10, wv_wiki_en, 10, stopWords,extension=False)\n",
    "    \n",
    "#     tfidf_test = dr.tfidf_score(tokenized+candidates, text, dr.tfidf_sum,0)\n",
    "#     for tupl in tfidf_test:\n",
    "#         main_dicti3[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    tfidf_cand10 = dr.tfidf_score(tokenized+candidates10, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_cand10:\n",
    "        main_dicti0[key][tupl[0]] = tupl[1]\n",
    "    \n",
    "    tfidf_cand20 = dr.tfidf_score(tokenized+candidates20, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_cand20:\n",
    "        main_dicti1[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "    tfidf_cand30 = dr.tfidf_score(tokenized+candidates30, text, dr.tfidf_sum,0)\n",
    "    for tupl in tfidf_cand30:\n",
    "        main_dicti2[key][tupl[0]] = tupl[1]\n",
    "        \n",
    "# file = open('tfidf_test',\"w+\") \n",
    "# dict_to_write = json.dumps(main_dicti3)\n",
    "# file.write(dict_to_write +\"\\n\")\n",
    "# file.close()    \n",
    "        \n",
    "file = open('tfidf_cand10',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti0)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "        \n",
    "file = open('tfidf_cand20',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti1)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()\n",
    "\n",
    "file = open('tfidf_cand30',\"w+\") \n",
    "dict_to_write = json.dumps(main_dicti2)\n",
    "file.write(dict_to_write +\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we add annotations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "def tokenized_query(text, stopwords):\n",
    "    \"\"\"Tokenizes and removes stopwords from the document\"\"\"\n",
    "    without_punctuations = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(without_punctuations)\n",
    "    filtered = ([lemmatizer.lemmatize(w.lower(), get_wordnet_pos(w.lower())) for w in tokens if not w in stopwords])\n",
    "    return filtered\n",
    "\n",
    "def extend_tokens(token_list, wv):\n",
    "    \"\"\"Extends token list summing vector pairs\"\"\"\n",
    "    tokens = []\n",
    "    for token in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            tokens.append(token)\n",
    "    extention = set()\n",
    "    for i in range(len(tokens)-1):\n",
    "        new_token = wv.most_similar(positive=[tokens[i], tokens[i+1]])[0][0]\n",
    "        extention.add(new_token)\n",
    "    extention = list(extention)\n",
    "    return extention\n",
    "\n",
    "def candidate_expansion_terms(tokens, k, wv):\n",
    "    \"\"\"Gets the candidate expansion terms\"\"\"\n",
    "    candidates = set()\n",
    "    for token in tokens:\n",
    "        # check if the token is in the vocabulary\n",
    "        if token in wv.vocab.keys():\n",
    "            result = wv.similar_by_word(token)\n",
    "            limit = k if len(result) > k else len(result)\n",
    "            # iterate through the most similar words\n",
    "            for i in range(limit):\n",
    "                candidates.add(result[i][0])\n",
    "    # return list of candidates\n",
    "    candidates = list(candidates)\n",
    "    return candidates\n",
    "\n",
    "def similarity(token, token_list, wv ):\n",
    "    \"\"\"calculates the similarity between word and list of words\"\"\"\n",
    "    # calculate the similarity of the token to all tokens\n",
    "    similarity = 0\n",
    "    num_of_tokens = 0\n",
    "    for toks in token_list:\n",
    "        # check if the token is in the vocabulary\n",
    "        if toks in wv.vocab.keys():\n",
    "            num_of_tokens += 1\n",
    "            similarity += wv.similarity(toks, token)\n",
    "    return similarity/num_of_tokens\n",
    "\n",
    "def get_similarity_pairs(tokens, candidates, wv):\n",
    "    \"\"\"Gets the actual expansion terms\"\"\"\n",
    "    similarity_pairs = []\n",
    "    for candidate in candidates:\n",
    "        sim = similarity(candidate, tokens, wv)\n",
    "        similarity_pairs.append((candidate, sim))\n",
    "    # return the list of expansion terms with their similarities\n",
    "    return similarity_pairs\n",
    "\n",
    "# updated function\n",
    "def pre_retrieval_KNN(query, k, wv, n, stop_words,extension=False):\n",
    "    \"\"\"Find the most similar tokens(candidates) to the given query, optional:query can be extended, then the candidates are found for extended query\"\"\"\n",
    "    tokens = tokenized_query(query, stop_words)\n",
    "    if extension:\n",
    "        extended = extend_tokens(tokens,wv)\n",
    "        candidates = candidate_expansion_terms(tokens+extended, k, wv)\n",
    "        candidates_sim = get_similarity_pairs(tokens+extended, candidates, wv)\n",
    "    else:\n",
    "        candidates = candidate_expansion_terms(tokens, k, wv)\n",
    "        candidates_sim = get_similarity_pairs(tokens, candidates, wv)\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "    sort = sorted(candidates_sim, key=takeSecond)[::-1]\n",
    "    sort = sort[:n]\n",
    "    candidate_list = []\n",
    "    for tupl in sort:\n",
    "        candidate_list.append(tupl[0])\n",
    "    cleaned = [word for word in candidate_list if word.isalpha()]\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in cleaned]\n",
    "    \n",
    "    return [w for w in lemmatized if w not in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samo na cetrtini evaluacija\n",
    "import ast\n",
    "directory = \"/Users/sarab/work/try/TREC/processed_data/t_eval_results\"\n",
    "map_results = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    map_results[filename] = {}\n",
    "    q301_350 = 0\n",
    "    q351_400 = 0\n",
    "    q401_450 = 0\n",
    "    map_val = 0\n",
    "    count = 0\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "\n",
    "    with open(directory + '/' + filename, 'r') as file:\n",
    "        results = json.loads(file.read(), cls=Decoder)\n",
    "    for key,value in results.items():\n",
    "        key = int(key)\n",
    "        val = value.get(\"map\")\n",
    "        if 301 <= key <=350:\n",
    "            c1 += 1\n",
    "            q301_350 = q301_350 + val\n",
    "                \n",
    "        elif 351 <= key <=400:\n",
    "            c2 += 1\n",
    "            q351_400 = q351_400 + val\n",
    "            \n",
    "        elif 401 <= key <=450:\n",
    "            c3 += 1\n",
    "            q401_450 = q401_450 + val\n",
    "            \n",
    "        else:\n",
    "            print('error')\n",
    "            \n",
    "        map_val = map_val + val\n",
    "        count += 1\n",
    "    map_final = map_val / count\n",
    "    q301_350 = q301_350 / c1\n",
    "    q351_400 = q351_400 / c2\n",
    "    q401_450 = q401_450 / c3\n",
    "    map_results[filename]['q301_450'] = map_final\n",
    "    map_results[filename]['q301_350'] = q301_350\n",
    "    map_results[filename]['q351_400'] = q401_450\n",
    "    map_results[filename]['q401_450'] = q401_450\n",
    "    if c1 !=50 or c2 !=50 or c1 !=50 or count!=150 :\n",
    "        print('error_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q301_450</th>\n",
       "      <th>q301_350</th>\n",
       "      <th>q351_400</th>\n",
       "      <th>q401_450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_eval_t_t_tfidf_sum2</th>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.017023</td>\n",
       "      <td>0.017023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum3</th>\n",
       "      <td>0.022818</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>0.016374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum1</th>\n",
       "      <td>0.021488</td>\n",
       "      <td>0.038904</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.012390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_tfidf_sum0</th>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.038280</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum009</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum3</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum01</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum006</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum007</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum008</th>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum109</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum108</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum107</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum106</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_wsum11</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum2</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum1</th>\n",
       "      <td>0.016947</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.008897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_sum0</th>\n",
       "      <td>0.016576</td>\n",
       "      <td>0.031240</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.008114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply1</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.012352</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.003850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply2</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_t_multiply3</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       q301_450  q301_350  q351_400  q401_450\n",
       "t_eval_t_t_tfidf_sum2  0.023011  0.035387  0.017023  0.017023\n",
       "t_eval_t_tfidf_sum3    0.022818  0.033636  0.016374  0.016374\n",
       "t_eval_t_tfidf_sum1    0.021488  0.038904  0.012390  0.012390\n",
       "t_eval_t_tfidf_sum0    0.021015  0.038280  0.011436  0.011436\n",
       "t_eval_t_wsum009       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_sum3          0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum01        0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum006       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum007       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum008       0.019337  0.029803  0.011889  0.011889\n",
       "t_eval_t_wsum109       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum108       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum107       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum106       0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_wsum11        0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_sum2          0.019190  0.031984  0.013021  0.013021\n",
       "t_eval_t_sum1          0.016947  0.032057  0.008897  0.008897\n",
       "t_eval_t_sum0          0.016576  0.031240  0.008114  0.008114\n",
       "t_eval_t_multiply1     0.006923  0.012352  0.003850  0.003850\n",
       "t_eval_t_multiply2     0.000102  0.000110  0.000100  0.000100\n",
       "t_eval_t_multiply3     0.000102  0.000110  0.000100  0.000100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame.from_dict(map_results,'index')\n",
    "df.sort_values(by=['q301_450'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na vseh dokumentih\n",
    "import ast\n",
    "directory = \"/Users/sarab/work/try/TREC/processed_data/whole_t_eval_results\"\n",
    "map_results = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    map_results[filename] = {}\n",
    "    q301_350 = 0\n",
    "    q351_400 = 0\n",
    "    q401_450 = 0\n",
    "    map_val = 0\n",
    "    count = 0\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "\n",
    "    with open(directory + '/' + filename, 'r') as file:\n",
    "        results = json.loads(file.read(), cls=Decoder)\n",
    "    for key,value in results.items():\n",
    "        key = int(key)\n",
    "        val = value.get(\"map\")\n",
    "        if 301 <= key <=350:\n",
    "            c1 += 1\n",
    "            q301_350 = q301_350 + val\n",
    "                \n",
    "        elif 351 <= key <=400:\n",
    "            c2 += 1\n",
    "            q351_400 = q351_400 + val\n",
    "            \n",
    "        elif 401 <= key <=450:\n",
    "            c3 += 1\n",
    "            q401_450 = q401_450 + val\n",
    "            \n",
    "        else:\n",
    "            print('error')\n",
    "            \n",
    "        map_val = map_val + val\n",
    "        count += 1\n",
    "    map_final = map_val / count\n",
    "    q301_350 = q301_350 / c1\n",
    "    q351_400 = q351_400 / c2\n",
    "    q401_450 = q401_450 / c3\n",
    "    map_results[filename]['q301_450'] = map_final\n",
    "    map_results[filename]['q301_350'] = q301_350\n",
    "    map_results[filename]['q351_400'] = q401_450\n",
    "    map_results[filename]['q401_450'] = q401_450\n",
    "    if c1 !=50 or c2 !=50 or c1 !=50 or count!=150 :\n",
    "        print('error_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q301_450</th>\n",
       "      <th>q301_350</th>\n",
       "      <th>q351_400</th>\n",
       "      <th>q401_450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_t_tfidf_sum2</th>\n",
       "      <td>0.045725</td>\n",
       "      <td>0.068429</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>0.036077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_tfidf_sum3</th>\n",
       "      <td>0.043693</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>0.033151</td>\n",
       "      <td>0.033151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_tfidf_sum1</th>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.060245</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>0.030062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_sum2</th>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>0.028995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_tfidf_sum0</th>\n",
       "      <td>0.039299</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.027445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_sum3</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_wsum006</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_wsum007</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_wsum008</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_wsum009</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_wsum01</th>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_sum1</th>\n",
       "      <td>0.035247</td>\n",
       "      <td>0.052328</td>\n",
       "      <td>0.023994</td>\n",
       "      <td>0.023994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_eval_wholet_sum0</th>\n",
       "      <td>0.034834</td>\n",
       "      <td>0.051653</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>0.023089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            q301_450  q301_350  q351_400  q401_450\n",
       "t_eval_wholet_t_tfidf_sum2  0.045725  0.068429  0.036077  0.036077\n",
       "t_eval_wholet_tfidf_sum3    0.043693  0.063716  0.033151  0.033151\n",
       "t_eval_wholet_tfidf_sum1    0.040793  0.060245  0.030062  0.030062\n",
       "t_eval_wholet_sum2          0.040380  0.062740  0.028995  0.028995\n",
       "t_eval_wholet_tfidf_sum0    0.039299  0.058075  0.027445  0.027445\n",
       "t_eval_wholet_sum3          0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_wsum006       0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_wsum007       0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_wsum008       0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_wsum009       0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_wsum01        0.038869  0.058917  0.026184  0.026184\n",
       "t_eval_wholet_sum1          0.035247  0.052328  0.023994  0.023994\n",
       "t_eval_wholet_sum0          0.034834  0.051653  0.023089  0.023089"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame.from_dict(map_results,'index')\n",
    "df.sort_values(by=['q301_450'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = tokenized\n",
    "# 1 = tokenized+ext\n",
    "# 2 = tokenized+ext+ext_candidates\n",
    "# 3 = tokenized+candidates\n",
    "# pri alpha: 0 = tokenized, 1 = tokenized + ext\n",
    "#\n",
    "# zgleda da tfidf metrika dela bolje na vec dokumentih, mi pa iz baze potegnemo le relavantne (malo - to ni najbolje)\n",
    "# kaj pa ce se tfidf v naprej preracuna? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
