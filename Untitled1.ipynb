{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, FastText\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "def tokenized_query(text, stopwords):\n",
    "    \"\"\"Tokenizes, lowers words and removes stopwords from the document.\n",
    "        Args:\n",
    "            text (str): Text we want to tokenize.\n",
    "            stopwords (list): List of words we want to remove from the tokenized text. \n",
    "        Returns:\n",
    "            filtered_tokens (list): List of low case tokens wich does not contain stop words.\n",
    "        \"\"\"\n",
    "    without_punctuations = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(without_punctuations)\n",
    "    filtered = ([lemmatizer.lemmatize(w.lower(), get_wordnet_pos(w.lower())) for w in tokens if not w in stopwords])\n",
    "    return filtered\n",
    "\n",
    "def extend_tokens(token_list, model):\n",
    "    \"\"\"Extends token list by summing consecutive vector pairs.\n",
    "        Args: \n",
    "            token_list (list): List of tokens we want to extend.\n",
    "        Returns:\n",
    "            extension (list): List of extensions.\n",
    "            wv (Word2VecKeyedVectors): Word embeddings.\n",
    "        \"\"\"\n",
    "    tokens = []\n",
    "    if model_format == 'word2vec':\n",
    "        for token in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in model.vocab.keys():\n",
    "                tokens.append(token)\n",
    "    if model_format == 'fasttext':\n",
    "        for token in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in model.wv.vocab:\n",
    "                tokens.append(token)\n",
    "    extention = set()\n",
    "    for i in range(len(tokens)-1):\n",
    "        new_token = model.most_similar(positive=[tokens[i], tokens[i+1]])[0][0]\n",
    "        extention.add(new_token)\n",
    "    extention = list(extention)\n",
    "    return extention\n",
    "\n",
    "def candidate_expansion_terms(tokens, k, model, model_format):\n",
    "    \"\"\"Gets the candidates for expansion based on kNN.\n",
    "        Args: \n",
    "            tokens (list): List of tokens we want to expand.\n",
    "            k (int): Number of nearest neighbours.\n",
    "            wv (Word2VecKeyedVectors): Word embeddings.\n",
    "        Returns:\n",
    "            candidates (list): List of candidates.\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    if model_format == 'word2vec':\n",
    "        for token in tokens:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in model.vocab.keys():\n",
    "                result = model.similar_by_word(token)\n",
    "                limit = k if len(result) > k else len(result)\n",
    "                # iterate through the most similar words\n",
    "                for i in range(limit):\n",
    "                    candidates.add(result[i][0])\n",
    "    elif model_format == 'fasttext':\n",
    "        for token in tokens:\n",
    "            # check if the token is in the vocabulary\n",
    "            if token in model.wv.vocab:\n",
    "                result = model.most_similar(token)\n",
    "                limit = k if len(result) > k else len(result)\n",
    "                # iterate through the most similar words\n",
    "                for i in range(limit):\n",
    "                    candidates.add(result[i][0])\n",
    "    else:\n",
    "        raise Exception('Model type incorrect')\n",
    "    # return list of candidates\n",
    "    candidates = list(candidates)\n",
    "    return candidates\n",
    "\n",
    "def similarity(token, token_list, model, model_format ):\n",
    "#      \"\"\"Calculates the similarity between token and list of tokens.\n",
    "#         Args: \n",
    "#             token (str): String for wich we are calculating similarity.\n",
    "#             token_list (list): List of tokens to wich we are calculating similarity of token.\n",
    "#             wv (Word2VecKeyedVectors): Word embeddings.\n",
    "#         Returns:\n",
    "#             avreage_similarity (float): Number that signifes the similarity of token to token list words.\n",
    "#         \"\"\"\n",
    "        \n",
    "    similarity = 0\n",
    "    num_of_tokens = 0\n",
    "    if model_format == 'word2vec':\n",
    "        for toks in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if toks in model.vocab.keys():\n",
    "                num_of_tokens += 1\n",
    "                similarity += model.similarity(toks, token)\n",
    "    elif model_format == 'fasttext':\n",
    "        for toks in token_list:\n",
    "            # check if the token is in the vocabulary\n",
    "            if toks in model.wv.vocab:\n",
    "                num_of_tokens += 1\n",
    "                similarity += model.similarity(toks, token)\n",
    "    else:\n",
    "        raise Exception('Model type incorrect')\n",
    "    return similarity/num_of_tokens\n",
    "\n",
    "\n",
    "def get_similarity_pairs(tokens, candidates, wv, model_format):\n",
    "    \"\"\"Calculates similarity to tokens for list of candidates.\n",
    "        Args: \n",
    "            tokens (list): List of tokens to wich similarity is calculated\n",
    "            candidates (list): List of tokens for wich similarity is calculated.\n",
    "            wv (Word2VecKeyedVectors): Word embeddings.\n",
    "        Returns:\n",
    "            similarity_pairs (list): List of tuples. Tuples are pairs of candidates and their similarity to tokens.\n",
    "        \"\"\"\n",
    "    similarity_pairs = []\n",
    "    for candidate in candidates:\n",
    "        sim = similarity(candidate, tokens, wv, model_format)\n",
    "        similarity_pairs.append((candidate, sim))\n",
    "    # return the list of expansion terms with their similarities\n",
    "    return similarity_pairs\n",
    "\n",
    "# updated function\n",
    "def pre_retrieval_KNN(query, k, wv, n, stop_words,model_format, extension=False):\n",
    "    \"\"\"Find n most similar tokens(candidates) to the given query, optional: \n",
    "        query can be extended, then the candidates are found for extended query.\n",
    "        Args: \n",
    "            query (string): User query we want to expand.\n",
    "            k (int): Number of nearest neighbours.\n",
    "            wv (Word2VecKeyedVectors): Word embeddings.\n",
    "            n (int): Number of candidates (with the highest simiarity) that is returned.\n",
    "            stopwords (list): List of words we want to remove from the tokenized text. \n",
    "        Returns:\n",
    "            candidate_list (list): List of n candidates with the highest similarity to query tokens.\n",
    "        \"\"\"\n",
    "    tokens = tokenized_query(query, stop_words)\n",
    "    if extension:\n",
    "        extended = extend_tokens(tokens,wv)\n",
    "        candidates = candidate_expansion_terms(tokens+extended, k, wv,model_format)\n",
    "        candidates_sim = get_similarity_pairs(tokens+extended, candidates, wv, model_format)\n",
    "    else:\n",
    "        candidates = candidate_expansion_terms(tokens, k, wv,model_format)\n",
    "        candidates_sim = get_similarity_pairs(tokens, candidates, wv, model_format)\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "    sort = sorted(candidates_sim, key=takeSecond)[::-1]\n",
    "    candidate_list = []\n",
    "    for tupl in sort:\n",
    "        candidate_list.append(tupl[0])\n",
    "    cleaned = [word for word in candidate_list if word.isalpha()]\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in cleaned]\n",
    "    candidate_list = [w for w in lemmatized if w not in tokens]\n",
    "    candidate_list = candidate_list[:n]\n",
    "    return candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\Miniconda3\\envs\\envirolens\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "wiki_en_align = './../data/fasttext/wiki.en.align.vec'\n",
    "wv_wiki_en = KeyedVectors.load_word2vec_format(wiki_en_align)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['undergroung',\n",
       " 'undergrounders',\n",
       " 'undergroun',\n",
       " 'undergrounder',\n",
       " 'undergrounded']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_retrieval_KNN('fish underground', 10, wv_wiki_en, 5, stop_words,'word2vec', extension=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
